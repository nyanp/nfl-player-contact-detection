{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7912e72c-943a-44af-a037-26014cacdc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import gc\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from typing import Any, Dict, List, Tuple, Optional, Union\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_ema import ExponentialMovingAverage\n",
    "from tqdm import tqdm\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold, PredefinedSplit\n",
    "from sklearn.metrics import matthews_corrcoef, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "\n",
    "\n",
    "def save_file(path: str, filename=None, base_path=None) -> None:\n",
    "    filename = filename or os.path.basename(path)\n",
    "    shutil.copy(\n",
    "        path,\n",
    "        os.path.join(wandb.run.dir, filename),\n",
    "    )\n",
    "    wandb.save(os.path.join(wandb.run.dir, filename), base_path)\n",
    "\n",
    "    \n",
    "def mcc_sweep(y_true, y_pred):\n",
    "    best_score = 0\n",
    "    min_th = 0.3\n",
    "    max_th = 0.45\n",
    "    th_step = 0.01\n",
    "    \n",
    "    th = min_th\n",
    "    while th <= max_th:\n",
    "        score = matthews_corrcoef(y_true, y_pred >= th)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "        th += th_step\n",
    "    return best_score\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, x_num: np.ndarray, x_cat: np.ndarray, y: Optional[np.ndarray]):\n",
    "        super().__init__()\n",
    "        self.x_num = x_num\n",
    "        self.x_cat = x_cat\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_num)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.x_num[idx], torch.LongTensor(self.x_cat[idx])\n",
    "        else:\n",
    "            return self.x_num[idx], torch.LongTensor(self.x_cat[idx]), self.y[idx]\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 src_num_dim: int,\n",
    "                 n_categories: List[int],\n",
    "                 dropout: float = 0.0,\n",
    "                 hidden: int = 50,\n",
    "                 emb_dim: int = 10,\n",
    "                 dropout_cat: float = 0.2,\n",
    "                 bn: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embs = nn.ModuleList([\n",
    "            nn.Embedding(x, emb_dim) for x in n_categories])\n",
    "        self.cat_dim = emb_dim * len(n_categories)\n",
    "        self.dropout_cat = nn.Dropout(dropout_cat)\n",
    "\n",
    "        if bn:\n",
    "            self.sequence = nn.Sequential(\n",
    "                nn.Linear(src_num_dim + self.cat_dim, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, 1)\n",
    "            )\n",
    "        else:\n",
    "            self.sequence = nn.Sequential(\n",
    "                nn.Linear(src_num_dim + self.cat_dim, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n",
    "        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n",
    "        x_all = torch.cat([x_num, x_cat_emb], 1)\n",
    "        x = self.sequence(x_all)\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_features: int,\n",
    "                 hidden_size: int,\n",
    "                 n_categories: List[int],\n",
    "                 emb_dim: int = 10,\n",
    "                 dropout_cat: float = 0.2,\n",
    "                 channel_1: int = 256,\n",
    "                 channel_2: int = 512,\n",
    "                 channel_3: int = 512,\n",
    "                 dropout_top: float = 0.1,\n",
    "                 dropout_mid: float = 0.3,\n",
    "                 dropout_bottom: float = 0.2,\n",
    "                 weight_norm: bool = True,\n",
    "                 two_stage: bool = True,\n",
    "                 celu: bool = True,\n",
    "                 kernel1: int = 5,\n",
    "                 no_cat: bool = True,\n",
    "                 leaky_relu: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        num_targets = 1\n",
    "\n",
    "        cha_1_reshape = int(hidden_size / channel_1)\n",
    "        cha_po_1 = int(hidden_size / channel_1 / 2)\n",
    "        cha_po_2 = int(hidden_size / channel_1 / 2 / 2) * channel_3\n",
    "        \n",
    "        print(f\"cha_po: {cha_1_reshape}/{cha_po_1}/{cha_po_2}\")\n",
    "        \n",
    "        assert cha_1_reshape > 0\n",
    "        assert cha_po_1 > 0\n",
    "        assert cha_po_2 > 0\n",
    "\n",
    "        self.cat_dim = 0 if no_cat else emb_dim * len(n_categories)\n",
    "        self.cha_1 = channel_1\n",
    "        self.cha_2 = channel_2\n",
    "        self.cha_3 = channel_3\n",
    "        self.cha_1_reshape = cha_1_reshape\n",
    "        self.cha_po_1 = cha_po_1\n",
    "        self.cha_po_2 = cha_po_2\n",
    "        self.two_stage = two_stage\n",
    "\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.BatchNorm1d(num_features + self.cat_dim),\n",
    "            nn.Dropout(dropout_top),\n",
    "            nn.utils.weight_norm(nn.Linear(num_features + self.cat_dim, hidden_size), dim=None),\n",
    "            nn.CELU(0.06) if celu else nn.ReLU()\n",
    "        )\n",
    "\n",
    "        def _norm(layer, dim=None):\n",
    "            return nn.utils.weight_norm(layer, dim=dim) if weight_norm else layer\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(channel_1),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_1, channel_2, kernel_size=kernel1, stride=1, padding=kernel1 // 2, bias=False)),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(output_size=cha_po_1),\n",
    "            nn.BatchNorm1d(channel_2),\n",
    "            nn.Dropout(dropout_top),\n",
    "            _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        if self.two_stage:\n",
    "            self.conv2 = nn.Sequential(\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_mid),\n",
    "                _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(channel_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Conv1d(channel_2, channel_3, kernel_size=5, stride=1, padding=2, bias=True)),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.flt = nn.Flatten()\n",
    "\n",
    "        if leaky_relu:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0),\n",
    "                nn.LeakyReLU()\n",
    "            )\n",
    "        else:\n",
    "            self.dense = nn.Sequential(\n",
    "                nn.BatchNorm1d(cha_po_2),\n",
    "                nn.Dropout(dropout_bottom),\n",
    "                _norm(nn.Linear(cha_po_2, num_targets), dim=0)\n",
    "            )\n",
    "\n",
    "        self.no_cat = no_cat\n",
    "        \n",
    "        if not no_cat:\n",
    "            self.embs = nn.ModuleList([nn.Embedding(x, emb_dim) for x in n_categories])\n",
    "            self.cat_dim = emb_dim * len(n_categories)\n",
    "            self.dropout_cat = nn.Dropout(dropout_cat)\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        if self.no_cat:\n",
    "            x = x_num\n",
    "        else:\n",
    "            embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n",
    "            x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n",
    "            x = torch.cat([x_num, x_cat_emb], 1)\n",
    "\n",
    "        x = self.expand(x)\n",
    "\n",
    "        x = x.reshape(x.shape[0], self.cha_1, self.cha_1_reshape)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        if self.two_stage:\n",
    "            x = self.conv2(x) * x\n",
    "\n",
    "        x = self.max_po_c2(x)\n",
    "        x = self.flt(x)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        return torch.squeeze(x)\n",
    "\n",
    "\n",
    "def preprocess_nn(\n",
    "        X: pd.DataFrame,\n",
    "        non_feature_cols: List[str],\n",
    "        cat_cols: List[str],\n",
    "        null_check_cols: List[str],\n",
    "        scaler: Optional[StandardScaler] = None):\n",
    "    for c in null_check_cols:\n",
    "        if c in X.columns:\n",
    "            X[f\"{c}_isnull\"] = X[c].isnull().astype(int)\n",
    "            \n",
    "    print(f\"null_check_cols: {null_check_cols}\")\n",
    "\n",
    "    cat_cols = [c for c in X.columns if c not in non_feature_cols and c in cat_cols]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols and c not in non_feature_cols]\n",
    "\n",
    "    X_num = X[num_cols].values.astype(np.float32)\n",
    "    X_cat = np.nan_to_num(X[cat_cols].values.astype(np.int32))\n",
    "\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        X_num = scaler.fit_transform(X_num)\n",
    "        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "        return X_num, X_cat, cat_cols, scaler\n",
    "    else:\n",
    "        X_num = scaler.transform(X_num) #TODO: infでも大丈夫？\n",
    "        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n",
    "        return X_num, X_cat, cat_cols\n",
    "\n",
    "\n",
    "def train_epoch(data_loader: DataLoader,\n",
    "                model: nn.Module,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                device,\n",
    "                clip_grad: float = 1.5,\n",
    "                debug: bool = False,\n",
    "                ema = None):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    step = 0\n",
    " \n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Training', disable=not debug):\n",
    "        batch_size = x_num.size(0)\n",
    "        x_num = x_num.to(device, dtype=torch.float)\n",
    "        x_cat = x_cat.to(device)\n",
    "        y = y.to(device, dtype=torch.float)\n",
    "        \n",
    "        loss = criterion(model(x_num, x_cat), y)\n",
    "        \n",
    "        losses.update(loss.detach().cpu().numpy(), batch_size)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "        if ema is not None:\n",
    "            ema.update()\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def evaluate(data_loader: DataLoader, model, device, ema=None):\n",
    "    model.eval()\n",
    "\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    final_targets = []\n",
    "    final_outputs = []\n",
    "    \n",
    "    criterion = torch.nn.BCELoss()\n",
    "    \n",
    "    if ema is not None:\n",
    "        ema.store()\n",
    "        ema.copy_to()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n",
    "            batch_size = x_num.size(0)\n",
    "            x_num = x_num.to(device, dtype=torch.float)\n",
    "            x_cat = x_cat.to(device)\n",
    "            y = y.to(device, dtype=torch.float)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(x_num, x_cat)\n",
    "                output = torch.sigmoid(output)\n",
    "\n",
    "            loss = criterion(output, y)\n",
    "            # record loss\n",
    "            losses.update(loss.detach().cpu().numpy(), batch_size)\n",
    "\n",
    "            targets = y.detach().cpu().numpy()\n",
    "            output = output.detach().cpu().numpy()\n",
    "\n",
    "            final_targets.append(targets)\n",
    "            final_outputs.append(output)\n",
    "\n",
    "    final_targets = np.concatenate(final_targets)\n",
    "    final_outputs = np.concatenate(final_outputs)\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(final_targets, final_outputs)\n",
    "        mcc = mcc_sweep(final_targets, final_outputs)\n",
    "    except:\n",
    "        auc = None\n",
    "        mcc = None\n",
    "        \n",
    "    if ema is not None:\n",
    "        ema.restore()\n",
    "\n",
    "    return final_outputs, final_targets, losses.avg, auc, mcc\n",
    "\n",
    "\n",
    "def predict_nn(X: pd.DataFrame,\n",
    "               non_feature_cols: List[str], # from: artifacts/metadata.json\n",
    "               cat_cols: List[str], # from: artifacts/metadata.json\n",
    "               null_check_cols: List[str], # from: artifacts/metadata.json\n",
    "               model: Union[List[CNN], CNN],\n",
    "               scaler: StandardScaler, # from: artifacts/scaler\n",
    "               device,\n",
    "               ensemble_method='mean'):\n",
    "    if not isinstance(model, list):\n",
    "        model = [model]\n",
    "\n",
    "    for m in model:\n",
    "        m.eval()\n",
    "    X_num, X_cat, cat_cols = preprocess_nn(\n",
    "        X.copy(),\n",
    "        non_feature_cols=non_feature_cols,\n",
    "        cat_cols=cat_cols,\n",
    "        null_check_cols=null_check_cols,\n",
    "        scaler=scaler\n",
    "    )\n",
    "    valid_dataset = TabularDataset(X_num, X_cat, None)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                               batch_size=512,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=1)\n",
    "\n",
    "    final_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_num, x_cat in tqdm(valid_loader, position=0, leave=True, desc='Evaluating'):\n",
    "            x_num = x_num.to(device, dtype=torch.float)\n",
    "            x_cat = x_cat.to(device)\n",
    "\n",
    "            outputs = []\n",
    "            with torch.no_grad():\n",
    "                for m in model:\n",
    "                    output = m(x_num, x_cat)\n",
    "                    output = torch.sigmoid(output)\n",
    "                    outputs.append(output.detach().cpu().numpy())\n",
    "\n",
    "            if ensemble_method == 'median':\n",
    "                pred = np.nanmedian(np.array(outputs), axis=0)\n",
    "            else:\n",
    "                pred = np.array(outputs).mean(axis=0)\n",
    "            final_outputs.append(pred)\n",
    "\n",
    "    final_outputs = np.concatenate(final_outputs)\n",
    "    return final_outputs\n",
    "\n",
    "\n",
    "def train_nn(X: pd.DataFrame,\n",
    "             y: pd.DataFrame,\n",
    "             non_feature_cols: List[str],\n",
    "             cat_cols: List[str],\n",
    "             null_check_cols: List[str],\n",
    "             folds: List[Tuple],\n",
    "             device,\n",
    "             emb_dim: int = 25,\n",
    "             batch_size: int = 1024,\n",
    "             cnn_hidden: int = 256,\n",
    "             cnn_channel1: int = 32,\n",
    "             cnn_channel2: int = 32,\n",
    "             cnn_channel3: int = 32,\n",
    "             cnn_kernel1: int = 5,\n",
    "             cnn_celu: bool = False,\n",
    "             cnn_weight_norm: bool = False,\n",
    "             dropout_emb: bool = 0.0,\n",
    "             two_stage: bool = False,\n",
    "             lr: float = 1e-3,\n",
    "             weight_decay: float = 0.0,\n",
    "             model_path: str = 'fold_{}.pth',\n",
    "             output_dir: str = 'artifacts',\n",
    "             scheduler_type: str = 'onecycle',\n",
    "             optimizer_type: str = 'adam',\n",
    "             max_lr: float = 0.01,\n",
    "             epochs: int = 30,\n",
    "             seed: int = 42,\n",
    "             batch_double_freq: int = 50,\n",
    "             cnn_dropout: float = 0.1,\n",
    "             cnn_leaky_relu: bool = False,\n",
    "             patience: int = 8,\n",
    "             factor: float = 0.5,\n",
    "             debug: bool = False,\n",
    "             ema_decay: float = None,\n",
    "             use_wandb: bool = True):\n",
    "    seed_everything(seed)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    y = y.values.astype(np.float32)\n",
    "    X_num, X_cat, cat_cols, scaler = preprocess_nn(\n",
    "        X.copy(),\n",
    "        non_feature_cols,\n",
    "        cat_cols,\n",
    "        null_check_cols)\n",
    "\n",
    "    best_losses = []\n",
    "    best_predictions = []\n",
    "    \n",
    "    oof = np.zeros(len(X_num))\n",
    "\n",
    "    for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n",
    "        X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n",
    "        X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n",
    "        y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "\n",
    "        cur_batch = batch_size\n",
    "        best_loss = 0\n",
    "        best_prediction = None\n",
    "\n",
    "        if debug:\n",
    "            print(f\"fold {cv_idx} train: {X_tr.shape}, valid: {X_va.shape}\")\n",
    "\n",
    "        train_dataset = TabularDataset(X_tr, X_tr_cat, y_tr)\n",
    "        valid_dataset = TabularDataset(X_va, X_va_cat, y_va)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=cur_batch, shuffle=True,\n",
    "                                                   num_workers=0)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=cur_batch, shuffle=False,\n",
    "                                                   num_workers=0)\n",
    "\n",
    "        model = CNN(X_tr.shape[1],\n",
    "                    hidden_size=cnn_hidden,\n",
    "                    n_categories=[128],\n",
    "                    emb_dim=emb_dim,\n",
    "                    dropout_cat=dropout_emb,\n",
    "                    channel_1=cnn_channel1,\n",
    "                    channel_2=cnn_channel2,\n",
    "                    channel_3=cnn_channel3,\n",
    "                    two_stage=two_stage,\n",
    "                    kernel1=cnn_kernel1,\n",
    "                    celu=cnn_celu,\n",
    "                    dropout_top=cnn_dropout,\n",
    "                    dropout_mid=cnn_dropout,\n",
    "                    dropout_bottom=cnn_dropout,\n",
    "                    weight_norm=cnn_weight_norm,\n",
    "                    leaky_relu=cnn_leaky_relu)\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        if ema_decay:\n",
    "            ema = ExponentialMovingAverage(model.parameters(), decay=ema_decay)\n",
    "        else:\n",
    "            ema = None\n",
    "\n",
    "        if optimizer_type == 'adamw':\n",
    "            opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        elif optimizer_type == 'adam':\n",
    "            opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        scheduler = epoch_scheduler = None\n",
    "        if scheduler_type == 'onecycle':\n",
    "            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, pct_start=0.1, div_factor=1e3,\n",
    "                                                            max_lr=max_lr, epochs=epochs,\n",
    "                                                            steps_per_epoch=len(train_loader))\n",
    "        elif scheduler_type == 'reduce':\n",
    "            epoch_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=opt,\n",
    "                                                                         mode='max',\n",
    "                                                                         min_lr=1e-7,\n",
    "                                                                         patience=patience,\n",
    "                                                                         verbose=True,\n",
    "                                                                         factor=factor)\n",
    "\n",
    "        best_model_path = os.path.join(output_dir, model_path.format(cv_idx))\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            if debug:\n",
    "                print(f\"epoch {epoch}\")\n",
    "            if epoch > 0 and epoch % batch_double_freq == 0:\n",
    "                cur_batch = cur_batch * 2\n",
    "                if debug:\n",
    "                    print(f'batch: {cur_batch}')\n",
    "                train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                           batch_size=cur_batch,\n",
    "                                                           shuffle=True,\n",
    "                                                           num_workers=4)\n",
    "            train_loss = train_epoch(train_loader, model, opt, scheduler, device, debug=debug, ema=ema)\n",
    "            predictions, valid_targets, valid_loss, auc, mcc = evaluate(valid_loader, model, device=device, ema=ema)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"epoch {epoch}, train loss: {train_loss:.4f}, valid auc: {auc:.4f}, valid mcc: {mcc:.4f}\")\n",
    "                \n",
    "            if use_wandb:\n",
    "                wandb.log({\n",
    "                    \"epoch\": epoch,\n",
    "                    f\"fold{cv_idx}_train_loss\": train_loss,\n",
    "                    f\"fold{cv_idx}_valid_loss\": valid_loss,\n",
    "                    f\"fold{cv_idx}_valid_auc\": auc,\n",
    "                    f\"fold{cv_idx}_valid_mcc\": mcc,\n",
    "                })\n",
    "\n",
    "            if epoch_scheduler is not None:\n",
    "                epoch_scheduler.step(auc)\n",
    "\n",
    "            if mcc > best_loss:\n",
    "                if debug:\n",
    "                    print(f'new best:{mcc}')\n",
    "                best_loss = mcc\n",
    "                best_prediction = predictions\n",
    "                \n",
    "                if ema is not None:\n",
    "                    ema.store()\n",
    "                    ema.copy_to()\n",
    "                torch.save(model, best_model_path)\n",
    "                if ema is not None:\n",
    "                    ema.restore()\n",
    "                oof[valid_idx] = best_prediction\n",
    "\n",
    "        if use_wandb:\n",
    "            wandb.run.summary[f\"fold{cv_idx}_valid_mcc_best\"] = best_loss\n",
    "            save_file(best_model_path)\n",
    "\n",
    "        best_predictions.append(best_prediction)\n",
    "        best_losses.append(best_loss)\n",
    "        del model, train_dataset, valid_dataset, train_loader, valid_loader, X_tr, X_va, X_tr_cat, X_va_cat, y_tr, y_va, opt\n",
    "        if scheduler is not None:\n",
    "            del scheduler\n",
    "        gc.collect()\n",
    "\n",
    "    return best_losses, best_predictions, oof, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3410fe8-4126-47ff-bf29-b2987e5ea0ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def search_best_threshold_pair(y_true, y_pred, is_ground):\n",
    "    def func(x_list):\n",
    "        score = matthews_corrcoef(y_true, binarize_pred(y_pred, x_list[0], x_list[1], is_ground))\n",
    "        return -score\n",
    "\n",
    "    x0 = [0.3, 0.3]\n",
    "    result = minimize(func, x0,  method=\"nelder-mead\")\n",
    "\n",
    "    return result.x[0], result.x[1]\n",
    "\n",
    "\n",
    "def binarize_pred(y_pred, threshold, threshold2, threshold2_mask):\n",
    "    return ~threshold2_mask*(y_pred>threshold)+threshold2_mask*(y_pred>threshold2)\n",
    "\n",
    "\n",
    "def train_nn_from_df(\n",
    "    train_df: pd.DataFrame,\n",
    "    split_defs: pd.DataFrame, \n",
    "    nn_params: Dict[str, Any],\n",
    "    null_check_top_n: int = 30,\n",
    "    debug: bool = True,\n",
    "    output_dir: str = \"artifacts\",\n",
    "    use_wandb: bool = True,\n",
    "    wandb_entity: str = \"nyanp\",\n",
    "    wandb_project: str = \"nfl-nn\"\n",
    "): \n",
    "    if use_wandb:\n",
    "        wandb.init(\n",
    "            project=wandb_project,\n",
    "            entity=wandb_entity,\n",
    "            config={\n",
    "                **params\n",
    "            }\n",
    "        )\n",
    "\n",
    "    non_feature_cols = [\n",
    "        \"contacgt_id\",\n",
    "        \"game_play\",\n",
    "        \"datetime\",\n",
    "        \"step\",\n",
    "        \"nfl_player_id_1\",\n",
    "        \"nfl_player_id_2\",\n",
    "        \"contact\",\n",
    "        \"team_1\",\n",
    "        \"team_2\",\n",
    "        \"contact_id\",\n",
    "        #\"position_1\",\n",
    "        #\"position_2\"\n",
    "        #\"direction_1\",\n",
    "        #\"direction_2\",\n",
    "        \"x_position_1\",\n",
    "        \"x_position_2\",\n",
    "        \"y_position_1\",\n",
    "        \"y_position_2\",\n",
    "        \"x_position_start_1\",\n",
    "        \"x_position_start_2\",\n",
    "        \"y_position_start_1\",\n",
    "        \"y_position_start_2\",\n",
    "\n",
    "        \"x_position_future5_1\",\n",
    "        \"x_position_future5_2\",\n",
    "        \"y_position_future5_1\",\n",
    "        \"y_position_future5_2\",\n",
    "        \"x_position_past5_1\",\n",
    "        \"x_position_past5_2\",\n",
    "        \"y_position_past5_1\",\n",
    "        \"y_position_past5_2\",\n",
    "        \"nfl_player_id_interceptor_1\",\n",
    "        \"nfl_player_id_interceptor_2\",\n",
    "\n",
    "        #\"orientation_past5_1\",\n",
    "        #\"direction_past5_1\",\n",
    "        #\"orientation_past5_2\",\n",
    "        #\"direction_past5_2\",\n",
    "    ]\n",
    "\n",
    "\n",
    "    split_df = train_df[[\"game_play\"]].copy()\n",
    "    split_df[\"game\"] = split_df[\"game_play\"].str[:5].astype(int)\n",
    "    split_df = pd.merge(split_df, split_defs, how=\"left\")\n",
    "    split = list(PredefinedSplit(split_df[\"fold\"]).split())\n",
    "\n",
    "    feature_names = [c for c in train_df.columns if c not in non_feature_cols]\n",
    "\n",
    "    # 欠損数から代表的な列を抽出する\n",
    "    num_nulls = train_df[feature_names].isnull().sum().drop_duplicates()\n",
    "    n_top = train_df[feature_names].isnull().sum().value_counts().index[:null_check_top_n]\n",
    "\n",
    "    null_check_cols = num_nulls[num_nulls.isin(set(n_top))].index.tolist()\n",
    "\n",
    "    extras = [\n",
    "        \"cnn_pred_Endzone_roll21\",\n",
    "        \"cnn_pred_Endzone_roll11\",\n",
    "        \"cnn_pred_Endzone_roll5\",\n",
    "        \"cnn_pred_Sideline_roll21\",\n",
    "        \"cnn_pred_Sideline_roll11\",\n",
    "        \"cnn_pred_Sideline_roll5\",\n",
    "        \"distance\",\n",
    "    ]\n",
    "\n",
    "    for e in extras:\n",
    "        if e not in null_check_cols:\n",
    "            null_check_cols.append(e)\n",
    "\n",
    "    print(null_check_cols)\n",
    "\n",
    "    X_train = np.empty((len(train_df), len(feature_names)), dtype=np.float32)\n",
    "\n",
    "    cat_cols = []\n",
    "    for i, c in enumerate(feature_names):\n",
    "        if train_df[c].dtype.name == \"object\":\n",
    "            X_train[:, i] = 0\n",
    "            cat_cols.append(c)\n",
    "        else:\n",
    "            X_train[:, i] = train_df[c]\n",
    "\n",
    "    X_train = pd.DataFrame(X_train, columns=feature_names)\n",
    "    y = train_df[\"contact\"]\n",
    "\n",
    "    assert torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    if use_wandb:\n",
    "        wandb.config.update({\n",
    "            \"null_check_cols\": null_check_cols, \n",
    "            \"cat_cols\": cat_cols,\n",
    "            \"n_features\": X_train.shape[1],\n",
    "            \"n_rows\": len(X_train),\n",
    "            \"null_check_top_n\": null_check_top_n\n",
    "        })\n",
    "\n",
    "    loss, preds, oof, scaler = train_nn(\n",
    "            X=X_train,\n",
    "            y=y,\n",
    "            non_feature_cols=non_feature_cols,\n",
    "            folds=split,\n",
    "            device=device,\n",
    "            debug=debug,\n",
    "            output_dir=output_dir,\n",
    "            null_check_cols=null_check_cols,\n",
    "            cat_cols=cat_cols,\n",
    "            **nn_params\n",
    "        )\n",
    "\n",
    "    is_ground = train_df[\"nfl_player_id_2\"] == -1\n",
    "    th1, th2 = search_best_threshold_pair(y, oof, is_ground)\n",
    "    y_pred = binarize_pred(oof, th1, th2, is_ground)\n",
    "\n",
    "    mcc = matthews_corrcoef(y, y_pred)\n",
    "\n",
    "    np.save(os.path.join(output_dir, \"oof.npy\"), oof)\n",
    "\n",
    "    with open(os.path.join(output_dir, \"scaler\"), \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    metadata = {\n",
    "        \"threshold_p\": th1,\n",
    "        \"threshold_g\": th2,\n",
    "        \"null_check_cols\": null_check_cols,\n",
    "        \"cat_cols\": cat_cols,\n",
    "        \"mcc\": mcc,\n",
    "        \"non_feature_cols\": non_feature_cols\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(output_dir, \"metadata.json\"), \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb.run.summary[\"mcc\"] = mcc\n",
    "        wandb.run.summary[\"auc\"] = roc_auc_score(y, oof)\n",
    "        wandb.run.summary[\"th1\"] = th1\n",
    "        wandb.run.summary[\"th2\"] = th2\n",
    "\n",
    "        save_file(os.path.join(output_dir, \"oof.npy\"))\n",
    "        save_file(os.path.join(output_dir, \"scaler\"))\n",
    "        save_file(os.path.join(output_dir, \"metadata.json\"))\n",
    "        \n",
    "        wandb.finish()\n",
    "\n",
    "    return oof, mcc, th1, th2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd958669-4b86-4082-b854-571af9319140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dfは、make_featuresした後のhard sampleをto_featherで保存したやつです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "627c2a3f-c84e-4a2c-adeb-b5fec1e955bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_df.f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21449/3866285792.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_feather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_df.f\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msplit_defs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"../input/nfl-game-fold/game_fold.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/feather_format.py\u001b[0m in \u001b[0;36mread_feather\u001b[0;34m(path, columns, use_threads, storage_options)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     with get_handle(\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     ) as handles:\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_df.f'"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df = pd.read_feather(\"train_df.f\")\n",
    "print(train_df.shape)\n",
    "\n",
    "split_defs = pd.read_csv(f\"../input/nfl-game-fold/game_fold.csv\")\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"optimizer_type\": \"adamw\",\n",
    "    \"scheduler_type\": \"reduce\",\n",
    "    \"factor\": 0.4,\n",
    "    \"cnn_kernel1\": 3,\n",
    "    \"weight_decay\": 2e-5,\n",
    "    \"cnn_weight_norm\": False,\n",
    "    \"cnn_channel1\": 96,\n",
    "    \"cnn_channel2\": 96,\n",
    "    \"cnn_channel3\": 96,\n",
    "    \"cnn_hidden\": 1536,\n",
    "    \"cnn_dropout\": 0.65,\n",
    "    \"batch_size\": 512,\n",
    "    \"max_lr\": 0.0026,\n",
    "    \"lr\": 0.00075,\n",
    "    \"epochs\": 40,\n",
    "    \"ema_decay\": 0.995, # 0.995,\n",
    "    \"patience\": 8\n",
    "}\n",
    "\n",
    "oof, mcc, th1, th2 = train_nn_from_df(train_df, \n",
    "                                      split_defs, \n",
    "                                      params, \n",
    "                                      use_wandb=True,\n",
    "                                      null_check_top_n=30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
