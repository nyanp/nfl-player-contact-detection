{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbaf30f-aee0-479a-9c58-ad5bb29641fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "import tempfile\n",
    "import traceback\n",
    "from dataclasses import dataclass, asdict\n",
    "from contextlib import contextmanager\n",
    "from copy import deepcopy\n",
    "\n",
    "import cv2\n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import treelite\n",
    "import treelite_runtime\n",
    "from scipy import special\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold, PredefinedSplit\n",
    "from sklearn.metrics import matthews_corrcoef, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from typing import Union\n",
    "from typing import List, Optional\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb745d-53a0-402a-8fa5-920a7e10a827",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    INPUT_DIR = \"../../input\"\n",
    "    CACHE_DIR = \"../../cache\"\n",
    "\n",
    "    # competition data\n",
    "    INPUT = f\"{INPUT_DIR}/nfl-player-contact-detection\"\n",
    "    \n",
    "    # pre-required datasets\n",
    "    CAMARO_DF1_PATH = f'{INPUT_DIR}/nfl-exp048/val_df.csv'\n",
    "    CAMARO_DF2_PATH = f'{INPUT_DIR}/camaro-exp117/exp117_val_preds.csv'\n",
    "    KMAT_PATH = f'{INPUT_DIR}/mfl2cnnkmat0221'\n",
    "\n",
    "    IS_TRAIN = True\n",
    "\n",
    "    KMAT_CNN_ALL_FRAME = False\n",
    "    TREELITE_COMPILE= True\n",
    "    USE_GPU = True  # set False when running without GPU\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6010b2b9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 9.710637,
     "end_time": "2023-02-24T23:49:55.051263",
     "exception": false,
     "start_time": "2023-02-24T23:49:45.340626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "sys.path.insert(1, cfg.KMAT_PATH)\n",
    "\n",
    "os.makedirs(cfg.CACHE_DIR, exist_ok=True)\n",
    "\n",
    "from train_contact_det import NFLContact, view_contact_mask  # この辺バージョンの違いはない？\n",
    "from train_utils.dataloader import inference_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc647d12",
   "metadata": {
    "papermill": {
     "duration": 0.052696,
     "end_time": "2023-02-24T23:49:55.193625",
     "exception": false,
     "start_time": "2023-02-24T23:49:55.140929",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "kalman smooth系の関数追加。\n",
    "\n",
    "0219 update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce33a0ba",
   "metadata": {
    "papermill": {
     "duration": 0.051224,
     "end_time": "2023-02-24T23:49:55.296672",
     "exception": false,
     "start_time": "2023-02-24T23:49:55.245448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "del df[\"datetime_ngs\"] 残しといてもらえると嬉しいのでとりあえずキープ。CNN終了後に消します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fa16e7",
   "metadata": {
    "papermill": {
     "duration": 0.095955,
     "end_time": "2023-02-24T23:49:55.444727",
     "exception": false,
     "start_time": "2023-02-24T23:49:55.348772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(name: str):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - s\n",
    "    print(f\"[{name}] {elapsed:.3f}sec\")\n",
    "\n",
    "        \n",
    "    \n",
    "class LabelEncoders:\n",
    "    def __init__(self):\n",
    "        self.encoders = {}\n",
    "    \n",
    "    def fit(self, df):\n",
    "        for c in df:\n",
    "            if df[c].dtype.name == \"object\":\n",
    "                enc = self.encoders.get(c, LabelEncoder())\n",
    "                enc.fit(df[c])\n",
    "                self.encoders[c] = enc\n",
    "                \n",
    "    def transform(self, df):\n",
    "        for c in df:\n",
    "            if c in self.encoders:\n",
    "                df[c] = self.encoders[c].transform(df[c])\n",
    "        return df\n",
    "    \n",
    "    def fit_one(self, s):\n",
    "        enc = self.encoders.get(s.name, LabelEncoder())\n",
    "        enc.fit(s)\n",
    "        self.encoders[s.name] = enc\n",
    "        \n",
    "    def transform_one(self, s):\n",
    "        if s.name in self.encoders:\n",
    "            return self.encoders[s.name].transform(s)\n",
    "        else:\n",
    "            return s\n",
    "\n",
    "    def fit_transform(self, df):\n",
    "        self.fit(df)\n",
    "        return self.transform(df)\n",
    "                \n",
    "    def fit_transform_one(self, s):\n",
    "        self.fit_one(s)\n",
    "        return self.transform_one(s)\n",
    "\n",
    "    def patch(self, target: str = \"position_2\"):\n",
    "        # nanの時とNoneの時がある。featherでのシリアライズを噛ませているかどうかで変わってしまう。ここでNoneに合わせる。\n",
    "        try:\n",
    "            if np.isnan(self.encoders[target].classes_[-1]):\n",
    "                self.encoders[target].classes_[-1] = None  # nan to None (patch)\n",
    "        except Exception:\n",
    "            print(self.encoders[target].classes_[-1])\n",
    "            print(type(self.encoders[target].classes_[-1]))\n",
    "\n",
    "    \n",
    "def cast_player_id(df):\n",
    "    # RAM消費を減らしたいので、Gを-1に置換して整数で持つ。\n",
    "    if \"nfl_player_id_2\" in df.columns:\n",
    "        df.loc[df[\"nfl_player_id_2\"] == \"G\", \"nfl_player_id_2\"] = \"-1\"\n",
    "        \n",
    "    for c in [\"nfl_player_id\", \"nfl_player_id_1\", \"nfl_player_id_2\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "def reduce_dtype(df):\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype.name == \"float64\":\n",
    "            df[c] = df[c].astype(np.float32)\n",
    "    return df\n",
    "\n",
    "\n",
    "def distance(x1, y1, x2, y2):\n",
    "    return np.sqrt(np.square(x2 - x1) + np.square(y2 - y1))\n",
    "    \n",
    "    \n",
    "def add_bbox_move_features(df):\n",
    "    shifts = [1]\n",
    "\n",
    "    for shift in shifts:\n",
    "        for view in [\"Endzone\", \"Sideline\"]:\n",
    "            for pl in [\"1\", \"2\"]:\n",
    "                for xy in [\"x\", \"y\"]:\n",
    "                    df[f\"bbox_center_{xy}_{view}_{pl}_m{shift}\"] = df.groupby([\"game_play\", \"nfl_player_id_1\", \"nfl_player_id_2\"])[f\"bbox_center_{xy}_{view}_{pl}\"].shift(shift)\n",
    "                    df[f\"bbox_center_{xy}_{view}_{pl}_p{shift}\"] = df.groupby([\"game_play\", \"nfl_player_id_1\", \"nfl_player_id_2\"])[f\"bbox_center_{xy}_{view}_{pl}\"].shift(-shift)\n",
    "\n",
    "                df[f\"bbox_move_{view}_{pl}_m{shift}\"] = distance(\n",
    "                    df[f\"bbox_center_x_{view}_{pl}_m{shift}\"],\n",
    "                    df[f\"bbox_center_y_{view}_{pl}_m{shift}\"],\n",
    "                    df[f\"bbox_center_x_{view}_{pl}\"],\n",
    "                    df[f\"bbox_center_y_{view}_{pl}\"],\n",
    "                )\n",
    "                df[f\"bbox_move_{view}_{pl}_p{shift}\"] = distance(\n",
    "                    df[f\"bbox_center_x_{view}_{pl}_p{shift}\"],\n",
    "                    df[f\"bbox_center_y_{view}_{pl}_p{shift}\"],\n",
    "                    df[f\"bbox_center_x_{view}_{pl}\"],\n",
    "                    df[f\"bbox_center_y_{view}_{pl}\"],\n",
    "                )\n",
    "\n",
    "                for xy in [\"x\", \"y\"]:\n",
    "                    del df[f\"bbox_center_{xy}_{view}_{pl}_m{shift}\"]\n",
    "                    del df[f\"bbox_center_{xy}_{view}_{pl}_p{shift}\"]\n",
    "\n",
    "            df[f\"bbox_move_{view}_m{shift}_mean_1\"] = df.groupby([\"nfl_player_id_1\", \"game_play\"])[f\"bbox_move_{view}_1_m{shift}\"].transform(\"mean\")\n",
    "            df[f\"bbox_move_{view}_m{shift}_max_1\"] = df.groupby([\"nfl_player_id_1\", \"game_play\"])[f\"bbox_move_{view}_1_m{shift}\"].transform(\"max\")\n",
    "\n",
    "    return df\n",
    "\n",
    "    \n",
    "def merge_helmet(df, helmet, meta):\n",
    "    start_times = meta[[\"game_play\", \"start_time\"]].drop_duplicates()\n",
    "    start_times[\"start_time\"] = pd.to_datetime(start_times[\"start_time\"])\n",
    "\n",
    "    helmet = pd.merge(helmet,\n",
    "                      start_times,\n",
    "                      on=\"game_play\",\n",
    "                      how=\"left\")\n",
    "    \n",
    "    # 追加\n",
    "    helmet['xmin'] = helmet['left']\n",
    "    helmet['ymin'] = helmet['top']\n",
    "    helmet['xmax'] = helmet['left'] + helmet['width']\n",
    "    helmet['ymax'] = helmet['top'] + helmet['height']\n",
    "    bbox = np.array([helmet['xmin'], \n",
    "                     helmet['ymin'], \n",
    "                     helmet['xmax'], \n",
    "                     helmet['ymax']]).T\n",
    "    helmet['bbox_center_x'] = (bbox[:, 0] + bbox[:, 2]) / 2\n",
    "    helmet['bbox_center_y'] = (bbox[:, 1] + bbox[:, 3]) / 2\n",
    "    \n",
    "    \n",
    "    fps=59.94\n",
    "    helmet[\"datetime\"] = helmet[\"start_time\"] + pd.to_timedelta(helmet[\"frame\"] * (1 / fps), unit=\"s\")\n",
    "    helmet[\"datetime\"] = pd.to_datetime(helmet[\"datetime\"], utc=True)\n",
    "    helmet[\"datetime_ngs\"] = pd.DatetimeIndex(helmet[\"datetime\"] + pd.to_timedelta(50, \"ms\")).floor(\"100ms\").values\n",
    "    helmet[\"datetime_ngs\"] = pd.to_datetime(helmet[\"datetime_ngs\"], utc=True)\n",
    "    df[\"datetime_ngs\"] = pd.to_datetime(df[\"datetime\"], utc=True)\n",
    "    \n",
    "    # 追加\n",
    "    feature_cols = [\"width\", \"height\",  \"bbox_center_x\", \"bbox_center_y\",\n",
    "                    \"bbox_center_x_std\", \"bbox_center_y_std\"]\n",
    "    helmet_agg = helmet.groupby([\"datetime_ngs\", \"nfl_player_id\", \"view\"]).agg(\n",
    "        width=pd.NamedAgg(\"width\", \"mean\"),\n",
    "        height=pd.NamedAgg(\"height\", \"mean\"),\n",
    "        bbox_center_x=pd.NamedAgg(\"bbox_center_x\", \"mean\"),\n",
    "        bbox_center_y=pd.NamedAgg(\"bbox_center_y\", \"mean\"),\n",
    "        bbox_center_x_std=pd.NamedAgg(\"bbox_center_x\", \"std\"),\n",
    "        bbox_center_y_std=pd.NamedAgg(\"bbox_center_y\", \"std\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    for view in [\"Sideline\", \"Endzone\"]:\n",
    "        helmet_ = helmet_agg[helmet_agg[\"view\"]==view].drop(\"view\", axis=1)\n",
    "    \n",
    "        helmet_global = helmet[helmet[\"view\"]==view].groupby(\"datetime_ngs\").agg(\n",
    "        **{\n",
    "            f\"width_{view}_mean\": pd.NamedAgg(\"width\", \"mean\"),\n",
    "            f\"height_{view}_mean\": pd.NamedAgg(\"height\", \"mean\"),\n",
    "            f\"{view}_count\": pd.NamedAgg(\"width\", \"count\"),\n",
    "        }).reset_index()\n",
    "        \n",
    "        helmet_global[f\"aspect_{view}_mean\"] = helmet_global[f\"height_{view}_mean\"] / helmet_global[f\"width_{view}_mean\"]\n",
    "\n",
    "        for postfix in [\"_1\", \"_2\"]:\n",
    "            column_renames = {c: f\"{c}_{view}{postfix}\" for c in feature_cols}\n",
    "            column_renames[\"nfl_player_id\"] = f\"nfl_player_id{postfix}\"\n",
    "            df = pd.merge(\n",
    "                df,\n",
    "                helmet_.rename(columns=column_renames),\n",
    "                on=[\"datetime_ngs\", f\"nfl_player_id{postfix}\"],\n",
    "                how=\"left\"\n",
    "            )\n",
    "            \n",
    "\n",
    "        df = pd.merge(\n",
    "            df,\n",
    "            helmet_global,\n",
    "            on=\"datetime_ngs\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "    \n",
    "    # del df[\"datetime_ngs\"] 残しといてもらえると嬉しい…。CNN終了後に消します。\n",
    "    # del df[\"datetime\"] # 0219 merge helmet smoothに使用するためキープ\n",
    "    \n",
    "    df = add_bbox_move_features(df)\n",
    "    \n",
    "    return reduce_dtype(df)\n",
    "\n",
    "# 0219 update. カルマンスムーズ後のデータマージ。\n",
    "def merge_helmet_smooth(df, helmet, meta):\n",
    "    start_times = meta[[\"game_play\", \"start_time\"]].drop_duplicates()\n",
    "    start_times[\"start_time\"] = pd.to_datetime(start_times[\"start_time\"])\n",
    "\n",
    "    helmet = pd.merge(helmet,\n",
    "                      start_times,\n",
    "                      on=\"game_play\",\n",
    "                      how=\"left\")\n",
    "    \n",
    "    fps=59.94\n",
    "    helmet[\"datetime\"] = helmet[\"start_time\"] + pd.to_timedelta(helmet[\"frame\"] * (1 / fps), unit=\"s\")\n",
    "    helmet[\"datetime\"] = pd.to_datetime(helmet[\"datetime\"], utc=True)\n",
    "    helmet[\"datetime_ngs\"] = pd.DatetimeIndex(helmet[\"datetime\"] + pd.to_timedelta(50, \"ms\")).floor(\"100ms\").values\n",
    "    helmet[\"datetime_ngs\"] = pd.to_datetime(helmet[\"datetime_ngs\"], utc=True)\n",
    "    df[\"datetime_ngs\"] = pd.to_datetime(df[\"datetime\"], utc=True)\n",
    "    \n",
    "    feature_cols = [\"bbox_smooth_velx\",\"bbox_smooth_vely\",\n",
    "                    \"bbox_smooth_x\",\"bbox_smooth_y\",\n",
    "                    \"bbox_smooth_outlier\",\n",
    "                    \"bbox_smooth_accx\",\"bbox_smooth_accy\"]\n",
    "    \n",
    "    helmet_agg = helmet.groupby([\"datetime_ngs\", \"nfl_player_id\", \"view\"]).agg({c: \"mean\" for c in feature_cols}).reset_index()\n",
    "\n",
    "    for view in [\"Sideline\", \"Endzone\"]:\n",
    "        helmet_ = helmet_agg[helmet_agg[\"view\"]==view].drop(\"view\", axis=1)\n",
    "    \n",
    "        for postfix in [\"_1\", \"_2\"]:\n",
    "            column_renames = {c: f\"{c}_{view}{postfix}\" for c in feature_cols}\n",
    "            column_renames[\"nfl_player_id\"] = f\"nfl_player_id{postfix}\"\n",
    "            df = pd.merge(\n",
    "                df,\n",
    "                helmet_.rename(columns=column_renames),\n",
    "                on=[\"datetime_ngs\", f\"nfl_player_id{postfix}\"],\n",
    "                how=\"left\"\n",
    "            )\n",
    "\n",
    "    try:\n",
    "        del df[\"datetime\"]\n",
    "    except:\n",
    "        pass\n",
    "    return reduce_dtype(df)\n",
    "\n",
    "\n",
    "def read_csv_with_cache(filename, cfg: Config, usecols=None):\n",
    "    cache_filename = filename.split(\".\")[0] + \".f\"\n",
    "    cache_path = os.path.join(cfg.CACHE_DIR, cache_filename)\n",
    "    if not os.path.exists(cache_path):\n",
    "        df = pd.read_csv(os.path.join(cfg.INPUT, filename), usecols=usecols)\n",
    "        df = reduce_dtype(cast_player_id(df))\n",
    "        df.to_feather(cache_path)\n",
    "    return pd.read_feather(cache_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d82826",
   "metadata": {
    "papermill": {
     "duration": 0.105789,
     "end_time": "2023-02-24T23:49:55.601769",
     "exception": false,
     "start_time": "2023-02-24T23:49:55.495980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "class KalmanFilter:\n",
    "    def __init__(self, \n",
    "                 num_sensor=2, \n",
    "                 timestep=0.02, \n",
    "                 acc_band=1, # change of xx pix/s during timestep. (unit can be changed)\n",
    "                 sensor_errors=[5, 5], # measurement error(pix)\n",
    "                 initial_state=[0,0,0,0], # x pix, vx pix/s, y pix, vy pix/s\n",
    "                 clip_dist=20,\n",
    "                 outlier_dist=100,\n",
    "                 \n",
    "                 ):\n",
    "        if len(sensor_errors)!=num_sensor:\n",
    "            raise Exception(\"len(sensor_errors) must be same as the num_sensor\")\n",
    "        self.state_dim = 4\n",
    "        self.num_sensor = num_sensor\n",
    "        self.timestep = timestep\n",
    "        self.acc_band = acc_band\n",
    "        self.sensor_errors = sensor_errors\n",
    "        self.initial_state = np.array(initial_state)#.reshape(2,1)\n",
    "        self.base_clip_dist = clip_dist\n",
    "        self.base_outlier_dist = outlier_dist\n",
    "        self.initial_cov_ratio = 100\n",
    "        self.current_cov_ratio = self.initial_cov_ratio\n",
    "        self.initialize_matrix()\n",
    "    \n",
    "    def initialize_matrix(self):\n",
    "        # Motion Model\n",
    "        self.A = np.array([[1, self.timestep, 0, 0],# x, vx, y, vy\n",
    "                           [0, 1, 0, 0],\n",
    "                           [0, 0, 1, self.timestep],\n",
    "                           [0, 0, 0, 1]])\n",
    "        # Jacobian of Observation Model\n",
    "        self.C = np.array([[1,0,0,0],\n",
    "                            [0,0,1,0]]) # x and y\n",
    "        self.base_Q = self.get_cov_mat_self() # Covariance Matrix for motion\n",
    "        self.base_R = self.get_cov_mat_sensor() # Covariance Matrix for observation\n",
    "        self.Q = self.base_Q.copy() * self.initial_cov_ratio\n",
    "        self.R = self.base_R.copy() * self.initial_cov_ratio\n",
    "        self.outlier_dist = self.base_outlier_dist * self.initial_cov_ratio\n",
    "        self.clip_dist = self.base_clip_dist * self.initial_cov_ratio\n",
    "        self.x = self.initial_state\n",
    "        self.P = np.eye(self.state_dim)\n",
    "    \n",
    "    def get_cov_mat_sensor(self):\n",
    "        R = np.diag([e**2 for e in self.sensor_errors])\n",
    "        return R\n",
    "        \n",
    "    def get_cov_mat_self(self):\n",
    "        #Q = np.diag([self.acc_band**2, self.acc_band**2])\n",
    "        Q = np.diag([0, self.acc_band**2, 0, self.acc_band**2])\n",
    "        return Q\n",
    "    \n",
    "    def update_cov(self):\n",
    "        reduction_ratio = 2\n",
    "        self.current_cov_ratio = self.current_cov_ratio / reduction_ratio\n",
    "        if self.current_cov_ratio > 1:\n",
    "            self.Q = self.Q / reduction_ratio\n",
    "            self.R = self.R / reduction_ratio\n",
    "            self.outlier_dist = self.outlier_dist / reduction_ratio\n",
    "            self.clip_dist = self.clip_dist / reduction_ratio\n",
    "        else:\n",
    "            self.Q = self.base_Q\n",
    "            self.R = self.base_R\n",
    "            self.outlier_dist = self.base_outlier_dist\n",
    "            self.clip_dist = self.base_clip_dist\n",
    "    \n",
    "    def prior_estimate(self):\n",
    "        \"\"\"\n",
    "        prior prediction of state\n",
    "        and covariant matrix of prior prediction\n",
    "        \"\"\"\n",
    "        x_prior = np.matmul(self.A, self.x) # 現在のstateと運動モデルから次のstateを予測\n",
    "        P_prior = np.matmul(self.A, np.matmul(self.P, self.A.T)) + self.Q # motionの共分散にここまでの状態量の共分散を上乗せ、事前予測の共分散を得る\n",
    "        return x_prior, P_prior\n",
    "    \n",
    "    def update(self, x_prior, P_prior, sensor_vals):\n",
    "        \"\"\"\n",
    "        update\n",
    "        - kalman gain\n",
    "        - state\n",
    "        - cov matrix\n",
    "        \"\"\"\n",
    "        S = np.matmul(self.C, np.matmul(P_prior, self.C.T)) + self.R\n",
    "        K = np.matmul(np.matmul(P_prior, self.C.T), np.linalg.inv(S))\n",
    "        x = x_prior + np.matmul(K, sensor_vals - np.matmul(self.C, x_prior))\n",
    "        P = np.matmul((np.eye(self.state_dim) - np.matmul(K, self.C)), P_prior)\n",
    "        self.update_cov()\n",
    "        return x, P\n",
    "    \n",
    "    def clip_by_dist(self, pos_prior, pos):\n",
    "        delta = pos - pos_prior\n",
    "        dist = math.sqrt(delta[0]**2 + delta[1]**2)\n",
    "        # dist = np.sqrt((delta**2).sum())\n",
    "        is_outlier = dist > self.outlier_dist\n",
    "        if dist > self.clip_dist:\n",
    "            pos = delta * min(dist, self.clip_dist) / (dist + 1e-12) + pos_prior\n",
    "        if is_outlier:\n",
    "            self.outlier_dist = self.outlier_dist + self.base_clip_dist\n",
    "            self.clip_dist = self.clip_dist + self.base_clip_dist\n",
    "        else:\n",
    "            self.outlier_dist = max(self.outlier_dist - self.clip_dist, self.base_outlier_dist)\n",
    "            self.clip_dist = max(self.clip_dist - self.clip_dist, self.base_clip_dist)            \n",
    "        return pos, is_outlier\n",
    "        \n",
    "    def __call__(self, sensor_vals):\n",
    "        x_prior, P_prior = self.prior_estimate()\n",
    "        if np.isnan(sensor_vals[0]):\n",
    "            self.x, self.P = x_prior, P_prior\n",
    "        else:\n",
    "            sensor_vals_clip, is_outlier = self.clip_by_dist(x_prior[::2], sensor_vals)\n",
    "            if is_outlier:\n",
    "                self.x, self.P = x_prior, P_prior\n",
    "            else:\n",
    "                self.x, self.P = self.update(x_prior, P_prior, sensor_vals_clip)\n",
    "        return self.x\n",
    "    \n",
    "    def smooth_step(self, p_prior, p, x_prior, x):\n",
    "        C = np.matmul(p, self.A.T).dot(np.linalg.inv(p_prior))\n",
    "        x_smooth = x + C.dot(self.x - x_prior)#.squeeze()\n",
    "        p_smooth = p + C.dot(self.P - p_prior).dot(C.T)\n",
    "        return x_smooth, p_smooth\n",
    "    \n",
    "    def smoother(self, sequence_sensor_vals):\n",
    "        x_priors = [self.x] # initial_value\n",
    "        p_priors = [self.P]\n",
    "        xs = [self.x]\n",
    "        ps = [self.P]\n",
    "        is_outlier_predicted = [0]\n",
    "        for sensor_vals in sequence_sensor_vals[:]:#[1:]\n",
    "            x_prior, P_prior = self.prior_estimate()\n",
    "            if np.isnan(sensor_vals[0]):\n",
    "                self.x, self.P = x_prior, P_prior\n",
    "                is_outlier_predicted += [1]\n",
    "            else:\n",
    "                sensor_vals_clip, is_outlier = self.clip_by_dist(x_prior[::2], sensor_vals)\n",
    "                if is_outlier:\n",
    "                    self.x, self.P = x_prior, P_prior\n",
    "                    is_outlier_predicted += [1]\n",
    "                else:\n",
    "                    self.x, self.P = self.update(x_prior, P_prior, sensor_vals_clip)\n",
    "                    is_outlier_predicted += [0]\n",
    "            # self.x, self.P = self.update(x_prior, P_prior, sensor_vals)\n",
    "            x_priors.append(x_prior)\n",
    "            p_priors.append(P_prior)\n",
    "            xs.append(self.x)\n",
    "            ps.append(self.P)\n",
    "        xs_smooth = [self.x]\n",
    "        ps_smooth = [self.P]\n",
    "        for i in reversed(range(len(sequence_sensor_vals))):#-1\n",
    "            self.x, self.P = self.smooth_step(p_priors[i+1], ps[i], x_priors[i+1], xs[i])\n",
    "            xs_smooth.append(self.x)\n",
    "            ps_smooth.append(self.P)\n",
    "        xs_smooth = xs_smooth[::-1]\n",
    "        ps_smooth = ps_smooth[::-1]\n",
    "        return xs_smooth[1:], is_outlier_predicted[1:]\n",
    "    \n",
    "def apply_smoother_split(df_single_player, \n",
    "                         num_split_threshold_no_detect=20, # outlierも線引きしたほうがいいかもな。 \n",
    "                         draw_output=False):\n",
    "    xy_sequence = df_single_player[[\"cx\", \"cy\"]].values\n",
    "    frame_no_detected = df_single_player[\"frame\"].values\n",
    "    frame_no_to_xy = {fn: xy for fn,xy in zip(frame_no_detected, xy_sequence)}\n",
    "    frame_no_min = frame_no_detected.min()\n",
    "    frame_no_max = frame_no_detected.max()\n",
    "    frame_range_all = np.arange(frame_no_min, frame_no_max+1)\n",
    "    xy_sequence_all = np.array([frame_no_to_xy[fn] if fn in frame_no_detected else [np.nan, np.nan] for fn in frame_range_all])\n",
    "    split_frame_range = []\n",
    "    split_xy_sequence = []\n",
    "    first_time_detected = 0\n",
    "    last_time_detected = 0\n",
    "    unseen_duration = 0\n",
    "    missed = False\n",
    "    for idx, f_no in enumerate(frame_range_all):\n",
    "        if f_no in frame_no_detected:\n",
    "            last_time_detected = idx\n",
    "            if missed:\n",
    "                first_time_detected = idx\n",
    "            missed = False\n",
    "            unseen_duration = 0\n",
    "        else:\n",
    "            unseen_duration += 1\n",
    "            if missed:\n",
    "                continue\n",
    "            if unseen_duration >= num_split_threshold_no_detect:\n",
    "                missed = True\n",
    "                split_xy_sequence += [xy_sequence_all[first_time_detected:last_time_detected+1]]\n",
    "                split_frame_range += [frame_range_all[first_time_detected:last_time_detected+1]]\n",
    "            continue\n",
    "    split_xy_sequence += [xy_sequence_all[first_time_detected:]]\n",
    "    split_frame_range += [frame_range_all[first_time_detected:]]\n",
    "    split_smoothed_pos = []\n",
    "    split_smoothed_vel = []\n",
    "    split_outlier_predicted = []\n",
    "    for xy_sequence, frame_range in zip(split_xy_sequence, split_frame_range):\n",
    "        initial_xyz = xy_sequence[~np.isnan(xy_sequence[:,0])][:3].mean(axis=0)\n",
    "        kf = KalmanFilter(num_sensor=2,\n",
    "                         timestep=1/60, \n",
    "                         acc_band=25, # change of N pix/s during timestep\n",
    "                         sensor_errors=[5, 5], # measurement error(pix)\n",
    "                         initial_state=[initial_xyz[0],0,initial_xyz[1],0], # x pix, vx pix/s, y pix, vy pix/s\n",
    "                         clip_dist=20,\n",
    "                         outlier_dist=100,\n",
    "                         )\n",
    "\n",
    "        # kalman smoother\n",
    "        kf.initialize_matrix()\n",
    "        smoothed, outlier_predicted = kf.smoother(xy_sequence)\n",
    "        split_smoothed_pos.append(np.array(smoothed)[:,[0,2]])\n",
    "        split_smoothed_vel.append(np.array(smoothed)[:,[1,3]])\n",
    "        split_outlier_predicted.append(np.array(outlier_predicted))\n",
    "    \n",
    "    smoothed_pos = np.concatenate(split_smoothed_pos, axis=0)\n",
    "    smoothed_vel = np.concatenate(split_smoothed_vel, axis=0)\n",
    "    outlier_predicted = np.concatenate(split_outlier_predicted, axis=0)\n",
    "    frame_range = np.concatenate(split_frame_range, axis=0)\n",
    "    xy_sequence = np.concatenate(split_xy_sequence, axis=0)\n",
    "    \n",
    "    if draw_output:\n",
    "        #plt.figure(figsize=[5,15])\n",
    "        figs, axes = plt.subplots(1, 2, figsize=[15,3.5])\n",
    "        axes[0].scatter(xy_sequence[:,0], xy_sequence[:,1], c=np.arange(len(xy_sequence)), s=35, vmin=0, vmax=len(xy_sequence))\n",
    "        axes[0].plot(xy_sequence[:,0], xy_sequence[:,1])\n",
    "        axes[0].invert_yaxis()\n",
    "        axes[0].grid()\n",
    "        axes[0].set_title(f\" no postprocess\")\n",
    "\n",
    "        axes[1].scatter(smoothed_pos[:,0], smoothed_pos[:,1], c=np.arange(len(smoothed_pos)), s=35, vmin=0, vmax=len(smoothed_pos))\n",
    "        axes[1].plot(smoothed_pos[:,0], smoothed_pos[:,1])\n",
    "        axes[1].invert_yaxis()\n",
    "        axes[1].grid()\n",
    "        axes[1].set_title(f\" kalman smooth\")\n",
    "        plt.show()\n",
    "    return smoothed_pos, smoothed_vel, outlier_predicted, frame_range\n",
    "\n",
    "def run_smoother_on_helmets(phase=\"train\", path=None, num_split_threshold_no_detect = 10):\n",
    "    if path is not None:\n",
    "        if os.path.exists(path):\n",
    "            df_smoothed = pd.read_csv(path)\n",
    "            if 'Unnamed: 0' in df_smoothed.columns:\n",
    "                df_smoothed = df_smoothed.drop(columns=['Unnamed: 0'])\n",
    "            return df_smoothed\n",
    "    \n",
    "    draw_output = False\n",
    "    \n",
    "    df_helmet = pd.read_csv(f\"{cfg.INPUT}/{phase}_baseline_helmets.csv\")\n",
    "    df_helmet[\"cx\"] = df_helmet[\"left\"] + df_helmet[\"width\"] / 2\n",
    "    df_helmet[\"cy\"] = df_helmet[\"top\"] + df_helmet[\"height\"] / 2\n",
    "    \n",
    "    df_smoothed = []\n",
    "    print(\"start kalman smooth.\")\n",
    "    for game_play in tqdm(df_helmet[\"game_play\"].unique()):\n",
    "        #print(\"GamePlay: \", game_play)\n",
    "        # print(f\"\\r GamePlay {game_play}\",end=\"\")\n",
    "        for view in [\"Sideline\", \"Endzone\"]:\n",
    "            df_gp = df_helmet.query(\"game_play == @game_play and view == @view\")\n",
    "            smoothed_positions = []\n",
    "            df_smoothed_velocities = []\n",
    "            df_smoothed_gp = []\n",
    "            vel_column_names = []\n",
    "            for p_idx, nfl_player_id in enumerate(df_gp[\"nfl_player_id\"].unique()):\n",
    "                df_single_player = df_gp[df_gp[\"nfl_player_id\"]==nfl_player_id]\n",
    "                smoothed_pos, smoothed_vel, outlier_predicted, frame_range = apply_smoother_split(df_single_player, num_split_threshold_no_detect=num_split_threshold_no_detect, draw_output = draw_output)\n",
    "\n",
    "                df_vel_player = pd.DataFrame(smoothed_vel, columns=[f\"bbox_smooth_velx\",f\"bbox_smooth_vely\"], index=frame_range)\n",
    "                df_vel_player[[\"bbox_smooth_x\",\"bbox_smooth_y\"]] = smoothed_pos\n",
    "                df_vel_player[\"bbox_smooth_outlier\"] = outlier_predicted\n",
    "                df_vel_player.loc[frame_range[:-1], [f\"bbox_smooth_accx\",f\"bbox_smooth_accy\"]] = smoothed_vel[1:] - smoothed_vel[:-1]\n",
    "                df_vel_player[\"nfl_player_id\"] = nfl_player_id\n",
    "                df_smoothed_gp += [df_vel_player]\n",
    "\n",
    "            df_smoothed_gp = pd.concat(df_smoothed_gp, axis=0).reset_index().rename(columns={\"index\":\"frame\"})\n",
    "            df_smoothed_gp[\"view\"] = view\n",
    "            df_smoothed_gp[\"game_play\"] = game_play\n",
    "\n",
    "            df_smoothed.append(df_smoothed_gp)\n",
    "    df_smoothed = pd.concat(df_smoothed, axis=0).reset_index(drop=True)\n",
    "    df_smoothed[[\"bbox_smooth_x\",\"bbox_smooth_y\"]] = df_smoothed[[\"bbox_smooth_x\",\"bbox_smooth_y\"]].astype(np.float32)\n",
    "    df_smoothed[[\"bbox_smooth_velx\",\"bbox_smooth_vely\"]] = df_smoothed[[\"bbox_smooth_velx\",\"bbox_smooth_vely\"]].astype(np.float32)\n",
    "    df_smoothed[[\"bbox_smooth_accx\",\"bbox_smooth_accy\"]] = df_smoothed[[\"bbox_smooth_accx\",\"bbox_smooth_accy\"]].astype(np.float32)\n",
    "\n",
    "    if path is not None:\n",
    "        df_smoothed.to_csv(path, index=False)\n",
    "    return df_smoothed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a0ea63",
   "metadata": {
    "papermill": {
     "duration": 0.051736,
     "end_time": "2023-02-24T23:49:55.714795",
     "exception": false,
     "start_time": "2023-02-24T23:49:55.663059",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### bbox features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9257bea",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.083422,
     "end_time": "2023-02-24T23:49:55.854708",
     "exception": false,
     "start_time": "2023-02-24T23:49:55.771286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "## 畑追加箇所\n",
    "\n",
    "def create_bbox_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for view in [\"Sideline\", \"Endzone\"]:\n",
    "        df = add_bbox_center_distance(df, view)\n",
    "        df = add_bbox_from_step0_feature(df, view)\n",
    "        df = add_shift_feature(df, view)\n",
    "        df = add_diff_features(df, view)\n",
    "        df = add_agg_bbox_feature(df, view)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_agg_bbox_feature(df:pd.DataFrame, view:str) -> pd.DataFrame:\n",
    "    for agg in ['min', 'mean', 'std']:\n",
    "        df[f'bbox_center_{view}_distance_{agg}'] = df.groupby(['game_play', 'nfl_player_id_1', 'nfl_player_id_2'])[f'bbox_center_{view}_distance'].transform(agg)\n",
    "        df[f'bbox_center_y_{view}_1_{agg}'] = df.groupby(['game_play', 'nfl_player_id_1'])[f'bbox_center_y_{view}_1'].transform(agg)\n",
    "        df[f'bbox_center_y_std_{view}_1_{agg}'] = df.groupby(['game_play', 'nfl_player_id_1'])[f'bbox_center_y_std_{view}_1'].transform(agg)\n",
    "        \n",
    "    df[f'bbox_center_{view}_distance_ratio'] = df[f'bbox_center_{view}_distance_min'] / df[f'bbox_center_{view}_distance'] \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_bbox_center_distance(df:pd.DataFrame, view:str) -> pd.DataFrame:\n",
    "    '''\n",
    "    二人のplayerの中心間距離\n",
    "    '''\n",
    "    df[f'bbox_center_{view}_distance'] = distance(df[f'bbox_center_x_{view}_1'], \n",
    "                                                  df[f'bbox_center_y_{view}_1'], \n",
    "                                                  df[f'bbox_center_x_{view}_2'],\n",
    "                                                  df[f'bbox_center_y_{view}_2'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_bbox_from_step0_feature(df:pd.DataFrame, view:str) -> pd.DataFrame:\n",
    "    for postfix in [\"1\", \"2\"]:\n",
    "        step0_columns = [f'bbox_center_x_{view}_{postfix}', \n",
    "                         f'bbox_center_y_{view}_{postfix}']\n",
    "        df = add_bbox_step0(df, step0_columns, postfix)\n",
    "        df = add_distance_step0(df, step0_columns, view, postfix)\n",
    "    return df\n",
    "\n",
    "def add_bbox_step0(df:pd.DataFrame, step0_columns:List[str], postfix:str) -> pd.DataFrame:\n",
    "    merge_key_columns =[\"game_play\", f\"nfl_player_id_{postfix}\"]\n",
    "        \n",
    "    use_columns = deepcopy(merge_key_columns)\n",
    "    use_columns.extend(step0_columns)\n",
    "        \n",
    "    _df = df[df['step']==0].copy()\n",
    "    _df = _df[use_columns].drop_duplicates()\n",
    "        \n",
    "    rename_columns = [i+'_start' for i in step0_columns]\n",
    "        \n",
    "    _df.rename(columns=dict(zip(step0_columns, rename_columns)), \n",
    "                         inplace=True)\n",
    "    \n",
    "    df = pd.merge(df, \n",
    "                  _df, \n",
    "                  how='left', \n",
    "                  on=merge_key_columns)\n",
    "    return df\n",
    "    \n",
    "def add_distance_step0(df: pd.DataFrame, step0_columns:List[str], view, postfix) -> pd.DataFrame:\n",
    "        \n",
    "    for column in step0_columns:\n",
    "        df['diff_step_0_' + column] = df[column] - df[column + '_start']\n",
    "        df['diff_step_0_' + column] = df[column] - df[column + '_start']\n",
    "            \n",
    "    df[f'distance_from_step0_{view}_{postfix}'] = distance(df[step0_columns[0]], \n",
    "                                                           df[step0_columns[1]],\n",
    "                                                           df[step0_columns[0] + '_start'],\n",
    "                                                           df[step0_columns[1] + '_start'])\n",
    "    \n",
    "    # 邪魔なので削除する\n",
    "    df.drop(columns=[i + '_start' for i in step0_columns], inplace=True)\n",
    "        \n",
    "    return df\n",
    "    \n",
    "def add_shift_feature(df: pd.DataFrame, view: str) -> pd.DataFrame:\n",
    "    result = []\n",
    "\n",
    "    shift_columns = [f'distance_from_step0_{view}_1',\n",
    "                     f'distance_from_step0_{view}_2',\n",
    "                     f'bbox_center_{view}_distance',\n",
    "                     f'bbox_center_y_{view}_1',\n",
    "                     f'bbox_center_y_std_{view}_1']\n",
    "    for i in [-5, -3, -1, 1, 3, 5]:\n",
    "\n",
    "        _tmp = df.groupby(['game_play', 'nfl_player_id_1', 'nfl_player_id_2'])[shift_columns].shift(i).add_prefix(f'shift_{i}_')\n",
    "        result.append(_tmp)\n",
    "        \n",
    "    result = pd.concat(result, axis=1)\n",
    "    df = pd.concat([df, result], axis=1)\n",
    "    return df\n",
    "\n",
    "def add_diff_feature(df: pd.DataFrame, column_name: str, shift_amount:List[int]) -> pd.DataFrame:\n",
    "    for shift in shift_amount:\n",
    "        df[f'diff_{shift}_0_{column_name}'] = df[column_name] - df[f'shift_{shift}_{column_name}'] \n",
    "    return df\n",
    "\n",
    "def add_diff_features(df: pd.DataFrame, view:str) -> pd.DataFrame:\n",
    "    shift_columns = [f'distance_from_step0_{view}_1',\n",
    "                     f'distance_from_step0_{view}_2',\n",
    "                     f'bbox_center_{view}_distance',\n",
    "                     f'bbox_center_y_{view}_1',\n",
    "                     f'bbox_center_y_std_{view}_1']\n",
    "        \n",
    "    for column in shift_columns:\n",
    "        df = add_diff_feature(df, column, [-5, -3, -1, 1, 3, 5])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfab87b",
   "metadata": {
    "papermill": {
     "duration": 12.245108,
     "end_time": "2023-02-24T23:50:08.155231",
     "exception": false,
     "start_time": "2023-02-24T23:49:55.910123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expand_contact_id(df):\n",
    "    \"\"\"\n",
    "    Splits out contact_id into seperate columns.\n",
    "    \"\"\"\n",
    "    df[\"game_play\"] = df[\"contact_id\"].str[:12]\n",
    "    df[\"step\"] = df[\"contact_id\"].str.split(\"_\").str[-3].astype(\"int\")\n",
    "    df[\"nfl_player_id_1\"] = df[\"contact_id\"].str.split(\"_\").str[-2]\n",
    "    df[\"nfl_player_id_2\"] = df[\"contact_id\"].str.split(\"_\").str[-1]\n",
    "    return cast_player_id(df)\n",
    "\n",
    "\n",
    "def expand_helmet(cfg, df, phase=\"train\"):\n",
    "    helmet_cols = [\n",
    "        \"game_play\",\n",
    "        \"view\",\n",
    "        \"nfl_player_id\",\n",
    "        \"frame\",\n",
    "        \"left\",\n",
    "        \"width\",\n",
    "        \"top\",\n",
    "        \"height\"\n",
    "    ]\n",
    "    helmet = read_csv_with_cache(f\"{phase}_baseline_helmets.csv\", cfg, usecols=helmet_cols)\n",
    "    meta = read_csv_with_cache(f\"{phase}_video_metadata.csv\", cfg)\n",
    "    df = merge_helmet(df, helmet, meta)\n",
    "    return df\n",
    "\n",
    "# 0219 update\n",
    "# kalman smooth後のhelmet情報をマージ。このタイミングでsmoothing走る。\n",
    "def expand_helmet_smooth(cfg, df, phase=\"train\"):\n",
    "    if phase == \"train\":\n",
    "        helmet = run_smoother_on_helmets(phase=\"train\", \n",
    "                                         path=os.path.join(cfg.KMAT_PATH, f\"output/{phase}_baseline_helmet_smooth.csv\"),\n",
    "                                         num_split_threshold_no_detect = 10)\n",
    "    else:\n",
    "        helmet = run_smoother_on_helmets(phase=\"test\", \n",
    "                                         path=None, \n",
    "                                         num_split_threshold_no_detect = 10)\n",
    "    \n",
    "    meta = read_csv_with_cache(f\"{phase}_video_metadata.csv\", cfg)\n",
    "    df = merge_helmet_smooth(df, helmet, meta)\n",
    "    return df\n",
    "\n",
    "with timer(\"load file\"):\n",
    "    tracking_cols = [\n",
    "        \"game_play\",\n",
    "        \"nfl_player_id\",\n",
    "        \"datetime\",\n",
    "        \"step\",\n",
    "        \"team\",\n",
    "        \"position\",\n",
    "        \"x_position\",\n",
    "        \"y_position\",\n",
    "        \"speed\",\n",
    "        \"distance\",\n",
    "        \"direction\",\n",
    "        \"orientation\",\n",
    "        \"acceleration\",\n",
    "        \"sa\"\n",
    "    ]\n",
    "    train_cols = [\n",
    "        \"game_play\",\n",
    "        \"step\",\n",
    "        \"nfl_player_id_1\",\n",
    "        \"nfl_player_id_2\",\n",
    "        \"contact\",\n",
    "        \"datetime\"\n",
    "    ]\n",
    "    \n",
    "    if cfg.IS_TRAIN:\n",
    "        tr_tracking = read_csv_with_cache(\"train_player_tracking.csv\", cfg, usecols=tracking_cols)\n",
    "        train = read_csv_with_cache(\"train_labels.csv\", cfg, usecols=train_cols)\n",
    "        split_defs = pd.read_csv(f\"{cfg.KMAT_PATH}/output/game_fold.csv\")\n",
    "\n",
    "        \n",
    "    te_tracking = read_csv_with_cache(\"test_player_tracking.csv\", cfg, usecols=tracking_cols)\n",
    "    sub = read_csv_with_cache(\"sample_submission.csv\", cfg)\n",
    "    test = expand_contact_id(sub)\n",
    "    test = pd.merge(test, te_tracking[[\"step\", \"game_play\", \"datetime\"]].drop_duplicates(), on=[\"game_play\", \"step\"], how=\"left\")\n",
    "\n",
    "\n",
    "with timer(\"assign helmet metadata\"):\n",
    "    if cfg.IS_TRAIN:\n",
    "        train = expand_helmet(cfg, train, \"train\")\n",
    "        train = expand_helmet_smooth(cfg, train, \"train\")\n",
    "    test = expand_helmet(cfg, test, \"test\")\n",
    "    test = expand_helmet_smooth(cfg, test, \"test\")\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee67f18f",
   "metadata": {
    "papermill": {
     "duration": 0.063064,
     "end_time": "2023-02-24T23:50:08.273278",
     "exception": false,
     "start_time": "2023-02-24T23:50:08.210214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_cols(df, tr, use_cols):\n",
    "    key_cols = [\"nfl_player_id\", \"step\", \"game_play\"]\n",
    "    use_cols = [c for c in use_cols if c in tr.columns]\n",
    "    \n",
    "    dst = pd.merge(\n",
    "        df,\n",
    "        tr[key_cols+use_cols].rename(columns={c: c+\"_1\" for c in use_cols}),\n",
    "        left_on=[\"nfl_player_id_1\", \"step\", \"game_play\"],\n",
    "        right_on=key_cols,\n",
    "        how=\"left\"\n",
    "    ).drop(\"nfl_player_id\", axis=1)\n",
    "    dst = pd.merge(\n",
    "        dst,\n",
    "        tr[key_cols+use_cols].rename(columns={c: c+\"_2\" for c in use_cols}),\n",
    "        left_on=[\"nfl_player_id_2\", \"step\", \"game_play\"],\n",
    "        right_on=key_cols,\n",
    "        how=\"left\"\n",
    "    ).drop(\"nfl_player_id\", axis=1)\n",
    "    \n",
    "    return dst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c21e7e",
   "metadata": {
    "papermill": {
     "duration": 0.051643,
     "end_time": "2023-02-24T23:50:08.376835",
     "exception": false,
     "start_time": "2023-02-24T23:50:08.325192",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## basic features (nyanp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442e4ac8",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.073057,
     "end_time": "2023-02-24T23:50:08.502150",
     "exception": false,
     "start_time": "2023-02-24T23:50:08.429093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def angle_diff(s1, s2):\n",
    "    diff = s1 - s2\n",
    "    \n",
    "    return np.abs((diff + 180) % 360 - 180)\n",
    "\n",
    "def distance(x1, y1, x2, y2):\n",
    "    return np.sqrt(np.square(x2 - x1) + np.square(y2 - y1))\n",
    "\n",
    "\n",
    "def make_basic_features(df):\n",
    "    df[\"dx\"] = df[\"x_position_1\"] - df[\"x_position_2\"]\n",
    "    # df[\"dy\"] = df[\"y_position_1\"] - df[\"y_position_2\"]\n",
    "    df[\"distance\"] = distance(df[\"x_position_1\"], df[\"y_position_1\"], df[\"x_position_2\"], df[\"y_position_2\"])\n",
    "    df[\"different_team\"] = df[\"team_1\"] != df[\"team_2\"]\n",
    "    df[\"anglediff_dir1_dir2\"] = angle_diff(df[\"direction_1\"], df[\"direction_2\"])\n",
    "    df[\"anglediff_ori1_ori2\"] = angle_diff(df[\"orientation_1\"], df[\"orientation_2\"])\n",
    "    df[\"anglediff_dir1_ori1\"] = angle_diff(df[\"direction_1\"], df[\"orientation_1\"])\n",
    "    df[\"anglediff_dir2_ori2\"] = angle_diff(df[\"direction_2\"], df[\"orientation_2\"])\n",
    "    return df\n",
    "\n",
    "def tracking_prep(tracking):\n",
    "    for c in [\"direction\", \"orientation\", \"acceleration\", \"sa\", \"speed\", \"distance\"]:\n",
    "        tracking[f\"{c}_p1\"] = tracking.groupby([\"nfl_player_id\", \"game_play\"])[c].shift(-1)\n",
    "        tracking[f\"{c}_m1\"] = tracking.groupby([\"nfl_player_id\", \"game_play\"])[c].shift(1)\n",
    "        tracking[f\"{c}_p1_diff\"] = tracking[c] - tracking[f\"{c}_p1\"]\n",
    "        tracking[f\"{c}_m1_diff\"] = tracking[f\"{c}_m1\"] - tracking[c]\n",
    "        \n",
    "        if c in [\"direction\", \"orientation\"]:\n",
    "            tracking[f\"{c}_p1_diff\"] = np.abs((tracking[f\"{c}_p1_diff\"] + 180) % 360 - 180)\n",
    "            tracking[f\"{c}_m1_diff\"] = np.abs((tracking[f\"{c}_m1_diff\"] + 180) % 360 - 180)\n",
    "        \n",
    "    return tracking\n",
    "\n",
    "\n",
    "def tracking_agg_features(df, tracking):\n",
    "    \"\"\"トラッキングデータのstep内での単純集計。他プレイヤーの動きを見る\"\"\"\n",
    "    \n",
    "    team_agg = tracking.groupby([\"game_play\", \"step\", \"team\"]).agg(\n",
    "        x_position_team_mean = pd.NamedAgg(\"x_position\", \"mean\"),\n",
    "        y_position_team_mean = pd.NamedAgg(\"y_position\", \"mean\"),\n",
    "        speed_team_mean = pd.NamedAgg(\"speed\", \"mean\"),\n",
    "        acceleration_team_mean = pd.NamedAgg(\"acceleration\", \"mean\"),\n",
    "        sa_team_mean = pd.NamedAgg(\"sa\", \"mean\")\n",
    "    )\n",
    "    agg = tracking.groupby([\"game_play\", \"step\"]).agg(\n",
    "        x_position_mean = pd.NamedAgg(\"x_position\", \"mean\"),\n",
    "        y_position_mean = pd.NamedAgg(\"y_position\", \"mean\"),\n",
    "        speed_mean = pd.NamedAgg(\"speed\", \"mean\"),\n",
    "        acceleration_mean = pd.NamedAgg(\"acceleration\", \"mean\"),\n",
    "        sa_mean = pd.NamedAgg(\"sa\", \"mean\")\n",
    "    )\n",
    "    player_agg = tracking[tracking[\"step\"]>=0].groupby([\"game_play\", \"nfl_player_id\"]).agg(\n",
    "        sa_player_mean = pd.NamedAgg(\"sa\", \"mean\"),\n",
    "        sa_player_max = pd.NamedAgg(\"sa\", \"max\"),\n",
    "        acceleration_player_mean = pd.NamedAgg(\"acceleration\", \"mean\"),\n",
    "        acceleration_player_max = pd.NamedAgg(\"acceleration\", \"max\"),\n",
    "        speed_player_mean = pd.NamedAgg(\"speed\", \"mean\"),\n",
    "        speed_player_max = pd.NamedAgg(\"speed\", \"max\"),\n",
    "    )\n",
    "\n",
    "    for postfix in [\"_1\", \"_2\"]:\n",
    "        df = pd.merge(\n",
    "            df,\n",
    "            team_agg.rename(columns={c: c+postfix for c in team_agg.columns}).reset_index(),\n",
    "            left_on=[\"game_play\", \"step\", f\"team{postfix}\"],\n",
    "            right_on=[\"game_play\", \"step\", \"team\"],\n",
    "            how=\"left\"\n",
    "        ).drop(\"team\", axis=1)\n",
    "        \n",
    "        player_agg_renames = {c: c+postfix for c in player_agg.columns}\n",
    "        player_agg_renames[\"nfl_player_id\"] = f\"nfl_player_id{postfix}\"\n",
    "\n",
    "        df = pd.merge(\n",
    "            df,\n",
    "            player_agg.reset_index().rename(columns=player_agg_renames),\n",
    "            on=[\"game_play\", f\"nfl_player_id{postfix}\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "    \n",
    "    df = pd.merge(df, agg, on=[\"game_play\", \"step\"], how=\"left\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512cd9b",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.090678,
     "end_time": "2023-02-24T23:50:08.644448",
     "exception": false,
     "start_time": "2023-02-24T23:50:08.553770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def second_nearest_distance(df, tr_tracking, target=\"1\"):\n",
    "    stacked = pd.concat([\n",
    "        df[[\"game_play\", \"step\", \"nfl_player_id_1\", \"nfl_player_id_2\", \"distance\", \"different_team\"]],\n",
    "        df[[\"game_play\", \"step\", \"nfl_player_id_2\", \"nfl_player_id_1\", \"distance\", \"different_team\"]].rename(columns={\"nfl_player_id_2\": \"nfl_player_id_1\", \"nfl_player_id_1\": \"nfl_player_id_2\"}),\n",
    "    ])\n",
    "    \n",
    "    def _build(df, s, postfix=\"\"):\n",
    "        s[\"distance_rank\"] = s.groupby([\"game_play\", \"step\", \"nfl_player_id_1\"])[\"distance\"].rank()\n",
    "\n",
    "        stacked_1st = s[s[\"distance_rank\"]==1].drop([\"nfl_player_id_2\", \"distance_rank\", \"different_team\"], axis=1)\n",
    "        stacked_1st.columns = [\"game_play\", \"step\", f\"nfl_player_id_{target}\", f\"distance_1st_{target}{postfix}\"]\n",
    "        stacked_2nd = s[s[\"distance_rank\"]==2].drop([\"nfl_player_id_2\", \"distance_rank\", \"different_team\"], axis=1)\n",
    "        stacked_2nd.columns = [\"game_play\", \"step\", f\"nfl_player_id_{target}\", f\"distance_2nd_{target}{postfix}\"]\n",
    "\n",
    "        stacked_mrg = pd.merge(stacked_1st, stacked_2nd, on=[\"game_play\", \"step\", f\"nfl_player_id_{target}\"], how=\"left\")\n",
    "        stacked_mrg[f\"distance_diff_2nd_to_1st_{target}{postfix}\"] = stacked_mrg[f\"distance_2nd_{target}{postfix}\"] - stacked_mrg[f\"distance_1st_{target}{postfix}\"]\n",
    "\n",
    "        df = pd.merge(df, stacked_mrg, on=[\"game_play\", \"step\", f\"nfl_player_id_{target}\"], how=\"left\")\n",
    "        return df\n",
    "    \n",
    "    df = _build(df, stacked)\n",
    "    df = _build(df, stacked[stacked[\"different_team\"]], \"_different_team\")\n",
    "    return df\n",
    "    \n",
    "\n",
    "\n",
    "def bbox_distance_around_player(df):\n",
    "    \"\"\"距離など、pairwiseに計算された特徴量を集計しなおす\n",
    "    対象のプレイヤーの周りに他プレイヤーが密集しているか？\n",
    "    \"\"\"\n",
    "\n",
    "    stacked = pd.concat([\n",
    "        df[[\"game_play\", \"step\", \"nfl_player_id_1\", \"nfl_player_id_2\", \"bbox_center_Sideline_distance\", \"bbox_center_Endzone_distance\"]],\n",
    "        df[[\"game_play\", \"step\", \"nfl_player_id_2\", \"nfl_player_id_1\", \"bbox_center_Sideline_distance\", \"bbox_center_Endzone_distance\"]].rename(columns={\"nfl_player_id_2\": \"nfl_player_id_1\", \"nfl_player_id_1\": \"nfl_player_id_2\"}),\n",
    "    ])\n",
    "    def _arg_min(s):\n",
    "        try:\n",
    "            return np.nanargmin(s.values)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    feature_cols = [\n",
    "        \"mean_bbox_distance_around_player\"\n",
    "        \"min_bbox_distance_around_player\",\n",
    "        \"idxmin_bbox_distance_around_player\"\n",
    "    ]\n",
    "    aggfunc = [\"mean\", \"min\", _arg_min]\n",
    "    \n",
    "    def _merge_stacked_df(df, s, postfix=\"\"):\n",
    "        s = s.groupby([\"game_play\", \"step\", \"nfl_player_id_1\"]).agg({\n",
    "            \"bbox_center_Sideline_distance\": aggfunc,\n",
    "            \"bbox_center_Endzone_distance\": aggfunc,\n",
    "        }).reset_index()\n",
    "        s = reduce_dtype(s)\n",
    "        columns = [\"nfl_player_id\"] + [f\"{f}_Sideline{postfix}\" for f in feature_cols] + [f\"{f}_Endzone{postfix}\" for f in feature_cols]\n",
    "        s.columns = [\"game_play\", \"step\"] + columns\n",
    "\n",
    "        df = pd.merge(\n",
    "            df, \n",
    "            s.rename(columns={c: f\"{c}_1\" for c in columns}),\n",
    "            on=[\"game_play\", \"step\", \"nfl_player_id_1\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        df = pd.merge(\n",
    "            df, \n",
    "            s.rename(columns={c: f\"{c}_2\" for c in columns}),\n",
    "            on=[\"game_play\", \"step\", \"nfl_player_id_2\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        return df\n",
    "    \n",
    "    def _merge_stacked_df_pairwise(df, s):\n",
    "        s = s.groupby([\"game_play\", \"nfl_player_id_1\", \"nfl_player_id_2\"]).agg({\n",
    "            \"bbox_center_Sideline_distance\": aggfunc,\n",
    "            \"bbox_center_Endzone_distance\": aggfunc,\n",
    "        }).reset_index()\n",
    "        s = reduce_dtype(s)\n",
    "        columns = [f\"{f}_Sideline_pair\" for f in feature_cols] + [f\"{f}_Endzone_pair\" for f in feature_cols]\n",
    "        s.columns = [\"game_play\", \"nfl_player_id_1\", \"nfl_player_id_2\"] + columns\n",
    "        df = pd.merge(\n",
    "            df, \n",
    "            s,\n",
    "            on=[\"game_play\", \"nfl_player_id_1\", \"nfl_player_id_2\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        return df\n",
    "    \n",
    "    df = _merge_stacked_df(df, stacked, \"\")\n",
    "    df = _merge_stacked_df_pairwise(df, stacked)\n",
    "\n",
    "    return df\n",
    "\n",
    "def distance_around_player(df, on_full_sample = False):\n",
    "    \"\"\"距離など、pairwiseに計算された特徴量を集計しなおす\n",
    "    対象のプレイヤーの周りに他プレイヤーが密集しているか？\n",
    "    \"\"\"\n",
    "\n",
    "    stacked = pd.concat([\n",
    "        df[[\"game_play\", \"step\", \"nfl_player_id_1\", \"nfl_player_id_2\", \"distance\", \"different_team\"]],\n",
    "        df[[\"game_play\", \"step\", \"nfl_player_id_2\", \"nfl_player_id_1\", \"distance\", \"different_team\"]].rename(columns={\"nfl_player_id_2\": \"nfl_player_id_1\", \"nfl_player_id_1\": \"nfl_player_id_2\"}),\n",
    "    ])\n",
    "\n",
    "    def _arg_min(s):\n",
    "        try:\n",
    "            return np.nanargmin(s.values)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    \n",
    "    if on_full_sample:\n",
    "        # minはハードサンプルだけで見たときと同じなので飛ばす\n",
    "        feature_cols = [\n",
    "            \"mean_distance_around_player_full\",\n",
    "            \"std_distance_around_player_full\",\n",
    "            \"idxmin_distance_aronud_player_full\"\n",
    "        ]\n",
    "        aggfunc = [\"mean\", \"std\", _arg_min]\n",
    "    else:\n",
    "        feature_cols = [\n",
    "            \"mean_distance_around_player\",\n",
    "            \"min_distance_around_player\",\n",
    "            \"std_distance_around_player\",\n",
    "            # \"idxmin_distance_aronud_player\"\n",
    "        ]\n",
    "        aggfunc = [\"mean\", \"min\", \"std\"] #, _arg_min]\n",
    "\n",
    "\n",
    "    def _merge_stacked_df(df, s, postfix=\"\"):\n",
    "        s = s.groupby([\"game_play\", \"step\", \"nfl_player_id_1\"]).agg({\"distance\": aggfunc}).reset_index()\n",
    "        s = reduce_dtype(s)\n",
    "        columns = [\"nfl_player_id\"] + [f\"{f}{postfix}\" for f in feature_cols]\n",
    "        s.columns = [\"game_play\", \"step\"] + columns\n",
    "\n",
    "        df = pd.merge(\n",
    "            df, \n",
    "            s.rename(columns={c: f\"{c}_1\" for c in columns}),\n",
    "            on=[\"game_play\", \"step\", \"nfl_player_id_1\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        df = pd.merge(\n",
    "            df, \n",
    "            s.rename(columns={c: f\"{c}_2\" for c in columns}),\n",
    "            on=[\"game_play\", \"step\", \"nfl_player_id_2\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        return df\n",
    "    \n",
    "    def _merge_stacked_df_pairwise(df, s):\n",
    "        s = s.groupby([\"game_play\", \"nfl_player_id_1\", \"nfl_player_id_2\"]).agg({\"distance\": aggfunc}).reset_index()\n",
    "        s = reduce_dtype(s)\n",
    "        columns = [f\"{f}_pair\" for f in feature_cols]\n",
    "        s.columns = [\"game_play\", \"nfl_player_id_1\", \"nfl_player_id_2\"] + columns\n",
    "        df = pd.merge(\n",
    "            df, \n",
    "            s,\n",
    "            on=[\"game_play\", \"nfl_player_id_1\", \"nfl_player_id_2\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        return df\n",
    "        \n",
    "    df = _merge_stacked_df(df, stacked, \"\")\n",
    "    df = _merge_stacked_df(df, stacked[stacked[\"different_team\"]], \"_different_team\")\n",
    "    df = _merge_stacked_df_pairwise(df, stacked)\n",
    "\n",
    "    if on_full_sample:\n",
    "        df[\"step_diff_to_min_distance_full\"] = df[\"step\"] - df[\"idxmin_distance_aronud_player_full_pair\"]\n",
    "    else:\n",
    "        pass\n",
    "        # df[\"step_diff_to_min_distance\"] = df[\"step\"] - df[\"idxmin_distance_aronud_player_pair\"]\n",
    "    return df\n",
    "\n",
    "\n",
    "def step_feature(df, tracking):\n",
    "    \"\"\"stepの割合など\n",
    "    １プレーの長さや、その中で前半後半のどの辺のステップなのかに意味がある\n",
    "    （例えば、プレー開始直後に地面とコンタクトする可能性は低い）\n",
    "    \"\"\"\n",
    "    \n",
    "    # 予測対象フレーム\n",
    "    df[\"step_max_1\"] = df.groupby(\"game_play\")[\"step\"].transform(\"max\")\n",
    "    df[\"step_ratio_1\"] = (df[\"step\"] / df[\"step_max_1\"]).astype(np.float32)\n",
    "\n",
    "    # 全体\n",
    "    #step_agg = tracking.groupby(\"game_play\")[\"step\"].agg([\"min\", \"max\"])\n",
    "    #step_agg.columns = [\"step_min_2\", \"step_max_2\"]\n",
    "    #df = pd.merge(df, step_agg, left_on=\"game_play\", right_index=True, how=\"left\")\n",
    "    #df[\"step_ratio_2\"] = df[\"step\"] / df[\"step_max_2\"]\n",
    "    return df\n",
    "\n",
    "def aspect_ratio_feature(df, drop_original=False):\n",
    "    for postfix in [\"_1\", \"_2\"]:\n",
    "        for view in [\"Sideline\", \"Endzone\"]:\n",
    "            df[f\"aspect_{view}{postfix}\"] = df[f\"height_{view}{postfix}\"] / df[f\"width_{view}{postfix}\"]\n",
    "            \n",
    "            if drop_original:\n",
    "                del df[f\"height_{view}{postfix}\"]\n",
    "                del df[f\"width_{view}{postfix}\"]   \n",
    "    return df\n",
    "\n",
    "def misc_features_after_agg(df):\n",
    "    \"\"\"集約した特徴量とJoinした特徴量の比率や差など、他の特徴量を素材に作る特徴量\"\"\"\n",
    "    df[\"distance_from_mean_1\"] = distance(\n",
    "        df[\"x_position_1\"],\n",
    "        df[\"y_position_1\"],\n",
    "        df[\"x_position_mean\"],\n",
    "        df[\"y_position_mean\"],\n",
    "    )\n",
    "    df[\"distance_from_mean_2\"] = distance(\n",
    "        df[\"x_position_2\"],\n",
    "        df[\"y_position_2\"],\n",
    "        df[\"x_position_mean\"],\n",
    "        df[\"y_position_mean\"],\n",
    "    )\n",
    "    df[\"distance_team2team\"] = distance(\n",
    "        df[\"x_position_team_mean_1\"],\n",
    "        df[\"y_position_team_mean_1\"],\n",
    "        df[\"x_position_team_mean_2\"],\n",
    "        df[\"y_position_team_mean_2\"],\n",
    "    )\n",
    "    df[\"speed_diff_1_2\"] = df[\"speed_1\"] - df[\"speed_2\"]\n",
    "    df[\"speed_diff_1_team\"] = df[\"speed_1\"] - df[\"speed_team_mean_1\"]\n",
    "    df[\"speed_diff_2_team\"] = df[\"speed_2\"] - df[\"speed_team_mean_2\"]\n",
    "    df[\"distance_mean_in_play\"] = df.groupby(\"game_play\")[\"distance\"].transform(\"mean\")\n",
    "    #df[\"distance_std_in_play\"] = df.groupby(\"game_play\")[\"distance\"].transform(\"std\")\n",
    "    #df[\"distance_team2team_mean_in_play\"] = df.groupby(\"game_play\")[\"distance_team2team\"].transform(\"mean\")\n",
    "    \n",
    "    # player pairが一番近づいた瞬間の距離と現在の距離の比\n",
    "    df[\"distance_ratio_distance_to_min_pair_distance\"] = df[\"distance\"] / df[\"min_distance_around_player_pair\"]\n",
    "    \n",
    "    # player pairの距離と、現在一番player1の近くにいるplayerとの距離比\n",
    "    df[\"distance_ratio_distance_to_min_distance_around_player_1\"] = df[\"distance\"] / df[\"min_distance_around_player_1\"]\n",
    "    df[\"distance_ratio_distance_to_min_distance_around_player_2\"] = df[\"distance\"] / df[\"min_distance_around_player_2\"]\n",
    "    df[\"distance_ratio_distance_to_min_distance_around_player_diffteam_1\"] = df[\"distance\"] / df[\"min_distance_around_player_different_team_1\"]\n",
    "    df[\"distance_ratio_distance_to_min_distance_around_player_diffteam_2\"] = df[\"distance\"] / df[\"min_distance_around_player_different_team_2\"]\n",
    "\n",
    "    #df[\"distance_ratio_distance_1\"] = df[\"distance_1\"] / df[\"distance\"]\n",
    "    #df[\"distance_ratio_distance_2\"] = df[\"distance_2\"] / df[\"distance\"]\n",
    "   \n",
    "    # 進行方向以外の加速度成分\n",
    "    #df[\"sa_ratio_1\"] = np.abs(df[\"sa_1\"] / df[\"acceleration_1\"])\n",
    "    #df[\"sa_ratio_2\"] = np.abs(df[\"sa_2\"] / df[\"acceleration_2\"])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ad73e7",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.077457,
     "end_time": "2023-02-24T23:50:08.775381",
     "exception": false,
     "start_time": "2023-02-24T23:50:08.697924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def t0_feature(df, tracking):\n",
    "    # step=0時点での統計量を使った特徴量\n",
    "    on_play_start = tracking[tracking[\"step\"]==0].reset_index(drop=True)\n",
    "    on_play_start.rename(columns={\"x_position\": \"x_position_start\", \"y_position\": \"y_position_start\"}, inplace=True)\n",
    "\n",
    "    # step=0時点でのx位置は？\n",
    "    mean_x_on_play = on_play_start.groupby(\"game_play\")[\"x_position_start\"].mean()\n",
    "    mean_x_on_play.name = \"x_position_mean_on_start\"\n",
    "\n",
    "    feature_cols = [\"nfl_player_id\", \"x_position_start\", \"y_position_start\"]\n",
    "    df = pd.merge(\n",
    "        df, \n",
    "        on_play_start[[\"game_play\"]+feature_cols].rename(columns={c: f\"{c}_1\" for c in feature_cols}),\n",
    "        on=[\"game_play\", \"nfl_player_id_1\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    df = pd.merge(\n",
    "        df, \n",
    "        on_play_start[[\"game_play\"]+feature_cols].rename(columns={c: f\"{c}_2\" for c in feature_cols}),\n",
    "        on=[\"game_play\", \"nfl_player_id_2\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    df = pd.merge(\n",
    "        df,\n",
    "        mean_x_on_play.reset_index(),\n",
    "        on=\"game_play\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    # step=0時点からどれくら移動しているか？\n",
    "    df[\"distance_from_start_1\"] = distance(\n",
    "        df[\"x_position_start_1\"],\n",
    "        df[\"y_position_start_1\"],\n",
    "        df[\"x_position_1\"],\n",
    "        df[\"y_position_1\"]\n",
    "    )\n",
    "    df[\"distance_from_start_2\"] = distance(\n",
    "        df[\"x_position_start_2\"],\n",
    "        df[\"y_position_start_2\"],\n",
    "        df[\"x_position_2\"],\n",
    "        df[\"y_position_2\"]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def shift_of_player(df, tracking, shifts, add_diff=False, player_id=\"1\"):\n",
    "    step_orig = tracking[\"step\"].copy()\n",
    "    feature_cols = [\"x_position\", \"y_position\", \n",
    "                    \"speed\", \"orientation\", \"direction\", \n",
    "                    \"acceleration\", \"distance\", \"sa\"]\n",
    "\n",
    "    for shift in shifts:\n",
    "        tracking[\"step\"] = step_orig - shift\n",
    "        f_or_p = \"future\" if shift > 0 else \"past\"\n",
    "        abs_shift = np.abs(shift)\n",
    "        \n",
    "        renames = {c: f\"{c}_{f_or_p}{abs_shift}_{player_id}\" for c in feature_cols}\n",
    "        renames[\"nfl_player_id\"] = f\"nfl_player_id_{player_id}\"\n",
    "        \n",
    "        df = pd.merge(\n",
    "            df, \n",
    "            tracking[[\"step\", \"game_play\", \"nfl_player_id\"]+feature_cols].rename(columns=renames),\n",
    "            on=[\"step\", \"game_play\", f\"nfl_player_id_{player_id}\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "        if add_diff:\n",
    "            for c in feature_cols:\n",
    "                if c in [\"orientation\", \"direction\"]:\n",
    "                    df[f\"{c}_diff_{shift}_{player_id}\"] = angle_diff(df[f\"{c}_{player_id}\"], df[renames[c]])\n",
    "                else:\n",
    "                    df[f\"{c}_diff_{shift}_{player_id}\"] = df[f\"{c}_{player_id}\"] - df[renames[c]]\n",
    "        \n",
    "    tracking[\"step\"] = step_orig\n",
    "    \n",
    "    return df\n",
    "\n",
    "def bbox_y_endzone_diff_feature(df, distance_th=3.0):\n",
    "    \"\"\"近傍の選手のbboxのy座標との差をとる（転んでいる選手を検出）\"\"\"\n",
    "    bbox_y_neighbor = df[[\n",
    "        \"game_play\", \"step\", \"nfl_player_id_1\", \"nfl_player_id_2\",\n",
    "        \"bbox_center_y_Endzone_1\", \"bbox_center_y_Endzone_2\", \"distance\"\n",
    "    ]].copy()\n",
    "    \n",
    "    if df[\"distance\"].max() > distance_th:\n",
    "        bbox_y_neighbor = bbox_y_neighbor[df[\"distance\"] < distance_th]\n",
    "\n",
    "    bbox_y_neighbor[\"weight\"] = 1 / (bbox_y_neighbor[\"distance\"] + 0.1)\n",
    "\n",
    "    bbox_y_neighbor_swap = bbox_y_neighbor.copy()\n",
    "    bbox_y_neighbor_swap.columns = [\n",
    "        \"game_play\", \"step\", \"nfl_player_id_2\", \"nfl_player_id_1\",\n",
    "        \"bbox_center_y_Endzone_2\", \"bbox_center_y_Endzone_1\", \"distance\", \"weight\"\n",
    "    ]\n",
    "    bbox_y_neighbor = pd.concat([bbox_y_neighbor, bbox_y_neighbor_swap[bbox_y_neighbor.columns]])\n",
    "    \n",
    "    del bbox_y_neighbor_swap\n",
    "\n",
    "    # player_id_1の近傍distance_th以内にいる他プレーヤーのbbox座標を集約する\n",
    "    \n",
    "    bbox_y_neighbor[\"wy\"] = bbox_y_neighbor[\"weight\"] * bbox_y_neighbor[\"bbox_center_y_Endzone_2\"]\n",
    "\n",
    "    bbox_neighbor_agg = bbox_y_neighbor.groupby([\"game_play\", \"step\", \"nfl_player_id_1\"]).agg(\n",
    "        neighbor_count_1 = pd.NamedAgg(\"nfl_player_id_2\", \"count\"),\n",
    "        neighbor_y_wy_sum_1 = pd.NamedAgg(\"wy\", \"sum\"),\n",
    "        neighbor_y_w_sum_1 =pd.NamedAgg(\"weight\", \"sum\"),\n",
    "        neighbor_y_mean_1 = pd.NamedAgg(\"bbox_center_y_Endzone_2\", \"mean\"),\n",
    "        #neighbor_y_count = pd.NamedAgg(\"bbox_center_y_Endzone_2\", \"count\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    bbox_neighbor_agg[\"neighbor_y_w_mean_1\"] = bbox_neighbor_agg[\"neighbor_y_wy_sum_1\"] / bbox_neighbor_agg[\"neighbor_y_w_sum_1\"]\n",
    "\n",
    "    del bbox_neighbor_agg[\"neighbor_y_wy_sum_1\"]\n",
    "    del bbox_neighbor_agg[\"neighbor_y_w_sum_1\"]\n",
    "\n",
    "    # 転んでいる人を拾いたいので、player_id_1だけ特徴量を作る。id_2側に追加してもスコアは上がらない。\n",
    "    df = pd.merge(df, bbox_neighbor_agg, on=[\"game_play\", \"step\", \"nfl_player_id_1\"], how=\"left\")\n",
    "    df[\"bbox_y_endzone_diff_from_neighbors_1\"] = df[\"bbox_center_y_Endzone_1\"] - df[\"neighbor_y_mean_1\"]\n",
    "    df[\"bbox_y_endzone_diff_from_weighted_neighbors_1\"] = df[\"bbox_center_y_Endzone_1\"] - df[\"neighbor_y_w_mean_1\"]\n",
    "    \n",
    "    del df[\"neighbor_y_mean_1\"]\n",
    "    del df[\"neighbor_y_w_mean_1\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d335df9a",
   "metadata": {
    "papermill": {
     "duration": 0.063389,
     "end_time": "2023-02-24T23:50:08.901958",
     "exception": false,
     "start_time": "2023-02-24T23:50:08.838569",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CNN系特徴(kmat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b89af0",
   "metadata": {
    "papermill": {
     "duration": 0.218122,
     "end_time": "2023-02-24T23:50:09.182432",
     "exception": false,
     "start_time": "2023-02-24T23:50:08.964310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ロード及び前処理まわり。\n",
    "\"\"\"\n",
    "def cv2bgr_to_tf32(img):\n",
    "    #return tf.cast(img[:,:,::-1], tf.float32)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return tf.cast(img, tf.float32)\n",
    "\n",
    "def make_rectangle(df):\n",
    "    top = df.top.values.reshape(-1)\n",
    "    left = df.left.values.reshape(-1)\n",
    "    width = df.width.values.reshape(-1)\n",
    "    height = df.height.values.reshape(-1)\n",
    "    bottom = top + height\n",
    "    right = left + width\n",
    "    top = top.tolist() \n",
    "    left = left.tolist()\n",
    "    bottom = bottom.tolist()\n",
    "    right = right.tolist()\n",
    "    rectangles=[]\n",
    "    for i in range(len(top)):\n",
    "        rectangle = [[left[i], top[i]],[right[i], top[i]],\n",
    "                     [right[i], bottom[i]],[left[i], bottom[i]]]\n",
    "        rectangles.append(rectangle)\n",
    "    return rectangles\n",
    "\n",
    "def load_helmet_meta(cfg, phase=\"train\"):\n",
    "    helmet = pd.read_csv(f\"{cfg.INPUT}/{phase}_baseline_helmets.csv\")\n",
    "    meta = pd.read_csv(f\"{cfg.INPUT}/{phase}_video_metadata.csv\", parse_dates=[\"start_time\", \"end_time\", \"snap_time\"])\n",
    "\n",
    "    #helmet = read_csv_with_cache(f\"{phase}_baseline_helmets.csv\", cfg)\n",
    "    #meta = read_csv_with_cache(f\"{phase}_video_metadata.csv\", cfg)\n",
    "    #meta[\"start_time\"] = pd.to_datetime(meta[\"start_time\"], utc=True)\n",
    "    return helmet, meta\n",
    "\n",
    "def prepare_matching_dataframe(game_play, tr_tracking, helmets, meta, view=\"Sideline\", fps=59.94, only_center_of_step=True):\n",
    "    tr_tracking = tr_tracking.query(\"game_play == @game_play\").copy()\n",
    "    gp_helms = helmets.query(\"game_play == @game_play\").copy()\n",
    "\n",
    "    start_time = meta.query(\"game_play == @game_play and view == @view\")[\n",
    "        \"start_time\"\n",
    "    ].values[0]\n",
    "\n",
    "    gp_helms[\"datetime\"] = (\n",
    "        pd.to_timedelta(gp_helms[\"frame\"] * (1 / fps), unit=\"s\") + start_time\n",
    "    )\n",
    "    gp_helms[\"datetime\"] = pd.to_datetime(gp_helms[\"datetime\"], utc=True)\n",
    "    gp_helms[\"datetime_ngs\"] = (\n",
    "        pd.DatetimeIndex(gp_helms[\"datetime\"] + pd.to_timedelta(50, \"ms\"))\n",
    "        .floor(\"100ms\")\n",
    "        .values\n",
    "    )\n",
    "    gp_helms[\"datetime_ngs\"] = pd.to_datetime(gp_helms[\"datetime_ngs\"], utc=True)\n",
    "    gp_helms[\"delta_from_round_val\"] = (gp_helms[\"datetime\"]-gp_helms[\"datetime_ngs\"]).dt.total_seconds()\n",
    "    \n",
    "    gp_helms[\"center_frame_of_step\"] = np.abs(gp_helms[\"delta_from_round_val\"]) \n",
    "    gp_helms[\"center_frame_of_step\"] = gp_helms[\"center_frame_of_step\"].values==gp_helms.groupby(\"datetime_ngs\")[\"center_frame_of_step\"].transform(\"min\").values\n",
    "    if only_center_of_step:\n",
    "        gp_helms = gp_helms[gp_helms[\"center_frame_of_step\"]].drop(columns=[\"center_frame_of_step\"])\n",
    "    gp_helms = gp_helms[gp_helms[\"view\"]==view]\n",
    "\n",
    "    tr_tracking[\"datetime_ngs\"] = pd.to_datetime(tr_tracking[\"datetime\"], utc=True)\n",
    "    gp_helms = gp_helms.merge(\n",
    "        tr_tracking[[\"datetime_ngs\",\"step\",\"x_position\",\"y_position\", \"nfl_player_id\"]],\n",
    "        left_on=[\"datetime_ngs\", \"nfl_player_id\"],\n",
    "        right_on=[\"datetime_ngs\", \"nfl_player_id\"],\n",
    "        how=\"right\", # left -> outer to find the player out of the frame\n",
    "    )\n",
    "    gp_helms[\"view\"] = gp_helms[\"view\"].fillna(view)\n",
    "    gp_helms[\"frame\"] = gp_helms.groupby([\"datetime_ngs\"])[\"frame\"].transform(\"mean\")\n",
    "    gp_helms[\"game_play\"] = game_play\n",
    "    \n",
    "    gp_helms = gp_helms[~gp_helms[\"step\"].isna()]\n",
    "    gp_helms = gp_helms[~gp_helms[\"frame\"].isna()]\n",
    "    # 複数minimumが存在するケースもあるので気を付ける(あとでグループ平均とるなど)\n",
    "    return gp_helms\n",
    "    \n",
    "\n",
    "def prepare_cnn_dataframe(game_play, labels, helmets, meta, view=\"Sideline\", fps=59.94):\n",
    "    gp_labs = labels.query(\"game_play == @game_play\").copy()\n",
    "    gp_helms = helmets.query(\"game_play == @game_play\").copy()\n",
    "\n",
    "    start_time = meta.query(\"game_play == @game_play and view == @view\")[\n",
    "        \"start_time\"\n",
    "    ].values[0]\n",
    "\n",
    "    gp_helms[\"datetime\"] = (\n",
    "        pd.to_timedelta(gp_helms[\"frame\"] * (1 / fps), unit=\"s\") + start_time\n",
    "    )\n",
    "    gp_helms[\"datetime\"] = pd.to_datetime(gp_helms[\"datetime\"], utc=True)\n",
    "    gp_helms[\"datetime_ngs\"] = (\n",
    "        pd.DatetimeIndex(gp_helms[\"datetime\"] + pd.to_timedelta(50, \"ms\"))\n",
    "        .floor(\"100ms\")\n",
    "        .values\n",
    "    )\n",
    "    gp_helms[\"datetime_ngs\"] = pd.to_datetime(gp_helms[\"datetime_ngs\"], utc=True)\n",
    "    gp_helms[\"delta_from_round_val\"] = (gp_helms[\"datetime\"]-gp_helms[\"datetime_ngs\"]).dt.total_seconds()\n",
    "    \n",
    "    if \"datetime_ngs\" not in gp_labs.columns:\n",
    "        gp_labs[\"datetime_ngs\"] = pd.to_datetime(gp_labs[\"datetime\"], utc=True)\n",
    "    gp_helms = gp_helms.merge(\n",
    "        gp_labs[[\"datetime_ngs\",\"step\"]].drop_duplicates(),\n",
    "        on=[\"datetime_ngs\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    gp_helms[\"center_frame_of_step\"] = np.abs(gp_helms[\"delta_from_round_val\"]) \n",
    "    gp_helms[\"center_frame_of_step\"] = gp_helms[\"center_frame_of_step\"].values==gp_helms.groupby(\"step\")[\"center_frame_of_step\"].transform(\"min\").values\n",
    "    # 複数minimumが存在するケースもあるので気を付ける(あとでグループ平均とるなど)\n",
    "    \n",
    "    ###追加した。ok?\n",
    "    gp_helms = gp_helms[gp_helms[\"view\"]==view]\n",
    "    \n",
    "    return gp_labs, gp_helms\n",
    "    \n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "- Camaro-san Reader\n",
    "\"\"\"\n",
    "# import the necessary packages\n",
    "from threading import Thread\n",
    "\n",
    "if sys.version_info >= (3, 0):\n",
    "    from queue import Queue\n",
    "else:\n",
    "    from Queue import Queue\n",
    "\n",
    "\n",
    "class FileVideoStream:\n",
    "    def __init__(self, cv2_stream, transform=None, queue_size=16):\n",
    "        self.stream = cv2_stream#cv2.VideoCapture(path)\n",
    "        self.stopped = False\n",
    "        self.transform = transform\n",
    "\n",
    "        # initialize the queue used to store frames read from\n",
    "        # the video file\n",
    "        self.Q = Queue(maxsize=queue_size)\n",
    "        # intialize thread\n",
    "        self.thread = Thread(target=self.update, args=())\n",
    "        self.thread.daemon = True\n",
    "\n",
    "    def start(self):\n",
    "        # start a thread to read frames from the file video stream\n",
    "        self.thread.start()\n",
    "        return self\n",
    "\n",
    "    def update(self):\n",
    "        # keep looping infinitely\n",
    "        while True:\n",
    "            # if the thread indicator variable is set, stop the\n",
    "            # thread\n",
    "            if self.stopped:\n",
    "                break\n",
    "\n",
    "            if not self.Q.full():\n",
    "                (grabbed, frame) = self.stream.read()\n",
    "\n",
    "                if not grabbed:\n",
    "                    self.stopped = True\n",
    "                \n",
    "                if grabbed and self.transform:\n",
    "                    frame = self.transform(frame)\n",
    "                    # add the frame to the queue\n",
    "                    self.Q.put(frame)\n",
    "            else:\n",
    "                time.sleep(0.01)  # Rest for 10ms, we have a full queue\n",
    "\n",
    "        self.stream.release()\n",
    "\n",
    "    def read(self):\n",
    "        # return next frame in the queue\n",
    "        return self.Q.get()\n",
    "\n",
    "    def running(self):\n",
    "        return self.more() or not self.stopped\n",
    "\n",
    "    def more(self):\n",
    "        # return True if there are still frames in the queue. If stream is not stopped, try to wait a moment\n",
    "        tries = 0\n",
    "        while self.Q.qsize() == 0 and not self.stopped and tries < 5:\n",
    "            time.sleep(0.1)\n",
    "            tries += 1\n",
    "\n",
    "        return self.Q.qsize() > 0\n",
    "\n",
    "    def stop(self):\n",
    "        # indicate that the thread should be stopped\n",
    "        self.stopped = True\n",
    "        # wait until stream resources are released (producer thread might be still grabbing frame)\n",
    "        self.thread.join()\n",
    "        \n",
    "\"\"\"\n",
    "video loader for cnn\n",
    "\"\"\"\n",
    "        \n",
    "class Video2Input():\n",
    "    def __init__(self,\n",
    "                 game_play_view,\n",
    "                    video_path,\n",
    "                    labels,\n",
    "                 helms,\n",
    "                frame_interval=1,\n",
    "                only_center_frame_of_step=True):\n",
    "        \n",
    "        VIDEO_CODEC = \"MP4V\"\n",
    "        self.video_name = os.path.basename(video_path)\n",
    "        print(f\"Preparing {self.video_name}\")\n",
    "        self.labels = labels.copy()#.query(\"video == @self.video_name\").copy()\n",
    "        self.helms = helms.query(\"video == @self.video_name\").copy()\n",
    "        min_frame = self.helms[\"frame\"].values[np.argmin(self.helms[\"step\"].values)]\n",
    "        self.start_frame = max(0, min_frame-5)\n",
    "        self.vidcap = cv2.VideoCapture(video_path)\n",
    "        self.fps = self.vidcap.get(cv2.CAP_PROP_FPS)\n",
    "        #self.width = int(self.vidcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        #self.height = int(self.vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        self.total_frames = int(self.vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        self.current_frame = self.start_frame\n",
    "        self.vidcap.set(cv2.CAP_PROP_POS_FRAMES, self.start_frame)\n",
    "        self.interval = frame_interval\n",
    "        \n",
    "            \n",
    "        \n",
    "        self.vidcap = FileVideoStream(self.vidcap, \n",
    "                                      #transform=cv2bgr_to_tf32,\n",
    "                                     transform= lambda x: cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "                                     ).start()\n",
    "    \n",
    "    def get_next(self, only_center_frame_of_step=True):\n",
    "        \"\"\"\n",
    "        only_center_frame_of_step:\n",
    "            if True, use only center_frame_of_step and neglect other frames.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            #it_worked, img = self.vidcap.read()\n",
    "            img = self.vidcap.read()\n",
    "            #if not it_worked:\n",
    "            if not self.vidcap.running():\n",
    "                self.vidcap.stop()\n",
    "                return self.current_frame, None\n",
    "            # We need to add 1 to the frame count to match the label frame index\n",
    "            # that starts at 1\n",
    "            self.current_frame += 1        \n",
    "            if self.current_frame % self.interval != 0:\n",
    "                continue\n",
    "\n",
    "            # Now, add the boxes\n",
    "            df_frame_helms =  self.helms[self.helms[\"frame\"]==self.current_frame]#uery(\"frame == @self.current_frame\")\n",
    "            if len(df_frame_helms)==0:\n",
    "                continue\n",
    "            step_no = df_frame_helms[\"step\"].iloc[0]\n",
    "            if only_center_frame_of_step:\n",
    "                if df_frame_helms[\"center_frame_of_step\"].iloc[0]==False:\n",
    "                    continue\n",
    "            df_frame_label =  self.labels[self.labels[\"step\"]==step_no]\n",
    "            if len(df_frame_label)==0:\n",
    "                continue\n",
    "\n",
    "            rectangles = make_rectangle(df_frame_helms)\n",
    "            player_id = df_frame_helms[\"nfl_player_id\"].values.astype(int).tolist()\n",
    "            player_id_1 = df_frame_label[\"nfl_player_id_1\"].values.astype(int)#.tolist()\n",
    "            player_id_2 = df_frame_label[\"nfl_player_id_2\"].values.astype(int)#.tolist()\n",
    "            \n",
    "            # remove players not in the image\n",
    "            set_player = set([0]+player_id)\n",
    "            player_1_exist = [p in set_player for p in player_id_1]\n",
    "            player_2_exist = [p in set_player for p in player_id_2]\n",
    "            players_exist = np.logical_and(player_1_exist, player_2_exist)\n",
    "            \n",
    "            player_id_1 = player_id_1[players_exist]\n",
    "            player_id_2 = player_id_2[players_exist]\n",
    "            contact_labels = df_frame_label[\"contact\"].values[players_exist]#.tolist()\n",
    "            num_contact_labels = len(contact_labels)\n",
    "            num_player = len(set_player) - 1\n",
    "\n",
    "            contact_pairlabels = np.vstack([player_id_1, player_id_2, contact_labels]).T\n",
    "            \n",
    "            if num_player==0:\n",
    "                continue\n",
    "            \n",
    "            data = {\"rgb\": tf.cast(img, tf.float32),#img,#cv2bgr_to_tf32(img), \n",
    "                    \"rectangles\": tf.cast(rectangles, tf.float32),\n",
    "                    \"player_id\": tf.cast(player_id, tf.int32),\n",
    "                    \"contact_pairlabels\": tf.cast(contact_pairlabels, tf.int32),\n",
    "                    \"num_labels\": tf.cast(num_contact_labels, tf.int32),\n",
    "                    \"num_player\": num_player,\n",
    "                    \"img_height\": 720,\n",
    "                    \"img_width\": 1280,\n",
    "                    }\n",
    "            step_frame_no = [step_no, self.current_frame] # single step(100ms) contains multiple frames\n",
    "            return step_frame_no, data   \n",
    "\n",
    "\n",
    "class DataStacker:\n",
    "    def __init__(self, batch_size=2, as_list=False):\n",
    "        self.stacked = None\n",
    "        self.length = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.as_list = as_list\n",
    "    \n",
    "    def add(self, data):\n",
    "        if self.stacked is None:\n",
    "            self.stacked = {k: [data[k]] for k in data.keys()}\n",
    "        else:\n",
    "            self.stacked = {k: self.stacked[k]+[data[k]] for k in data.keys()}\n",
    "        self.length += 1\n",
    "        if self.length > self.batch_size:\n",
    "            raise Exception(\"stacked too much\")\n",
    "        \n",
    "    def get_if_ready(self, reset=True, neglect_readiness=False):\n",
    "        if self.length == self.batch_size or neglect_readiness:\n",
    "            is_ready = True\n",
    "            if self.as_list:\n",
    "                stacked = self.stacked\n",
    "            else:\n",
    "                stacked = {k: tf.stack(self.stacked[k]) for k in self.stacked.keys()}\n",
    "            if reset:\n",
    "                self.reset()\n",
    "        else:\n",
    "            stacked = None\n",
    "            is_ready = False\n",
    "        return is_ready, stacked\n",
    "    \n",
    "    def is_ready(self):\n",
    "        return self.length == self.batch_size\n",
    "        \n",
    "    def reset(self):\n",
    "        self.stacked = None\n",
    "        self.length = 0        \n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "CNN main実行系関数。\n",
    "foldとるところ少し一時的。。。\n",
    "\"\"\"\n",
    "\n",
    "def build_model(input_shape, output_shape, load_path, num_max_pair=153, skip_build_map_model=False):\n",
    "    from train_utils.tf_Augmentations_detection import ComposeNopos, Center_Crop\n",
    "    \n",
    "    is_crop_first_model = True if \"cropfirst\" in load_path else False\n",
    "    print(f\"load cnn weights {load_path}. crop model = {is_crop_first_model}\")\n",
    "    \n",
    "    model_params = {\"input_shape\": input_shape,\n",
    "                    \"output_shape\": output_shape,  \n",
    "                    \"weight_file\": load_path,\n",
    "                    \"is_train_model\": False,\n",
    "                    \"crop_first_model\": is_crop_first_model,\n",
    "                   }\n",
    "    model = NFLContact(**model_params)\n",
    "    if skip_build_map_model: # when ensemble, not need to build it again\n",
    "        model.use_map_model = True\n",
    "    else:\n",
    "        model.combine_map_model(f\"{cfg.KMAT_PATH}/model/weights/map_model_final_weights.h5\")\n",
    "    transforms = [\n",
    "                  Center_Crop(p=1, min_height=input_shape[0], min_width=input_shape[1]),\n",
    "                  ]\n",
    "    transforms = ComposeNopos(transforms)\n",
    "    preprocessor = lambda x: inference_preprocess(x, \n",
    "                                                  transforms=transforms,\n",
    "                                                  max_box_num=23, # \n",
    "                                                  max_pair_num=num_max_pair, # enough large\n",
    "                                                  padding=True)\n",
    "    # load なんかうまくいっていないときがある？？とりあえず再ロードで対策。なんでだ。\n",
    "    model.model.trainable = True\n",
    "    model.model.load_weights(load_path)\n",
    "    model.model.trainable = False\n",
    "    return model, preprocessor\n",
    "\n",
    "\n",
    "def get_foldcnntrain_set(train_df, train_fold=[0,1,2], val_fold=[3], model_type=\"A\", not_first_build=False):\n",
    "    # get game_play\n",
    "    fold_info = pd.read_csv(f\"{cfg.KMAT_PATH}/output/game_fold.csv\")\n",
    "    fold_01_game = fold_info.loc[np.logical_or(fold_info[\"fold\"]==0, fold_info[\"fold\"]==1), \"game\"].values\n",
    "    fold_23_game = fold_info.loc[np.logical_or(fold_info[\"fold\"]==2, fold_info[\"fold\"]==3), \"game\"].values\n",
    "    \n",
    "    # fold_train_game = fold_info.loc[np.any([fold_info[\"fold\"]==i for i in train_fold], axis=0), \"game\"].values\n",
    "    fold_val_game = fold_info.loc[np.any([fold_info[\"fold\"]==i for i in val_fold], axis=0), \"game\"].values\n",
    "    \n",
    "    game_play_names = list(train_df[\"game_play\"].unique())\n",
    "    game_names = [int(gp.rsplit(\"_\",1)[0]) for gp in game_play_names]\n",
    "    mask_fold_val = [name in fold_val_game for name in game_names]\n",
    "    val_set = np.array(game_play_names)[np.array(mask_fold_val)]\n",
    "    \n",
    "    train_title = \"fold\"\n",
    "    for f_num in sorted(train_fold):\n",
    "        train_title += str(f_num)\n",
    "    \n",
    "    print(f\"train by {train_title}, val by {len(val_set)}\")\n",
    "    if model_type == \"A\":\n",
    "        print(\"use itsumono model\")\n",
    "        load_path = f\"{cfg.KMAT_PATH}/model/weights/ex000_contdet_run070_{train_title}train_72crop6cbr_sc_mappretrain/final_weights.h5\"\n",
    "    elif model_type == \"B\":\n",
    "        print(\"use crop model (for ensemble?)\")\n",
    "        load_path = f\"{cfg.KMAT_PATH}/model/weights/ex000_contdet_run073_{train_title}train_160cropfirst_detpretrain/final_weights.h5\"\n",
    "        #load_path = f\"{cfg.KMAT_PATH}/model_192/weights/ex000_contdet_run074_{train_title}train_192cropfirst_detpretrain/final_weights.h5\"\n",
    "    else:\n",
    "        raise Exception(\"select from A, B \")\n",
    "    input_shape=(704, 1280, 3)\n",
    "    output_shape=(352, 640)\n",
    "    num_max_pair = train_df.groupby([\"game_play\", \"step\"])[\"nfl_player_id_2\"].size().max()\n",
    "    print(f\"cnn model predict {num_max_pair} pairs at max\")\n",
    "    model, preprocessor = build_model(input_shape, output_shape, load_path, num_max_pair, skip_build_map_model=not_first_build)\n",
    "    return model, preprocessor, val_set\n",
    "\n",
    "\n",
    "class CNNEnsembler:\n",
    "    def __init__(self, models, num_output_items=2):\n",
    "        self.models = models\n",
    "        self.num_models = len(models)\n",
    "        self.num_output_items = num_output_items\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        predictions = [[] for _ in range(self.num_output_items)]\n",
    "        for model in self.models:\n",
    "            preds, inputs = model.predict(inputs)\n",
    "            if len(preds)!=self.num_output_items:\n",
    "                raise Exception(\"set num_output_item correctly\")\n",
    "            for i in range(self.num_output_items):\n",
    "                predictions[i].append(preds[i])\n",
    "        predictions = [tf.reduce_mean(tf.stack(preds), axis=0) for preds in predictions]\n",
    "        return predictions, inputs # inputs include positions, (output mapping model)\n",
    "        \n",
    "\n",
    "from model.model import matthews_correlation_fixed\n",
    "\n",
    "# 0219 update\n",
    "def pred_by_cnn(train_df, tr_tracking, tr_helmets, tr_video_metadata, game_playes_to_pred, model, preprocessor, is_train_dataset=True):\n",
    "\n",
    "    only_center_frame_of_step = ~cfg.KMAT_CNN_ALL_FRAME\n",
    "    batch_size = 4\n",
    "    draw_pred = False\n",
    "\n",
    "    if only_center_frame_of_step: # every six frames(60fps->10fps)\n",
    "        frame_interval = 1 # must be set 1\n",
    "    else:\n",
    "        frame_interval = 1 # 中央フレームだけでは見えにくいフレームもあると思うので、理想的には1step予測に数フレーム使う方がいいと思う。\n",
    "\n",
    "    df_cnn_preds_end = []    \n",
    "    df_cnn_preds_side = []    \n",
    "    df_map_preds_end = []    \n",
    "    df_map_preds_side = []    \n",
    "    for game_play in game_playes_to_pred:#labels_mini[\"game_play\"].unique():game_play_names\n",
    "        print(game_play)\n",
    "        for view in [\"Sideline\", \"Endzone\"]:\n",
    "            print(gc.collect())\n",
    "            gp_labs, gp_helms = prepare_cnn_dataframe(game_play, train_df, tr_helmets, \n",
    "                                            tr_video_metadata, view,\n",
    "                                           )\n",
    "            if is_train_dataset:\n",
    "                video_path = f\"{cfg.INPUT}/train/{game_play}_{view}.mp4\"\n",
    "            else:\n",
    "                video_path = f\"{cfg.INPUT}/test/{game_play}_{view}.mp4\"\n",
    "                \n",
    "            game_play_view = f\"{game_play}_{view}\"\n",
    "\n",
    "            vi = Video2Input(game_play_view,\n",
    "                        video_path,\n",
    "                         gp_labs,   gp_helms,      \n",
    "                        frame_interval=frame_interval)\n",
    "\n",
    "            stacked_inputs = DataStacker(batch_size)\n",
    "            stacked_targets = DataStacker(batch_size)\n",
    "            stacked_info = DataStacker(batch_size, as_list=False)\n",
    "            start_time = time.time()\n",
    "            counter = 0\n",
    "            d_counter = 0\n",
    "            predicted_labels = []\n",
    "            gt_labels = []\n",
    "            players_1 = []\n",
    "            players_2 = []\n",
    "            step_numbers = []\n",
    "            frame_numbers = []\n",
    "            \n",
    "            map_positions =[]\n",
    "            map_player_id = []\n",
    "            player_single_contacts = []\n",
    "            step_numbers_map = []\n",
    "            frame_numbers_map = []\n",
    "                \n",
    "            is_last_batch = False\n",
    "            while True:\n",
    "                step_frame_no, data = vi.get_next(only_center_frame_of_step=only_center_frame_of_step)\n",
    "\n",
    "                if data is None:\n",
    "                    is_last_batch = True\n",
    "                    is_ready = True\n",
    "                    if stacked_inputs.length==0:\n",
    "                        break\n",
    "                else:\n",
    "                    d_counter += 1\n",
    "\n",
    "                    try:\n",
    "                        inputs, targets, info = preprocessor(data)\n",
    "                        #今少し良くないやり方.要修正、このプリ処理で消えるやつがおる。数変わるのでちゅうい\n",
    "                    except:# クロップレンジ外。一時的\n",
    "                        print(\"ラベル無し\")\n",
    "                        continue\n",
    "                    step_numbers += [step_frame_no[0]] * int(info[\"num_labels\"])\n",
    "                    frame_numbers += [step_frame_no[1]] * int(info[\"num_labels\"])\n",
    "                    \n",
    "                    step_numbers_map += [step_frame_no[0]] * int(info[\"num_player\"])\n",
    "                    frame_numbers_map += [step_frame_no[1]] * int(info[\"num_player\"])\n",
    "\n",
    "\n",
    "                    stacked_inputs.add(inputs)\n",
    "                    stacked_targets.add(targets)\n",
    "                    stacked_info.add(info)\n",
    "                    is_ready = stacked_inputs.is_ready()\n",
    "\n",
    "                if is_ready:\n",
    "\n",
    "                    _, inp = stacked_inputs.get_if_ready(neglect_readiness=is_last_batch)\n",
    "                    _, targ = stacked_targets.get_if_ready(neglect_readiness=is_last_batch)\n",
    "                    _, info = stacked_info.get_if_ready(neglect_readiness=is_last_batch)\n",
    "                    \n",
    "                    #for key in inp.keys():\n",
    "                    #    print(key, inp[key].shape)\n",
    "                    #    if key==\"input_boxes\":\n",
    "                    #        print(inp[key])\n",
    "                    #    if key==\"input_pairs\":\n",
    "                    #        print(inp[key])\n",
    "                        \n",
    "                    preds, inp = model.predict(inp) # inputs include positions, (output mapping model)\n",
    "                    pred_mask, pred_label, pred_single_contact = preds # pred_label_wo_map is not using. old output\n",
    "                    # pred_mask, pred_label, _ = preds # pred_label_wo_map is not using. old output\n",
    "                    \n",
    "                    \n",
    "                    if draw_pred:\n",
    "                        view_contact_mask(inp[\"input_rgb\"].numpy()[0], \n",
    "                                          inp[\"input_pairs\"].numpy()[0], \n",
    "                                              pred_mask.numpy()[0, :, :, :, 0], \n",
    "                                              pred_label.numpy()[0], \n",
    "                                              gt_label=targ[\"output_contact_label\"].numpy()[0],\n",
    "                                              title_epoch=\"\")\n",
    "\n",
    "                    for i, [p, gt, num, pairs] in enumerate(zip(pred_label.numpy(), targ[\"output_contact_label\"].numpy(), \n",
    "                                                 info[\"num_labels\"], info[\"contact_pairlabels\"])):\n",
    "                        predicted_labels += list(p[:num])\n",
    "                        gt_labels += list(gt[:num])\n",
    "                        players_1 += list(pairs[:num,0].numpy())\n",
    "                        players_2 += list(pairs[:num,1].numpy())\n",
    "                        \n",
    "                    for i, [pos, ps_con, pid, num] in enumerate(zip(inp[\"input_player_positions\"].numpy(), pred_single_contact.numpy(),\n",
    "                                                            info[\"player_id\"].numpy(), info[\"num_player\"])):\n",
    "                        map_positions += list(pos[:num])\n",
    "                        map_player_id += list(pid[:num])\n",
    "                        player_single_contacts += list(ps_con[:num])\n",
    "                        \n",
    "                    counter += batch_size\n",
    "                    time_elapsed = time.time() - start_time\n",
    "                    fps_inference = counter / time_elapsed\n",
    "                    print(f\"\\r{round(fps_inference, 1)} fps, at {vi.current_frame} / {vi.total_frames} in video\", end=\"\")\n",
    "                    \n",
    "                    \n",
    "                if is_last_batch:\n",
    "                    break\n",
    "           \n",
    "            df_pred = pd.DataFrame(np.array(predicted_labels).reshape(-1,1), columns=[f\"cnn_pred_{view}\"])\n",
    "            df_pred[\"nfl_player_id_1\"] = players_1\n",
    "            df_pred[\"nfl_player_id_2\"] = players_2\n",
    "            df_pred[\"game_play\"] = game_play\n",
    "            df_pred[\"step\"] = step_numbers\n",
    "            df_pred[\"frame\"] = frame_numbers\n",
    "            df_pred[\"gt_tmp\"] = gt_labels\n",
    "            \n",
    "            df_pred_map = pd.DataFrame(np.array(map_positions).reshape(-1,2), columns=[f\"pred_coords_i_{view}\", f\"pred_coords_j_{view}\"])\n",
    "            df_pred_map[f\"player_single_contacts_{view}\"] = player_single_contacts\n",
    "            df_pred_map[\"nfl_player_id\"] = map_player_id\n",
    "            df_pred_map[\"step\"] = step_numbers_map\n",
    "            df_pred_map[\"frame\"] = frame_numbers_map\n",
    "            df_pred_map[\"game_play\"] = game_play\n",
    "            # df_pred = df_pred.groupby([\"step\", \"game_play\", \"nfl_player_id_1\",\"nfl_player_id_2\"]).mean().reset_index()\n",
    "            # min, max, meanなど後で処理する\n",
    "            if view == \"Sideline\":\n",
    "                df_cnn_preds_side.append(df_pred)\n",
    "                df_map_preds_side.append(df_pred_map)\n",
    "            else:\n",
    "                df_cnn_preds_end.append(df_pred)\n",
    "                df_map_preds_end.append(df_pred_map)\n",
    "\n",
    "            show_each_matthews = True\n",
    "            if show_each_matthews:\n",
    "                gt_labels = tf.cast(gt_labels, tf.float32)\n",
    "                predicted_labels = tf.cast(predicted_labels, tf.float32)\n",
    "                for th in np.linspace(0.1, 0.9, 9):\n",
    "                    print(th, matthews_correlation_fixed(gt_labels, predicted_labels, threshold=th))\n",
    "\n",
    "    df_cnn_preds_end = pd.concat(df_cnn_preds_end, axis=0)\n",
    "    df_cnn_preds_side = pd.concat(df_cnn_preds_side, axis=0)\n",
    "    df_map_preds_end = pd.concat(df_map_preds_end, axis=0)\n",
    "    df_map_preds_side = pd.concat(df_map_preds_side, axis=0)\n",
    "    print(gc.collect())\n",
    "    return df_cnn_preds_end, df_cnn_preds_side, df_map_preds_end, df_map_preds_side\n",
    "\n",
    "# 0219 update\n",
    "def pred_by_cnn_for_ensemble(train_df, tr_tracking, tr_helmets, tr_video_metadata, game_playes_to_pred, models, preprocessor, is_train_dataset=True):\n",
    "    \"\"\"\n",
    "    models = [CNNEnsembler, CNNEnsembler] or [model, model]\n",
    "    output 'cnn_pred_{view}_{model_no}'\n",
    "    \"\"\"\n",
    "    num_models = len(models)\n",
    "\n",
    "    only_center_frame_of_step = ~cfg.KMAT_CNN_ALL_FRAME\n",
    "    batch_size = 4\n",
    "    draw_pred = False\n",
    "\n",
    "    if only_center_frame_of_step: # every six frames(60fps->10fps)\n",
    "        frame_interval = 1 # must be set 1\n",
    "    else:\n",
    "        frame_interval = 1 # 中央フレームだけでは見えにくいフレームもあると思うので、理想的には1step予測に数フレーム使う方がいいと思う。\n",
    "\n",
    "    df_cnn_preds_end = []    \n",
    "    df_cnn_preds_side = []    \n",
    "    df_map_preds_end = []    \n",
    "    df_map_preds_side = []    \n",
    "    for game_play in game_playes_to_pred:#labels_mini[\"game_play\"].unique():game_play_names\n",
    "        print(game_play)\n",
    "        for view in [\"Sideline\", \"Endzone\"]:\n",
    "            print(gc.collect())\n",
    "            gp_labs, gp_helms = prepare_cnn_dataframe(game_play, train_df, tr_helmets, \n",
    "                                            tr_video_metadata, view,\n",
    "                                           )\n",
    "            if is_train_dataset:\n",
    "                video_path = f\"{cfg.INPUT}/train/{game_play}_{view}.mp4\"\n",
    "            else:\n",
    "                video_path = f\"{cfg.INPUT}/test/{game_play}_{view}.mp4\"\n",
    "                \n",
    "            game_play_view = f\"{game_play}_{view}\"\n",
    "\n",
    "            vi = Video2Input(game_play_view,\n",
    "                        video_path,\n",
    "                         gp_labs,   gp_helms,      \n",
    "                        frame_interval=frame_interval)\n",
    "\n",
    "            stacked_inputs = DataStacker(batch_size)\n",
    "            stacked_targets = DataStacker(batch_size)\n",
    "            stacked_info = DataStacker(batch_size, as_list=False)\n",
    "            start_time = time.time()\n",
    "            counter = 0\n",
    "            d_counter = 0\n",
    "            predicted_labels = [[] for _ in range(num_models)]\n",
    "            gt_labels = []\n",
    "            players_1 = []\n",
    "            players_2 = []\n",
    "            step_numbers = []\n",
    "            frame_numbers = []\n",
    "            \n",
    "            map_positions =[]\n",
    "            map_player_id = []\n",
    "            player_single_contacts = [[] for _ in range(num_models)]\n",
    "            step_numbers_map = []\n",
    "            frame_numbers_map = []\n",
    "                \n",
    "            is_last_batch = False\n",
    "            while True:\n",
    "                step_frame_no, data = vi.get_next(only_center_frame_of_step=only_center_frame_of_step)\n",
    "\n",
    "                if data is None:\n",
    "                    is_last_batch = True\n",
    "                    is_ready = True\n",
    "                    if stacked_inputs.length==0:\n",
    "                        break\n",
    "                else:\n",
    "                    d_counter += 1\n",
    "\n",
    "                    try:\n",
    "                        inputs, targets, info = preprocessor(data)\n",
    "                        #今少し良くないやり方.要修正、このプリ処理で消えるやつがおる。数変わるのでちゅうい\n",
    "                    except:# クロップレンジ外。一時的\n",
    "                        print(\"ラベル無し\")\n",
    "                        continue\n",
    "                    step_numbers += [step_frame_no[0]] * int(info[\"num_labels\"])\n",
    "                    frame_numbers += [step_frame_no[1]] * int(info[\"num_labels\"])\n",
    "                    \n",
    "                    step_numbers_map += [step_frame_no[0]] * int(info[\"num_player\"])\n",
    "                    frame_numbers_map += [step_frame_no[1]] * int(info[\"num_player\"])\n",
    "\n",
    "\n",
    "                    stacked_inputs.add(inputs)\n",
    "                    stacked_targets.add(targets)\n",
    "                    stacked_info.add(info)\n",
    "                    is_ready = stacked_inputs.is_ready()\n",
    "\n",
    "                if is_ready:\n",
    "\n",
    "                    _, inp = stacked_inputs.get_if_ready(neglect_readiness=is_last_batch)\n",
    "                    _, targ = stacked_targets.get_if_ready(neglect_readiness=is_last_batch)\n",
    "                    _, info = stacked_info.get_if_ready(neglect_readiness=is_last_batch)\n",
    "                    \n",
    "                    #for key in inp.keys():\n",
    "                    #    print(key, inp[key].shape)\n",
    "                    #    if key==\"input_boxes\":\n",
    "                    #        print(inp[key])\n",
    "                    #    if key==\"input_pairs\":\n",
    "                    #        print(inp[key])\n",
    "                    \n",
    "                    list_pred_label = []\n",
    "                    list_pred_single_contact = []\n",
    "                    for model in models:\n",
    "                        preds, inp = model.predict(inp) # inputs include positions, (output mapping model) after first prediction\n",
    "                        list_pred_label.append(preds[1])\n",
    "                        list_pred_single_contact.append(preds[2])\n",
    "                    # pred_mask, pred_label, pred_single_contact = preds # pred_label_wo_map is not using. old output\n",
    "                    # pred_mask, pred_label, _ = preds # pred_label_wo_map is not using. old output\n",
    "                    list_pred_label = tf.stack(list_pred_label, axis=1).numpy()\n",
    "                    list_pred_single_contact = tf.stack(list_pred_single_contact, axis=1).numpy()\n",
    "                    for i, [p, gt, num, pairs] in enumerate(zip(list_pred_label, targ[\"output_contact_label\"].numpy(), \n",
    "                                                 info[\"num_labels\"], info[\"contact_pairlabels\"])):\n",
    "                        for j in range(num_models):\n",
    "                            predicted_labels[j] += list(p[j,:num])\n",
    "                        gt_labels += list(gt[:num])\n",
    "                        players_1 += list(pairs[:num,0].numpy())\n",
    "                        players_2 += list(pairs[:num,1].numpy())\n",
    "                        \n",
    "                    for i, [pos, ps_con, pid, num] in enumerate(zip(inp[\"input_player_positions\"].numpy(), list_pred_single_contact,\n",
    "                                                            info[\"player_id\"].numpy(), info[\"num_player\"])):\n",
    "                        map_positions += list(pos[:num])\n",
    "                        map_player_id += list(pid[:num])\n",
    "                        for j in range(num_models):\n",
    "                            player_single_contacts[j] += list(ps_con[j,:num])\n",
    "                        \n",
    "                    counter += batch_size\n",
    "                    time_elapsed = time.time() - start_time\n",
    "                    fps_inference = counter / time_elapsed\n",
    "                    print(f\"\\r{round(fps_inference, 1)} fps, at {vi.current_frame} / {vi.total_frames} in video\", end=\"\")\n",
    "                    \n",
    "                    \n",
    "                if is_last_batch:\n",
    "                    break\n",
    "            print(\"shape of prediction \", np.array(predicted_labels).shape, np.array(player_single_contacts).shape)\n",
    "            df_pred = pd.DataFrame(np.array(predicted_labels).T, columns=[f\"cnn_pred_{view}_{model_no}\" for model_no in range(num_models)])\n",
    "            df_pred[\"nfl_player_id_1\"] = players_1\n",
    "            df_pred[\"nfl_player_id_2\"] = players_2\n",
    "            df_pred[\"game_play\"] = game_play\n",
    "            df_pred[\"step\"] = step_numbers\n",
    "            df_pred[\"frame\"] = frame_numbers\n",
    "            df_pred[\"gt_tmp\"] = gt_labels\n",
    "            \n",
    "            df_pred_map = pd.DataFrame(np.array(map_positions).reshape(-1,2), columns=[f\"pred_coords_i_{view}\", f\"pred_coords_j_{view}\"])\n",
    "            df_pred_map[[f\"player_single_contacts_{view}_{model_no}\" for model_no in range(num_models)]] = np.array(player_single_contacts).T\n",
    "            df_pred_map[\"nfl_player_id\"] = map_player_id\n",
    "            df_pred_map[\"step\"] = step_numbers_map\n",
    "            df_pred_map[\"frame\"] = frame_numbers_map\n",
    "            df_pred_map[\"game_play\"] = game_play\n",
    "            # df_pred = df_pred.groupby([\"step\", \"game_play\", \"nfl_player_id_1\",\"nfl_player_id_2\"]).mean().reset_index()\n",
    "            # min, max, meanなど後で処理する\n",
    "            if view == \"Sideline\":\n",
    "                df_cnn_preds_side.append(df_pred)\n",
    "                df_map_preds_side.append(df_pred_map)\n",
    "            else:\n",
    "                df_cnn_preds_end.append(df_pred)\n",
    "                df_map_preds_end.append(df_pred_map)\n",
    "\n",
    "            show_each_matthews = True\n",
    "            if show_each_matthews:\n",
    "                gt_labels = tf.cast(gt_labels, tf.float32)\n",
    "                predicted_labels = tf.reduce_mean(tf.cast(predicted_labels, tf.float32), axis=0)\n",
    "                for th in np.linspace(0.1, 0.9, 9):\n",
    "                    print(th, matthews_correlation_fixed(gt_labels, predicted_labels, threshold=th))\n",
    "\n",
    "    df_cnn_preds_end = pd.concat(df_cnn_preds_end, axis=0)\n",
    "    df_cnn_preds_side = pd.concat(df_cnn_preds_side, axis=0)\n",
    "    df_map_preds_end = pd.concat(df_map_preds_end, axis=0)\n",
    "    df_map_preds_side = pd.concat(df_map_preds_side, axis=0)\n",
    "    print(gc.collect())\n",
    "    return df_cnn_preds_end, df_cnn_preds_side, df_map_preds_end, df_map_preds_side\n",
    "\n",
    "def postprocess_cnn_all_frame_outputs(df_pred):\n",
    "    # 追加。フレーム単位出力の場合、この辺で適当に処理を加える\n",
    "    # ここで追加したものは、後段のcnn_features関数でマージされる。特別指定しない限りは追加したものは全てマージされる（はず\n",
    "    # df_predのcolumnsは\"game_play\", \"step\", \"frame\", \"nfl_player_id_1\", \"nfl_player_id_2\", \"cnn_pred_Sideline\" or \"cnn_pred_Endzone\", \"gt_tmp\"\n",
    "    # TODO gt_tmpはゴミ。捨てる。cnn_predの名称統一した方が便利かも…。\n",
    "    df_pred = df_pred.groupby([\"step\", \"game_play\", \"nfl_player_id_1\",\"nfl_player_id_2\"]).mean().reset_index()\n",
    "    return df_pred\n",
    "\n",
    "# 0222 update\n",
    "def cnn_features_val(train_df, tr_tracking, tr_helmets, tr_video_metadata, cnn_pred_path=f\"{cfg.INPUT_DIR}/nfl2cnnpred1218\", model_type=\"B\"):\n",
    "    if cfg.KMAT_CNN_ALL_FRAME: # load all outputs\n",
    "        suffix = \"_raw\"\n",
    "    else:\n",
    "        suffix = \"\"\n",
    "    \n",
    "    if \"distance\" in train_df.columns:\n",
    "        dist_btw_players = train_df[\"distance\"].copy()\n",
    "    else:\n",
    "        dist_btw_players = distance(train_df[\"x_position_1\"], train_df[\"y_position_1\"], train_df[\"x_position_2\"], train_df[\"y_position_2\"])\n",
    "    dist_thresh = 3\n",
    "    ground_id = train_df[\"nfl_player_id_2\"].min()\n",
    "    train_df_mini = train_df[np.logical_or(dist_btw_players<dist_thresh, train_df[\"nfl_player_id_2\"]==ground_id)].copy()\n",
    "    train_df_mini['nfl_player_id_2'] = train_df_mini['nfl_player_id_2'].replace(ground_id,0).astype(int)\n",
    "    \n",
    "    if os.path.exists(cnn_pred_path):\n",
    "        print(\"load existing pred file\")\n",
    "        print(f\"fold01_cnn_pred_end{suffix}.csv\")\n",
    "        \n",
    "        df_side = []\n",
    "        df_end = []\n",
    "        for i in range(4):\n",
    "            df_cnn_preds_end = pd.read_csv(os.path.join(cnn_pred_path, f\"fold{i}_cnn_pred_end{suffix}.csv\"))\n",
    "            df_cnn_preds_side = pd.read_csv(os.path.join(cnn_pred_path, f\"fold{i}_cnn_pred_side{suffix}.csv\"))\n",
    "            df_cnn_preds_end[[\"nfl_player_id_1\", \"nfl_player_id_2\"]] = df_cnn_preds_end[[\"nfl_player_id_1\", \"nfl_player_id_2\"]].astype(int)\n",
    "            df_cnn_preds_side[[\"nfl_player_id_1\", \"nfl_player_id_2\"]] = df_cnn_preds_side[[\"nfl_player_id_1\", \"nfl_player_id_2\"]].astype(int)\n",
    "            df_end.append(df_cnn_preds_end)\n",
    "            df_side.append(df_cnn_preds_side)\n",
    "        df_side = pd.concat(df_side, axis=0)\n",
    "        df_end = pd.concat(df_end, axis=0)\n",
    "\n",
    "        df_end_map = []\n",
    "        for i in range(4):\n",
    "            df_end_map.append(pd.read_csv(os.path.join(cnn_pred_path, f\"fold{i}_map_pred_end{suffix}.csv\")))\n",
    "        df_end_map = pd.concat(df_end_map, axis=0)\n",
    "        df_end_map[\"nfl_player_id\"] = df_end_map[\"nfl_player_id\"].astype(int)\n",
    "\n",
    "        df_side_map = []\n",
    "        for i in range(4):\n",
    "            df_side_map.append(pd.read_csv(os.path.join(cnn_pred_path, f\"fold{i}_map_pred_side{suffix}.csv\")))\n",
    "        df_side_map = pd.concat(df_side_map, axis=0)\n",
    "        df_side_map[\"nfl_player_id\"] = df_side_map[\"nfl_player_id\"].astype(int)\n",
    "               \n",
    "    else:\n",
    "        print(\"start CNN validation pred\")\n",
    "        K.clear_session()\n",
    "        \n",
    "        os.makedirs(cnn_pred_path, exist_ok=True)\n",
    "\n",
    "        train_fold=[1,2,3]\n",
    "        val_fold=[0]\n",
    "        model, preprocessor, val_set = get_foldcnntrain_set(train_df_mini, train_fold, val_fold, model_type=model_type)\n",
    "        df_cnn_preds_end_0, df_cnn_preds_side_0, df_map_preds_end_0, df_map_preds_side_0 = pred_by_cnn(train_df_mini, tr_tracking, tr_helmets, tr_video_metadata, val_set, model, preprocessor, is_train_dataset=True)\n",
    "        df_cnn_preds_end_0.to_csv(os.path.join(cnn_pred_path, f\"fold0_cnn_pred_end{suffix}.csv\"), index=False)\n",
    "        df_cnn_preds_side_0.to_csv(os.path.join(cnn_pred_path, f\"fold0_cnn_pred_side{suffix}.csv\"), index=False)\n",
    "        df_map_preds_end_0.to_csv(os.path.join(cnn_pred_path, f\"fold0_map_pred_end{suffix}.csv\"), index=False)\n",
    "        df_map_preds_side_0.to_csv(os.path.join(cnn_pred_path, f\"fold0_map_pred_side{suffix}.csv\"), index=False)\n",
    "        \n",
    "        train_fold=[0,2,3]\n",
    "        val_fold=[1]\n",
    "        model, preprocessor, val_set = get_foldcnntrain_set(train_df_mini, train_fold, val_fold, model_type=model_type)\n",
    "        df_cnn_preds_end_1, df_cnn_preds_side_1, df_map_preds_end_1, df_map_preds_side_1 = pred_by_cnn(train_df_mini, tr_tracking, tr_helmets, tr_video_metadata, val_set, model, preprocessor, is_train_dataset=True)\n",
    "        df_cnn_preds_end_1.to_csv(os.path.join(cnn_pred_path, f\"fold1_cnn_pred_end{suffix}.csv\"), index=False)\n",
    "        df_cnn_preds_side_1.to_csv(os.path.join(cnn_pred_path, f\"fold1_cnn_pred_side{suffix}.csv\"), index=False)\n",
    "        df_map_preds_end_1.to_csv(os.path.join(cnn_pred_path, f\"fold1_map_pred_end{suffix}.csv\"), index=False)\n",
    "        df_map_preds_side_1.to_csv(os.path.join(cnn_pred_path, f\"fold1_map_pred_side{suffix}.csv\"), index=False)\n",
    "        \n",
    "        \n",
    "        train_fold=[0,1,3]\n",
    "        val_fold=[2]\n",
    "        model, preprocessor, val_set = get_foldcnntrain_set(train_df_mini, train_fold, val_fold, model_type=model_type)\n",
    "        df_cnn_preds_end_2, df_cnn_preds_side_2, df_map_preds_end_2, df_map_preds_side_2 = pred_by_cnn(train_df_mini, tr_tracking, tr_helmets, tr_video_metadata, val_set, model, preprocessor, is_train_dataset=True)\n",
    "        df_cnn_preds_end_2.to_csv(os.path.join(cnn_pred_path, f\"fold2_cnn_pred_end{suffix}.csv\"), index=False)\n",
    "        df_cnn_preds_side_2.to_csv(os.path.join(cnn_pred_path, f\"fold2_cnn_pred_side{suffix}.csv\"), index=False)\n",
    "        df_map_preds_end_2.to_csv(os.path.join(cnn_pred_path, f\"fold2_map_pred_end{suffix}.csv\"), index=False)\n",
    "        df_map_preds_side_2.to_csv(os.path.join(cnn_pred_path, f\"fold2_map_pred_side{suffix}.csv\"), index=False)\n",
    "        \n",
    "        train_fold=[0,1,2]\n",
    "        val_fold=[3]\n",
    "        model, preprocessor, val_set = get_foldcnntrain_set(train_df_mini, train_fold, val_fold, model_type=model_type)\n",
    "        df_cnn_preds_end_3, df_cnn_preds_side_3, df_map_preds_end_3, df_map_preds_side_3 = pred_by_cnn(train_df_mini, tr_tracking, tr_helmets, tr_video_metadata, val_set, model, preprocessor, is_train_dataset=True)\n",
    "        df_cnn_preds_end_3.to_csv(os.path.join(cnn_pred_path, f\"fold3_cnn_pred_end{suffix}.csv\"), index=False)\n",
    "        df_cnn_preds_side_3.to_csv(os.path.join(cnn_pred_path, f\"fold3_cnn_pred_side{suffix}.csv\"), index=False)\n",
    "        df_map_preds_end_3.to_csv(os.path.join(cnn_pred_path, f\"fold3_map_pred_end{suffix}.csv\"), index=False)\n",
    "        df_map_preds_side_3.to_csv(os.path.join(cnn_pred_path, f\"fold3_map_pred_side{suffix}.csv\"), index=False)\n",
    "        \n",
    "\n",
    "        df_side = pd.concat([df_cnn_preds_side_0, df_cnn_preds_side_1, df_cnn_preds_side_2, df_cnn_preds_side_3], axis=0)\n",
    "        df_end = pd.concat([df_cnn_preds_end_0, df_cnn_preds_end_1, df_cnn_preds_end_2, df_cnn_preds_end_3], axis=0)\n",
    "        df_side_map = pd.concat([df_map_preds_side_0, df_map_preds_side_1, df_map_preds_side_2, df_map_preds_side_3], axis=0)\n",
    "        df_end_map = pd.concat([df_map_preds_end_0, df_map_preds_end_1, df_map_preds_end_2, df_map_preds_end_3], axis=0)\n",
    "        \n",
    "    # 追加。フレーム単位出力の場合、この辺で適当に処理を加える\n",
    "    # ここで追加したものは、後段cnn_featuresでマージする\n",
    "    df_side = postprocess_cnn_all_frame_outputs(df_side)\n",
    "    df_end = postprocess_cnn_all_frame_outputs(df_end)\n",
    "    \n",
    "    df_end['nfl_player_id_2'] = df_end['nfl_player_id_2'].replace(0,ground_id).astype(int)\n",
    "    df_side['nfl_player_id_2'] = df_side['nfl_player_id_2'].replace(0,ground_id).astype(int)\n",
    "\n",
    "    return df_side, df_end, df_side_map, df_end_map\n",
    "\n",
    "# 0219 update\n",
    "def cnn_features_test(train_df, tr_tracking, tr_helmets, tr_video_metadata, model_type=\"B\"):\n",
    "    print(gc.collect())\n",
    "    save_path_end = \"cnn_pred_end.csv\"\n",
    "    save_path_side = \"cnn_pred_side.csv\"\n",
    "    save_path_map_end = \"map_pred_end.csv\"\n",
    "    save_path_map_side = \"map_pred_side.csv\"\n",
    "    if os.path.exists(save_path_end):\n",
    "        df_cnn_preds_end = pd.read_csv(save_path_end)\n",
    "        df_cnn_preds_side = pd.read_csv(save_path_side)\n",
    "        df_end_map = pd.read_csv(save_path_map_end)\n",
    "        df_side_map = pd.read_csv(save_path_map_side)\n",
    "        df_cnn_preds_end[[\"nfl_player_id_1\", \"nfl_player_id_2\"]] = df_cnn_preds_end[[\"nfl_player_id_1\", \"nfl_player_id_2\"]].astype(int)\n",
    "        df_cnn_preds_side[[\"nfl_player_id_1\", \"nfl_player_id_2\"]] = df_cnn_preds_side[[\"nfl_player_id_1\", \"nfl_player_id_2\"]].astype(int)\n",
    "        df_end_map[\"nfl_player_id\"] = df_end_map[\"nfl_player_id\"].astype(int)\n",
    "        df_side_map[\"nfl_player_id\"] = df_side_map[\"nfl_player_id\"].astype(int)\n",
    "    else:\n",
    "        if \"distance\" in train_df.columns:\n",
    "            dist_btw_players = train_df[\"distance\"].copy()\n",
    "        else:\n",
    "            dist_btw_players = distance(df[\"x_position_1\"], df[\"y_position_1\"], df[\"x_position_2\"], df[\"y_position_2\"])\n",
    "\n",
    "        dist_thresh = 3\n",
    "        ground_id = train_df[\"nfl_player_id_2\"].min() # kmat modelでは0を使用していたのでid No調整。関数出る前に戻す。\n",
    "        train_df_mini = train_df[np.logical_or(dist_btw_players<dist_thresh, train_df[\"nfl_player_id_2\"]==ground_id)].copy()\n",
    "        train_df_mini['nfl_player_id_2'] = train_df_mini['nfl_player_id_2'].replace(ground_id,0).astype(int)\n",
    "        train_df_mini[\"contact\"] = 1. # not use\n",
    "\n",
    "        game_plays = list(train_df_mini[\"game_play\"].unique())\n",
    "        # model_01, preprocessor, _ = get_foldcnntrain_set(train_df, train_01_val_23 = True)\n",
    "        # model_23, preprocessor, _ = get_foldcnntrain_set(train_df, train_01_val_23 = False)\n",
    "\n",
    "        train_fold, val_fold = [1,2,3], [0]\n",
    "        model_0, preprocessor, _ = get_foldcnntrain_set(train_df_mini, train_fold, val_fold, model_type=model_type)\n",
    "        train_fold, val_fold = [0,2,3], [1]\n",
    "        model_1, preprocessor, _ = get_foldcnntrain_set(train_df_mini, train_fold, val_fold, model_type=model_type, not_first_build=True)\n",
    "        train_fold, val_fold = [0,1,3], [2]\n",
    "        model_2, preprocessor, _ = get_foldcnntrain_set(train_df_mini, train_fold, val_fold, model_type=model_type, not_first_build=True)\n",
    "        train_fold, val_fold = [0,1,2], [3]\n",
    "        model_3, preprocessor, _ = get_foldcnntrain_set(train_df_mini, train_fold, val_fold, model_type=model_type, not_first_build=True)\n",
    "        model = CNNEnsembler([model_0, model_1, model_2, model_3], num_output_items=3)\n",
    "\n",
    "        df_end, df_side, df_end_map, df_side_map = pred_by_cnn(train_df_mini, tr_tracking, tr_helmets, tr_video_metadata, game_plays, model, preprocessor, is_train_dataset=False)\n",
    "        df_end.to_csv(save_path_end, index=False)\n",
    "        df_side.to_csv(save_path_side, index=False)\n",
    "        df_end_map.to_csv(save_path_map_end, index=False)\n",
    "        df_side_map.to_csv(save_path_map_side, index=False)\n",
    "\n",
    "    print(gc.collect())\n",
    "    df_side = postprocess_cnn_all_frame_outputs(df_side)\n",
    "    df_end = postprocess_cnn_all_frame_outputs(df_end)\n",
    "\n",
    "    df_end['nfl_player_id_2'] = df_end['nfl_player_id_2'].replace(0,ground_id).astype(int)\n",
    "    df_side['nfl_player_id_2'] = df_side['nfl_player_id_2'].replace(0,ground_id).astype(int)\n",
    "\n",
    "    return df_side, df_end, df_side_map, df_end_map\n",
    "\n",
    "# 0222 update\n",
    "def cnn_features_test_ensemble(train_df, tr_tracking, tr_helmets, tr_video_metadata, model_no_to_use, model_type=\"B\"):\n",
    "    \"\"\"\n",
    "    to load the outputs of first model, set model_no_to_use = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    print(gc.collect())\n",
    "    save_path_end = \"cnn_pred_end.csv\"\n",
    "    save_path_side = \"cnn_pred_side.csv\"\n",
    "    save_path_map_end = \"map_pred_end.csv\"\n",
    "    save_path_map_side = \"map_pred_side.csv\"\n",
    "    if os.path.exists(save_path_end):\n",
    "        ground_id = -1\n",
    "        print(\"read existing files\")\n",
    "        df_end = pd.read_csv(save_path_end)\n",
    "        df_side = pd.read_csv(save_path_side)\n",
    "        df_end_map = pd.read_csv(save_path_map_end)\n",
    "        df_side_map = pd.read_csv(save_path_map_side)\n",
    "        df_end[[\"nfl_player_id_1\", \"nfl_player_id_2\"]] = df_end[[\"nfl_player_id_1\", \"nfl_player_id_2\"]].astype(int)\n",
    "        df_side[[\"nfl_player_id_1\", \"nfl_player_id_2\"]] = df_side[[\"nfl_player_id_1\", \"nfl_player_id_2\"]].astype(int)\n",
    "        df_end_map[\"nfl_player_id\"] = df_end_map[\"nfl_player_id\"].astype(int)\n",
    "        df_side_map[\"nfl_player_id\"] = df_side_map[\"nfl_player_id\"].astype(int)\n",
    "    else:\n",
    "        if \"distance\" in train_df.columns:\n",
    "            dist_btw_players = train_df[\"distance\"].copy()\n",
    "        else:\n",
    "            dist_btw_players = distance(df[\"x_position_1\"], df[\"y_position_1\"], df[\"x_position_2\"], df[\"y_position_2\"])\n",
    "\n",
    "        dist_thresh = 3\n",
    "        ground_id = train_df[\"nfl_player_id_2\"].min() # kmat modelでは0を使用していたのでid No調整。関数出る前に戻す。\n",
    "        train_df_mini = train_df[np.logical_or(dist_btw_players<dist_thresh, train_df[\"nfl_player_id_2\"]==ground_id)].copy()\n",
    "        train_df_mini['nfl_player_id_2'] = train_df_mini['nfl_player_id_2'].replace(ground_id,0).astype(int)\n",
    "        train_df_mini[\"contact\"] = 1. # not use\n",
    "\n",
    "        game_plays = list(train_df_mini[\"game_play\"].unique())\n",
    "        # model_01, preprocessor, _ = get_foldcnntrain_set(train_df, train_01_val_23 = True)\n",
    "        # model_23, preprocessor, _ = get_foldcnntrain_set(train_df, train_01_val_23 = False)\n",
    "        \"\"\"\n",
    "        \n",
    "        model_type = \"A\"\n",
    "        train_fold, val_fold = [1,2,3], [0]\n",
    "        model_0, preprocessor, _ = get_foldcnntrain_set(train_df_mini, train_fold, val_fold, model_type=model_type)\n",
    "        train_fold, val_fold = [0,2,3], [1]\n",
    "        model_1, preprocessor, _ = get_foldcnntrain_set(train_df_mini, train_fold, val_fold, model_type=model_type, not_first_build=True)\n",
    "        train_fold, val_fold = [0,1,3], [2]\n",
    "        model_2, preprocessor, _ = get_foldcnntrain_set(train_df_mini, train_fold, val_fold, model_type=model_type, not_first_build=True)\n",
    "        train_fold, val_fold = [0,1,2], [3]\n",
    "        model_3, preprocessor, _ = get_foldcnntrain_set(train_df_mini, train_fold, val_fold, model_type=model_type, not_first_build=True)\n",
    "        model_A = CNNEnsembler([model_0, model_1, model_2, model_3], num_output_items=3)\n",
    "        \"\"\"\n",
    "        train_fold, val_fold = [1,2,3], [0]\n",
    "        \n",
    "        model_0, preprocessor, _ = get_foldcnntrain_set(train_df_mini, train_fold, val_fold, model_type=model_type)\n",
    "        train_fold, val_fold = [0,2,3], [1]\n",
    "        model_1, preprocessor, _ = get_foldcnntrain_set(train_df_mini, train_fold, val_fold, model_type=model_type, not_first_build=True)\n",
    "        train_fold, val_fold = [0,1,3], [2]\n",
    "        model_2, preprocessor, _ = get_foldcnntrain_set(train_df_mini, train_fold, val_fold, model_type=model_type, not_first_build=True)\n",
    "        train_fold, val_fold = [0,1,2], [3]\n",
    "        model_3, preprocessor, _ = get_foldcnntrain_set(train_df_mini, train_fold, val_fold, model_type=model_type, not_first_build=True)\n",
    "        # model_B = CNNEnsembler([model_0, model_1, model_2, model_3], num_output_items=3)\n",
    "        \n",
    "        models = [model_0, model_1, model_2, model_3]\n",
    "\n",
    "        # model = CNNEnsembler([model_0], num_output_items=3)\n",
    "        print(gc.collect())\n",
    "\n",
    "        # df_end, df_side, df_end_map, df_side_map = pred_by_cnn(train_df_mini, tr_tracking, tr_helmets, tr_video_metadata, game_plays, model, preprocessor, is_train_dataset=False)\n",
    "        df_end, df_side, df_end_map, df_side_map = pred_by_cnn_for_ensemble(train_df_mini, tr_tracking, tr_helmets, tr_video_metadata, game_plays, models, preprocessor, is_train_dataset=False)\n",
    "        df_end.to_csv(save_path_end, index=False)\n",
    "        df_side.to_csv(save_path_side, index=False)\n",
    "        df_end_map.to_csv(save_path_map_end, index=False)\n",
    "        df_side_map.to_csv(save_path_map_side, index=False)\n",
    "\n",
    "    print(gc.collect())\n",
    "    # select output columns and rename\n",
    "    print(df_end.columns)\n",
    "    view = \"Endzone\"\n",
    "    #'cnn_pred_Endzone_0'\n",
    "    columns_pair = [f\"cnn_pred_{view}_{model_no_to_use}\", \"nfl_player_id_1\", \"nfl_player_id_2\",\"game_play\",\"step\",\"frame\"]\n",
    "    columns_single = [f\"pred_coords_i_{view}\", f\"pred_coords_j_{view}\",f\"player_single_contacts_{view}_{model_no_to_use}\",\"nfl_player_id\",\"step\",\"frame\",\"game_play\"]\n",
    "    df_end = df_end[columns_pair].rename(columns={f\"cnn_pred_{view}_{model_no_to_use}\": f\"cnn_pred_{view}\"})\n",
    "    df_end_map = df_end_map[columns_single].rename(columns={f\"player_single_contacts_{view}_{model_no_to_use}\": f\"player_single_contacts_{view}\"})\n",
    "    print(df_end.columns)\n",
    "    \n",
    "    view = \"Sideline\"\n",
    "    columns_pair = [f\"cnn_pred_{view}_{model_no_to_use}\", \"nfl_player_id_1\", \"nfl_player_id_2\",\"game_play\",\"step\",\"frame\"]\n",
    "    columns_single = [f\"pred_coords_i_{view}\", f\"pred_coords_j_{view}\",f\"player_single_contacts_{view}_{model_no_to_use}\",\"nfl_player_id\",\"step\",\"frame\",\"game_play\"]\n",
    "    df_side = df_side[columns_pair].rename(columns={f\"cnn_pred_{view}_{model_no_to_use}\": f\"cnn_pred_{view}\"})\n",
    "    df_side_map = df_side_map[columns_single].rename(columns={f\"player_single_contacts_{view}_{model_no_to_use}\": f\"player_single_contacts_{view}\"})\n",
    "    print(gc.collect())\n",
    "    df_end = postprocess_cnn_all_frame_outputs(df_end)\n",
    "    df_side = postprocess_cnn_all_frame_outputs(df_side)\n",
    "    \n",
    "    df_end['nfl_player_id_2'] = df_end['nfl_player_id_2'].replace(0,ground_id).astype(int)\n",
    "    df_side['nfl_player_id_2'] = df_side['nfl_player_id_2'].replace(0,ground_id).astype(int)\n",
    "\n",
    "    return df_side, df_end, df_side_map, df_end_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da42a121",
   "metadata": {
    "papermill": {
     "duration": 0.052499,
     "end_time": "2023-02-24T23:50:09.287751",
     "exception": false,
     "start_time": "2023-02-24T23:50:09.235252",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## points set matching(kmat)\n",
    "\n",
    "検出対象外も含めて座標ワープする。(フレーム内にいるはずなのに見えていないのか、フレーム外にいるか、を明確に。)\n",
    "\n",
    "- prepare_matching_dataframeのmergeをright(tracking)側に。\n",
    "- 検出箱付きとは別に全プレイヤを一旦残して、出力された変換行列を用いてワープ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e56e1d",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.128795,
     "end_time": "2023-02-24T23:50:09.469467",
     "exception": false,
     "start_time": "2023-02-24T23:50:09.340672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "points set matching.\n",
    "run 1 play at once.\n",
    "\"\"\"\n",
    "\n",
    "def batch_points2points_fitting(targets, sources, padded, num_iter=6, l2_reg=0.1, init_rot=0):\n",
    "    \n",
    "    def get_transmatrix(kx, ky, rz, tx, ty):\n",
    "        \"\"\"\n",
    "        k : log(zoom ratio).\n",
    "        rz : rotation.\n",
    "        tx : x offset.\n",
    "        ty : z offset\n",
    "        shape [batch]\n",
    "        \n",
    "        returns:\n",
    "            transmatrix with shape [batch, 3, 3]\n",
    "        \"\"\"\n",
    "        exp_kx = tf.math.exp(kx)\n",
    "        exp_ky = tf.math.exp(ky)\n",
    "        \n",
    "        sin = tf.math.sin(rz)\n",
    "        cos = tf.math.cos(rz)\n",
    "        mat = tf.stack([[exp_kx*cos, -exp_ky*sin, 1*tx],\n",
    "                        [exp_kx*sin, exp_ky*cos, 1*ty],\n",
    "                        [tf.zeros_like(kx), tf.zeros_like(kx), tf.ones_like(kx)]])\n",
    "        mat = tf.transpose(mat, [2,0,1])\n",
    "        return mat\n",
    "        \n",
    "    def transform_points(transmatrix, points):\n",
    "        x, y = tf.split(points, 2, axis=-1)\n",
    "        xyones = tf.concat([x,y,tf.ones_like(x)], axis=-1)\n",
    "        trans_points = tf.matmul(xyones, transmatrix, transpose_b=True)[...,:2]\n",
    "        return trans_points\n",
    "    \n",
    "    def get_derivative_at(kx, ky, rz, tx, ty, points):\n",
    "        dev = 1e-5\n",
    "        original = transform_points(get_transmatrix(kx, ky, rz, tx, ty), points)\n",
    "        dxy_dkx = (transform_points(get_transmatrix(kx+dev, ky, rz, tx, ty), points) - original) / dev\n",
    "        dxy_dky = (transform_points(get_transmatrix(kx, ky+dev, rz, tx, ty), points) - original) / dev\n",
    "        dxy_drz = (transform_points(get_transmatrix(kx, ky, rz+dev, tx, ty), points) - original) / dev\n",
    "        dxy_dtx = (transform_points(get_transmatrix(kx, ky, rz, tx+dev, ty), points) - original) / dev\n",
    "        dxy_dty = (transform_points(get_transmatrix(kx, ky, rz, tx, ty+dev), points) - original) / dev\n",
    "        return original, dxy_dkx, dxy_dky, dxy_drz, dxy_dtx, dxy_dty\n",
    "    \n",
    "    # initial_values\n",
    "    batch, num_points = tf.unstack(tf.shape(targets))[:2]\n",
    "    kx = 0.0 * tf.ones((batch), tf.float32)#zoom ratio = exp(k)\n",
    "    ky = 0.0 * tf.ones((batch), tf.float32)#zoom ratio = exp(k)\n",
    "    rz = init_rot * tf.ones((batch), tf.float32)\n",
    "    tx = 0.0 * tf.ones((batch), tf.float32)\n",
    "    ty = 0.0 * tf.ones((batch), tf.float32)\n",
    "    \n",
    "    source_origin = sources#tf.stop_gradient(sources)\n",
    "    for i in range(num_iter):\n",
    "        currents, dxy_dkx, dxy_dky, dxy_rz, dxy_dtx, dxy_dty = get_derivative_at(kx, ky, rz, tx, ty, source_origin)\n",
    "        b = tf.reshape((targets-currents)*padded, [batch, num_points*2, 1])#xy flatten\n",
    "        a = tf.stack([dxy_dkx, dxy_dky, dxy_rz, dxy_dtx, dxy_dty], axis=-1)\n",
    "        a = a * padded[...,tf.newaxis]\n",
    "        a = tf.reshape(a, [batch, num_points*2, 5])\n",
    "        updates = tf.linalg.lstsq(a, b, l2_regularizer=l2_reg, fast=True)#batch, 5, 1\n",
    "        kx = tf.maximum(kx + updates[:,0,0], -10)\n",
    "        ky = tf.maximum(ky + updates[:,1,0], -10)\n",
    "        rz = rz + updates[:,2,0]\n",
    "        tx = tx + updates[:,3,0]\n",
    "        ty = ty + updates[:,4,0]\n",
    "           \n",
    "    trans_matrix = get_transmatrix(kx, ky, rz, tx, ty)\n",
    "    trans_sources = transform_points(trans_matrix, sources)\n",
    "    try:\n",
    "        trans_targets = transform_points(tf.linalg.inv(trans_matrix), targets)\n",
    "    except:\n",
    "        trans_targets = targets    \n",
    "    residuals_points = tf.reduce_sum(((targets-trans_sources)*padded)**2, axis=2)\n",
    "    residuals = tf.reduce_sum(residuals_points, axis=1)\n",
    "    return trans_sources, trans_targets, trans_matrix, kx, ky, rz, tx, ty, residuals_points, residuals\n",
    "\n",
    "def batch_points2points_fitting_highdof(targets, sources, padded, num_iter=6, l2_reg=0.1, init_rot=0):\n",
    "    \n",
    "    def get_transmatrix(kx, ky, rz, tx, ty, trape):\n",
    "        \"\"\"\n",
    "        k : log(zoom ratio).\n",
    "        rz : rotation.\n",
    "        tx : x offset.\n",
    "        ty : z offset\n",
    "        shape [batch]\n",
    "        \n",
    "        returns:\n",
    "            transmatrix with shape [batch, 3, 3]\n",
    "        \"\"\"\n",
    "        exp_kx = tf.math.exp(kx)\n",
    "        exp_ky = tf.math.exp(ky)\n",
    "        trape = trape#trapezoid\n",
    "        sin = tf.math.sin(rz)\n",
    "        cos = tf.math.cos(rz)\n",
    "        mat = tf.stack([[exp_kx*cos, trape*exp_kx*cos - exp_ky * sin, 1*tx],\n",
    "                        [exp_kx*sin, trape*exp_kx*sin + exp_ky * cos, 1*ty],\n",
    "                        [tf.zeros_like(kx), tf.zeros_like(kx), tf.ones_like(kx)]])\n",
    "        mat = tf.transpose(mat, [2,0,1])\n",
    "        return mat\n",
    "        \n",
    "    def transform_points(transmatrix, points):\n",
    "        x, y = tf.split(points, 2, axis=-1)\n",
    "        xyones = tf.concat([x,y,tf.ones_like(x)], axis=-1)\n",
    "        trans_points = tf.matmul(xyones, transmatrix, transpose_b=True)[...,:2]\n",
    "        return trans_points\n",
    "    \n",
    "    def get_derivative_at(kx, ky, rz, tx, ty, trape, points):\n",
    "        dev = 1e-5\n",
    "        original = transform_points(get_transmatrix(kx, ky, rz, tx, ty, trape), points)\n",
    "        dxy_dkx = (transform_points(get_transmatrix(kx+dev, ky, rz, tx, ty, trape), points) - original) / dev\n",
    "        dxy_dky = (transform_points(get_transmatrix(kx, ky+dev, rz, tx, ty, trape), points) - original) / dev\n",
    "        dxy_drz = (transform_points(get_transmatrix(kx, ky, rz+dev, tx, ty, trape), points) - original) / dev\n",
    "        dxy_dtx = (transform_points(get_transmatrix(kx, ky, rz, tx+dev, ty, trape), points) - original) / dev\n",
    "        dxy_dty = (transform_points(get_transmatrix(kx, ky, rz, tx, ty+dev, trape), points) - original) / dev\n",
    "        dxy_dttrape = (transform_points(get_transmatrix(kx, ky, rz, tx, ty, trape+dev), points) - original) / dev\n",
    "        return original, dxy_dkx, dxy_dky, dxy_drz, dxy_dtx, dxy_dty, dxy_dttrape\n",
    "    \n",
    "    # initial_values\n",
    "    batch, num_points = tf.unstack(tf.shape(targets))[:2]\n",
    "    kx = 0.0 * tf.ones((batch), tf.float32)#zoom ratio = exp(k)\n",
    "    ky = 0.0 * tf.ones((batch), tf.float32)#zoom ratio = exp(k)\n",
    "    rz = init_rot * tf.ones((batch), tf.float32)\n",
    "    tx = 0.0 * tf.ones((batch), tf.float32)\n",
    "    ty = 0.0 * tf.ones((batch), tf.float32)\n",
    "    trape = 0.0 * tf.ones((batch), tf.float32)\n",
    "    \n",
    "    source_origin = sources#tf.stop_gradient(sources)\n",
    "    for i in range(num_iter):\n",
    "        currents, dxy_dkx, dxy_dky, dxy_rz, dxy_dtx, dxy_dty, dxy_dttrape = get_derivative_at(kx, ky, rz, tx, ty, trape, source_origin)\n",
    "        b = tf.reshape((targets-currents)*padded, [batch, num_points*2, 1])#xy flatten\n",
    "        a = tf.stack([dxy_dkx, dxy_dky, dxy_rz, dxy_dtx, dxy_dty, dxy_dttrape], axis=-1)\n",
    "        a = a * padded[...,tf.newaxis]\n",
    "        a = tf.reshape(a, [batch, num_points*2, 6])\n",
    "        updates = tf.linalg.lstsq(a, b, l2_regularizer=l2_reg, fast=True)#batch, 6, 1\n",
    "        kx = tf.maximum(kx + updates[:,0,0], -10)\n",
    "        ky = tf.maximum(ky + updates[:,1,0], -10)\n",
    "        rz = rz + updates[:,2,0]\n",
    "        tx = tx + updates[:,3,0]\n",
    "        ty = ty + updates[:,4,0]\n",
    "        trape = tf.clip_by_value(trape + updates[:,5,0], -0.25, 0.25)#\n",
    "           \n",
    "    trans_matrix = get_transmatrix(kx, ky, rz, tx, ty, trape)\n",
    "    trans_sources = transform_points(trans_matrix, sources)\n",
    "    try:\n",
    "        trans_targets = transform_points(tf.linalg.inv(trans_matrix), targets)\n",
    "    except:\n",
    "        trans_targets = targets    \n",
    "    residuals_points = tf.reduce_sum(((targets-trans_sources)*padded)**2, axis=2)\n",
    "    residuals = tf.reduce_sum(residuals_points, axis=1)\n",
    "    return trans_sources, trans_targets, trans_matrix, kx, ky, rz, tx, ty, residuals_points, residuals\n",
    "\n",
    "def transform_xyz_to_img_coords(points, trans_matrix, inverse=True):\n",
    "    if inverse:\n",
    "        trans_matrix = tf.linalg.inv(trans_matrix)\n",
    "    x, y = tf.split(points, 2, axis=-1)\n",
    "    xyones = tf.concat([x,y,tf.ones_like(x)], axis=-1)\n",
    "    trans_points = tf.matmul(xyones, trans_matrix, transpose_b=True)[...,:2]\n",
    "    return trans_points\n",
    "\n",
    "def possible_misassignment(trans_sources, targets, padded, padded_player_ids, top_n=2):\n",
    "    \"\"\"\n",
    "    batch, num_player, 2(x,y)\n",
    "    \"\"\"\n",
    "    batch, num_player, _ = tf.unstack(tf.shape(targets))\n",
    "    dists = tf.reduce_sum((targets[:,:,tf.newaxis,:] - trans_sources[:,tf.newaxis,:,:])**2, axis=3)\n",
    "    padded_area = 1 - padded[:,:,tf.newaxis,0] * padded[:,tf.newaxis,:,0]\n",
    "    current_asign = tf.eye(num_player, dtype=dists.dtype)[tf.newaxis, :,:]\n",
    "    dists = dists + (padded_area + current_asign) * 1e7\n",
    "    argsort = tf.argsort(dists, axis=2)[:,:,:top_n]\n",
    "    possible_residuals = tf.gather(dists, argsort, batch_dims=2)\n",
    "    possible_player_ids = tf.gather(padded_player_ids, argsort, batch_dims=1)\n",
    "    return possible_residuals, possible_player_ids\n",
    "\n",
    "def pad_for_batch(list_tf_targets, list_tf_sources, list_tf_player_ids):\n",
    "    num_points = [points.shape[0] for points in list_tf_targets]\n",
    "    max_num = max(num_points)\n",
    "    num_pads = [max_num - num for num in num_points]\n",
    "    list_tf_targets = [tf.pad(points, [[0,pad_num],[0,0]]) for points, pad_num in zip(list_tf_targets, num_pads)]\n",
    "    list_tf_sources = [tf.pad(points, [[0,pad_num],[0,0]]) for points, pad_num in zip(list_tf_sources, num_pads)]\n",
    "    list_tf_player_ids = [tf.pad(ids, [[0,pad_num]], constant_values=-2) for ids, pad_num in zip(list_tf_player_ids, num_pads)]\n",
    "    tf_targets = tf.stack(list_tf_targets)\n",
    "    tf_sources = tf.stack(list_tf_sources)\n",
    "    tf_player_ids = tf.stack(list_tf_player_ids)\n",
    "    tf_pad_bools = tf.cast(tf.stack([tf.range(max_num) < num for num in num_points]), tf_targets.dtype)[:,:,tf.newaxis]\n",
    "    return tf_targets, tf_sources, tf_player_ids, tf_pad_bools, num_points\n",
    "\n",
    "def pad_for_batch_single_item(list_tf_targets):\n",
    "    num_points = [points.shape[0] for points in list_tf_targets]\n",
    "    max_num = max(num_points)\n",
    "    num_pads = [max_num - num for num in num_points]\n",
    "    list_tf_targets = [tf.pad(points, [[0,pad_num],[0,0]]) for points, pad_num in zip(list_tf_targets, num_pads)]\n",
    "    tf_targets = tf.stack(list_tf_targets)\n",
    "    tf_pad_bools = tf.cast(tf.stack([tf.range(max_num) < num for num in num_points]), tf_targets.dtype)[:,:,tf.newaxis]\n",
    "    return tf_targets, tf_pad_bools, num_points\n",
    "\n",
    "def depad_for_batch(tf_items, num_points):\n",
    "    list_np_items = [[points[:num] for points, num in zip(val.numpy(), num_points)] for val in tf_items]\n",
    "    #list_tf_sources = [points[:num] for points, num in zip(tf_sources.numpy(), num_points)]\n",
    "    return list_np_items\n",
    "\n",
    "def p2p_registration_features(tr_tracking, tr_helmets, tr_video_metadata, p2p_file = \"pathto/p2p_registration_residuals.csv\", view_some_results=False, num_possible_assign=2):\n",
    "    \"\"\"\n",
    "    num_possible_assign: \n",
    "    アサインメントミスの可能性のあるプレイヤを抽出する。\n",
    "    具体的には現在アサインされている以外に近いプレイヤを抽出する。同時にもしそのプレイヤであった場合の残差も出す(減残差と比較することで、あとでマスク的に使う)\n",
    "    \"\"\"\n",
    "    if p2p_file is not None and os.path.exists(p2p_file):\n",
    "        df_p2p_regist = pd.read_csv(p2p_file)\n",
    "        df_all_players_img_coords = pd.read_csv(p2p_file.replace(\".csv\", \"_img_coords.csv\"))\n",
    "        \n",
    "    else:\n",
    "        S = time.time()\n",
    "        outputs = []\n",
    "        outputs_all_players_img_coords = []\n",
    "        for game_play in tr_tracking[\"game_play\"].unique():\n",
    "            print(game_play)\n",
    "            for view in [\"Sideline\", \"Endzone\"]:\n",
    "                helm_merged = prepare_matching_dataframe(game_play, tr_tracking, tr_helmets, tr_video_metadata, view, fps=59.94, only_center_of_step=True)\n",
    "                helm_merged[\"box_x\"] = helm_merged[\"left\"] + helm_merged[\"width\"]/2\n",
    "                helm_merged[\"box_y\"] = helm_merged[\"top\"] + helm_merged[\"height\"]/2\n",
    "                \n",
    "                scale_rate = 10\n",
    "                list_targets = []\n",
    "                list_sources = []\n",
    "                list_flip_target = []\n",
    "                list_flip_target_all = []\n",
    "                list_box_centers = []\n",
    "                player_ids = []\n",
    "                df_out = []\n",
    "                df_out_all_players = []\n",
    "                for frame, df_frame in helm_merged.groupby(\"frame\"):\n",
    "                    #if len(df_frame)!=22:\n",
    "                    #    print(\"\")\n",
    "                    df_frame_bbox = df_frame[~df_frame[\"box_x\"].isna()]\n",
    "                    \n",
    "                    box_x = df_frame_bbox[\"box_x\"].values\n",
    "                    box_y = df_frame_bbox[\"box_y\"].values\n",
    "                    box_x_mean = box_x.mean()\n",
    "                    box_y_mean = box_y.mean()\n",
    "\n",
    "                    pos_x = df_frame_bbox[\"x_position\"].values\n",
    "                    pos_y = df_frame_bbox[\"y_position\"].values\n",
    "                    pos_x_mean = pos_x.mean()\n",
    "                    pos_y_mean = pos_y.mean()\n",
    "                    \n",
    "                    pos_x_all_player = df_frame[\"x_position\"].values\n",
    "                    pos_y_all_player = df_frame[\"y_position\"].values\n",
    "                    \n",
    "                    if np.isnan(box_x_mean) or np.isnan(pos_x_mean):\n",
    "                        #print(\"nan exist\")\n",
    "                        continue\n",
    "\n",
    "                    adjusted_bx = (box_x - box_x_mean) / scale_rate\n",
    "                    adjusted_by = (box_y - box_y_mean) / scale_rate\n",
    "\n",
    "                    adjusted_px = pos_x - pos_x_mean\n",
    "                    adjusted_py = pos_y - pos_y_mean\n",
    "\n",
    "                    adjusted_px_all = pos_x_all_player - pos_x_mean\n",
    "                    adjusted_py_all = pos_y_all_player - pos_y_mean\n",
    "\n",
    "                    list_targets += [tf.cast(tf.stack([adjusted_px, adjusted_py], axis=-1), tf.float32)]\n",
    "                    list_sources += [tf.cast(tf.stack([adjusted_bx, adjusted_by], axis=-1), tf.float32)]\n",
    "\n",
    "                    list_flip_target += [tf.cast(tf.stack([-adjusted_px, adjusted_py], axis=-1), tf.float32)]\n",
    "                    list_flip_target_all += [tf.cast(tf.stack([-adjusted_px_all, adjusted_py_all], axis=-1), tf.float32)]\n",
    "                    list_box_centers += [[box_x_mean, box_y_mean]]\n",
    "\n",
    "                    player_ids += [tf.cast(df_frame_bbox[\"nfl_player_id\"].values, tf.int32)]\n",
    "\n",
    "                    df_frame_bbox[\"average_box_size\"] = (np.sqrt(df_frame_bbox[\"width\"] * df_frame_bbox[\"height\"])).mean()\n",
    "                    df_out.append(df_frame_bbox[[\"game_play\",\"view\",\"frame\",\"step\",\"nfl_player_id\",\"average_box_size\"]].copy())\n",
    "                    df_out_all_players.append(df_frame[[\"game_play\",\"view\",\"frame\",\"step\",\"nfl_player_id\"]].copy())\n",
    "                    #print(df_frame[[\"game_play\",\"view\",\"frame\",\"step\",\"nfl_player_id\"]].isna().sum(axis=0))\n",
    "\n",
    "                tf_targets, tf_sources, tf_player_ids, tf_pad_bools, num_points = pad_for_batch(list_targets, list_sources, player_ids)\n",
    "                tf_flip_targets, tf_sources, _, tf_pad_bools, num_points = pad_for_batch(list_flip_target, list_sources, player_ids)\n",
    "                \n",
    "                tf_flip_targets_all, tf_pad_bools_all, num_points_all = pad_for_batch_single_item(list_flip_target_all) # 追加。全プレイヤ出力する。\n",
    "\n",
    "                with tf.device('/CPU:0'): # gpu少し怪しい。\n",
    "                    #result_flip = batch_points2points_fitting(tf_flip_targets, tf_sources, tf_pad_bools, num_iter=30, l2_reg=100., init_rot=0)\n",
    "                    #result_flip_180 = batch_points2points_fitting(tf_flip_targets, tf_sources, tf_pad_bools, num_iter=30, l2_reg=100., init_rot=3.14)\n",
    "                    result_flip = batch_points2points_fitting_highdof(tf_flip_targets, tf_sources, tf_pad_bools, num_iter=30, l2_reg=100., init_rot=0)\n",
    "                    result_flip_180 = batch_points2points_fitting_highdof(tf_flip_targets, tf_sources, tf_pad_bools, num_iter=30, l2_reg=100., init_rot=3.14)\n",
    "                    \n",
    "                # take smaller (better fit)\n",
    "                result_flip_stack = [tf.stack([r1,r2], axis=-1) for r1, r2 in zip(result_flip, result_flip_180)]\n",
    "                better = tf.argmin(result_flip_stack[-1], axis=-1) # last index is residual\n",
    "                result_flip = [tf.gather(result, better, batch_dims=1, axis=-1) for result in result_flip_stack]\n",
    "\n",
    "                flip_residual_median = np.median(result_flip[-1])\n",
    "                print(f\"{game_play}, {view}, residual flip: {flip_residual_median}\")\n",
    "\n",
    "                result = result_flip\n",
    "                view_target = tf_flip_targets\n",
    "\n",
    "                trans_sources, trans_targets, trans_matrix, kx, ky, rz, tx, ty, residual_points, residual = result\n",
    "                \n",
    "                # transform all players (include non-detected players)\n",
    "                tf_souce_all_players = transform_xyz_to_img_coords(tf_flip_targets_all, trans_matrix, inverse=True)                \n",
    "                all_players_img_coords = scale_rate * tf_souce_all_players + tf.cast(list_box_centers, tf.float32)[:,tf.newaxis,:]                \n",
    "                list_targets, list_sources, list_residual = depad_for_batch([view_target, trans_sources, residual_points],\n",
    "                                                                            num_points) \n",
    "                all_players_img_coords = depad_for_batch([all_players_img_coords], num_points_all)[0] \n",
    "               \n",
    "\n",
    "                tiled_resisual = []\n",
    "                for r, num_p in zip(residual.numpy(), num_points):\n",
    "                    tiled_resisual += [r] * num_p \n",
    "\n",
    "                df_out = pd.concat(df_out, axis=0)\n",
    "                df_out[[\"x_position_offset_on_img\", \"y_position_offset_on_img\"]] = (np.concatenate(list_sources, axis=0) - np.concatenate(list_targets, axis=0)) * scale_rate\n",
    "                df_out[\"p2p_registration_residual\"] = np.concatenate(list_residual, axis=0)\n",
    "                df_out[\"p2p_registration_residual_frame\"] = tiled_resisual\n",
    "                outputs.append(df_out)\n",
    "\n",
    "                df_out_all_players = pd.concat(df_out_all_players, axis=0)\n",
    "                df_out_all_players[[\"img_coords_x\", \"img_coords_y\"]] = np.concatenate(all_players_img_coords, axis=0)\n",
    "                outputs_all_players_img_coords.append(df_out_all_players)\n",
    "                if view_some_results:\n",
    "                    for t, s in zip(list_targets[::50], list_sources[::50]):\n",
    "                        plt.figure(figsize=(8,5))\n",
    "                        words = np.arange(len(t))\n",
    "                        plt.scatter(t[:,0], t[:,1], c=\"blue\", s=50, alpha=0.5)#, c=np.arange(len(t)))#, cmap=\"gray\")#color=\"blue\")\n",
    "                        for number, [x,y] in enumerate(t): \n",
    "                            plt.text(x, y+.8, number, fontsize=9, c=\"blue\")\n",
    "                            #plt.annotate(number, (x, y), c=\"blue\")\n",
    "                        plt.scatter(s[:,0], s[:,1], c=\"red\", s=50, alpha=0.5)#, c=np.arange(len(s)))#, cmap=\"gray\")#color=\"red\")\n",
    "                        for number, [x,y] in enumerate(s): \n",
    "                            plt.text(x, y-1.75, number, fontsize=9, c=\"red\")\n",
    "                            #plt.annotate(number, (x, y), c=\"red\")\n",
    "                        #plt.text(s[:,0]+.3, s[:,1]+.3, words, fontsize=9)\n",
    "                        plt.grid()\n",
    "                        plt.title(\"tracking X-Y  VS  transformed image coord\")\n",
    "                        plt.show()            \n",
    "        df_p2p_regist = pd.concat(outputs, axis=0)  \n",
    "        df_all_players_img_coords = pd.concat(outputs_all_players_img_coords, axis=0)\n",
    "        print(time.time()-S)\n",
    "        if p2p_file is not None:\n",
    "            df_p2p_regist.to_csv(p2p_file, index=False)\n",
    "            df_all_players_img_coords.to_csv(p2p_file.replace(\".csv\", \"_img_coords.csv\"), index=False)\n",
    "    return df_p2p_regist, df_all_players_img_coords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3257b1d5",
   "metadata": {
    "papermill": {
     "duration": 0.053164,
     "end_time": "2023-02-24T23:50:09.576554",
     "exception": false,
     "start_time": "2023-02-24T23:50:09.523390",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 以下2つの関数を make_features内で使用\n",
    "df_p2p_regist_img_coordsのmerge追加。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aa4d00",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.135359,
     "end_time": "2023-02-24T23:50:09.764877",
     "exception": false,
     "start_time": "2023-02-24T23:50:09.629518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def p2p_matching_features(train_df, tr_tracking, phase):\n",
    "    if phase == \"test\":\n",
    "        p2p_file = None\n",
    "    else:\n",
    "        p2p_file = f\"{cfg.KMAT_PATH}/output/p2p_registration_residuals.csv\"\n",
    "    tr_helmets, tr_video_metadata = load_helmet_meta(cfg, phase=phase)\n",
    "    # registrationのoutputに全プレイヤのimg_coordsを追加。\n",
    "    df_p2p_regist, df_p2p_regist_img_coords = p2p_registration_features(tr_tracking, tr_helmets, tr_video_metadata, p2p_file = p2p_file)\n",
    "    \n",
    "    df_p2p_regist[\"x_rel_position_offset_on_img\"] = df_p2p_regist[\"x_position_offset_on_img\"] / (df_p2p_regist[\"average_box_size\"]+1e-7)\n",
    "    df_p2p_regist[\"y_rel_position_offset_on_img\"] = df_p2p_regist[\"y_position_offset_on_img\"] / (df_p2p_regist[\"average_box_size\"]+1e-7)\n",
    "\n",
    "    merge_columns = [\"game_play\",\"step\",\"nfl_player_id\",\"x_position_offset_on_img\",\"y_position_offset_on_img\",\"x_rel_position_offset_on_img\",\"y_rel_position_offset_on_img\",\n",
    "                    \"p2p_registration_residual\", \n",
    "                     \"p2p_registration_residual_frame\"\n",
    "                    ]\n",
    "    merge_columns_img_coords = [\"game_play\",\"step\",\"nfl_player_id\"] + [\"img_coords_x\", \"img_coords_y\"]\n",
    "    \n",
    "    for pair_no in [1]:\n",
    "        train_df = pd.merge(train_df, df_p2p_regist.loc[df_p2p_regist[\"view\"]==\"Sideline\", merge_columns], how=\"left\", \n",
    "                     left_on=[\"game_play\",\"step\",f\"nfl_player_id_{pair_no}\"],\n",
    "                     right_on=[\"game_play\",\"step\",\"nfl_player_id\"])\n",
    "        train_df.rename(columns={\"x_position_offset_on_img\":f\"x_position_offset_on_img_Side_{pair_no}\",\n",
    "                                                                                            \"y_position_offset_on_img\":f\"y_position_offset_on_img_Side_{pair_no}\",\n",
    "                                                                                            \"x_rel_position_offset_on_img\":f\"x_rel_position_offset_on_img_Side_{pair_no}\",\n",
    "                                                                                            \"y_rel_position_offset_on_img\":f\"y_rel_position_offset_on_img_Side_{pair_no}\",\n",
    "                                                                                            \"p2p_registration_residual\": f\"nfl_player_id_{pair_no}_Side_residual\",\n",
    "                                                                                            \"p2p_registration_residual_frame\": f\"nfl_player_id_{pair_no}_Side_residual_frame\",\n",
    "                                                                                            }, \n",
    "                        inplace=True)\n",
    "        train_df.drop(columns=[\"nfl_player_id\"], inplace=True)\n",
    "\n",
    "        train_df = pd.merge(train_df, df_p2p_regist.loc[df_p2p_regist[\"view\"]==\"Endzone\", merge_columns], how=\"left\", \n",
    "                     left_on=[\"game_play\",\"step\",f\"nfl_player_id_{pair_no}\"],\n",
    "                     right_on=[\"game_play\",\"step\",\"nfl_player_id\"])\n",
    "        train_df.rename(columns={\"x_position_offset_on_img\":f\"x_position_offset_on_img_End_{pair_no}\",\n",
    "                                                                                            \"y_position_offset_on_img\":f\"y_position_offset_on_img_End_{pair_no}\",\n",
    "                                                                                            \"x_rel_position_offset_on_img\":f\"x_rel_position_offset_on_img_End_{pair_no}\",\n",
    "                                                                                            \"y_rel_position_offset_on_img\":f\"y_rel_position_offset_on_img_End_{pair_no}\",\n",
    "                                                                                            \"p2p_registration_residual\":f\"nfl_player_id_{pair_no}_End_residual\",\n",
    "                                                                                            \"p2p_registration_residual_frame\": f\"nfl_player_id_{pair_no}_End_residual_frame\",\n",
    "                                                                                            },\n",
    "                       inplace=True)\n",
    "        train_df.drop(columns=[\"nfl_player_id\"], inplace=True)\n",
    "    \n",
    "        \"\"\"\n",
    "        # image coords とりあえず外しときます。\n",
    "        print(train_df.shape)\n",
    "        train_df = pd.merge(train_df, df_p2p_regist_img_coords.loc[df_p2p_regist_img_coords[\"view\"]==\"Sideline\", merge_columns_img_coords], how=\"left\", \n",
    "                     left_on=[\"game_play\",\"step\",f\"nfl_player_id_{pair_no}\"],\n",
    "                     right_on=[\"game_play\",\"step\",\"nfl_player_id\"]).rename(columns={\"img_coords_x\":f\"img_coords_x_Side{pair_no}\",\n",
    "                                                                                    \"img_coords_y\":f\"img_coords_y_Side{pair_no}\",\n",
    "                                                                                    }).drop(columns=[\"nfl_player_id\"])\n",
    "        train_df = pd.merge(train_df, df_p2p_regist_img_coords.loc[df_p2p_regist_img_coords[\"view\"]==\"Endzone\", merge_columns_img_coords], how=\"left\", \n",
    "                     left_on=[\"game_play\",\"step\",f\"nfl_player_id_{pair_no}\"],\n",
    "                     right_on=[\"game_play\",\"step\",\"nfl_player_id\"]).rename(columns={\"img_coords_x\":f\"img_coords_x_End{pair_no}\",\n",
    "                                                                                    \"img_coords_y\":f\"img_coords_y_End{pair_no}\",\n",
    "                                                                                    }).drop(columns=[\"nfl_player_id\"])\n",
    "    \n",
    "        print(train_df.shape)\n",
    "        \"\"\"\n",
    "        \n",
    "    train_df.head()\n",
    "    return train_df\n",
    "\n",
    "# 0219 update\n",
    "def cnn_features(train_df, tr_tracking, phase, model_type=\"B\"):\n",
    "    tr_helmets, tr_video_metadata = load_helmet_meta(cfg, phase=phase) # 何回もロードしてマジすいません。\n",
    "    \n",
    "    if phase == \"test\":\n",
    "        df_side, df_end, df_side_map, df_end_map = cnn_features_test(train_df, tr_tracking, tr_helmets, tr_video_metadata, model_type=model_type)\n",
    "        # df_side, df_end, df_side_map, df_end_map = cnn_features_test_ensemble(train_df, tr_tracking, tr_helmets, tr_video_metadata, model_no_to_use, model_type=model_type)\n",
    "    else:\n",
    "        cnn_pred_path = f\"{cfg.KMAT_PATH}/output/pred_model_{model_type}/\"  \n",
    "        # cnn_pred_path = \"CROPCNN/\"  \n",
    "        df_side, df_end, df_side_map, df_end_map = cnn_features_val(train_df, tr_tracking, tr_helmets, tr_video_metadata, cnn_pred_path=cnn_pred_path, model_type=model_type)\n",
    "  \n",
    "    print(train_df.shape)\n",
    "\n",
    "    # もともとのペアコンタクト予測\n",
    "    train_df = pd.merge(train_df, \n",
    "                      df_side\n",
    "                      [[\"game_play\", \"step\", \"nfl_player_id_1\", \"nfl_player_id_2\", \"cnn_pred_Sideline\"]], \n",
    "                        on=[\"game_play\", \"step\", \"nfl_player_id_1\", \"nfl_player_id_2\"], how=\"left\")\n",
    "    train_df = pd.merge(train_df, \n",
    "                      df_end\n",
    "                      [[\"game_play\", \"step\", \"nfl_player_id_1\", \"nfl_player_id_2\",\"cnn_pred_Endzone\"]], \n",
    "                      on=[\"game_play\", \"step\", \"nfl_player_id_1\", \"nfl_player_id_2\"], how=\"left\")\n",
    "    print(train_df.shape)\n",
    "\n",
    "    del df_side, df_end\n",
    "    gc.collect()\n",
    "\n",
    "    camaro_df = pd.read_csv(cfg.CAMARO_DF1_PATH)\n",
    "    camaro_df['camaro_pred'] = np.nan  # np.nanじゃないとroll feature作れなかった\n",
    "    camaro_df['camaro_pred'] = camaro_df['camaro_pred'].astype(np.float32)\n",
    "    camaro_df.loc[camaro_df['masks'], 'camaro_pred'] = camaro_df.loc[camaro_df['masks'], 'preds']\n",
    "    merge_cols1 = ['game_play', 'step', 'nfl_player_id_1', 'nfl_player_id_2', 'camaro_pred']\n",
    "    train_df = train_df.merge(camaro_df[merge_cols1], how='left')\n",
    "    \n",
    "    del camaro_df\n",
    "    gc.collect()\n",
    "    \n",
    "    camaro_df2 = pd.read_csv(cfg.CAMARO_DF2_PATH)\n",
    "    camaro_df2['camaro_pred2'] = np.nan  # np.nanじゃないとroll feature作れなかった\n",
    "    camaro_df2['camaro_pred2'] = camaro_df2['camaro_pred2'].astype(np.float32)\n",
    "    camaro_df2.loc[camaro_df2['masks'], 'camaro_pred2'] = camaro_df2.loc[camaro_df2['masks'], 'preds']\n",
    "    merge_cols2 = ['game_play', 'step', 'nfl_player_id_1', 'nfl_player_id_2', 'camaro_pred2']\n",
    "    train_df = train_df.merge(camaro_df2[merge_cols2], how='left')\n",
    "    print('after merging camaro df:', train_df.shape)\n",
    "    \n",
    "    del camaro_df2\n",
    "    gc.collect()\n",
    "    \n",
    "    df_side_map = reduce_dtype(df_side_map)\n",
    "    df_end_map = reduce_dtype(df_end_map)\n",
    "\n",
    "    # それ以外のシングルプレイヤー系の予測値。座標予測と、単独でのコンタクト予測\n",
    "    new_columns = [] \n",
    "    for pid in [1,2]:\n",
    "        train_df = pd.merge(train_df, \n",
    "                          df_end_map\n",
    "                          [[\"game_play\", \"step\", \"nfl_player_id\",\"pred_coords_i_Endzone\",\"pred_coords_j_Endzone\", \"player_single_contacts_Endzone\"]], \n",
    "                          left_on=[\"game_play\", \"step\", f\"nfl_player_id_{pid}\"],\n",
    "                          right_on=[\"game_play\", \"step\", \"nfl_player_id\"], how=\"left\")\n",
    "        train_df.drop(columns=[\"nfl_player_id\"], inplace=True)\n",
    "        train_df.rename(\n",
    "            columns={\n",
    "                \"pred_coords_i_Endzone\": f\"pred_coords_i_Endzone_pid{pid}\",\n",
    "                \"pred_coords_j_Endzone\": f\"pred_coords_j_Endzone_pid{pid}\", \n",
    "                \"player_single_contacts_Endzone\": f\"player_single_contacts_Endzone_pid{pid}\"\n",
    "            },\n",
    "            inplace=True)\n",
    "        \n",
    "        train_df = pd.merge(train_df, \n",
    "                          df_side_map\n",
    "                          [[\"game_play\", \"step\", \"nfl_player_id\",\"pred_coords_i_Sideline\",\"pred_coords_j_Sideline\", \"player_single_contacts_Sideline\"]], \n",
    "                          left_on=[\"game_play\", \"step\", f\"nfl_player_id_{pid}\"],\n",
    "                          right_on=[\"game_play\", \"step\", \"nfl_player_id\"], how=\"left\")\n",
    "        train_df.drop(columns=[\"nfl_player_id\"], inplace=True)\n",
    "        train_df.rename(\n",
    "            columns={\n",
    "                \"pred_coords_i_Sideline\": f\"pred_coords_i_Sideline_pid{pid}\",\n",
    "                \"pred_coords_j_Sideline\": f\"pred_coords_j_Sideline_pid{pid}\", \n",
    "                \"player_single_contacts_Sideline\": f\"player_single_contacts_Sideline_pid{pid}\"\n",
    "            },\n",
    "            inplace=True\n",
    "        )\n",
    "        \n",
    "        new_columns += [f\"pred_coords_i_Endzone_pid{pid}\", f\"pred_coords_j_Endzone_pid{pid}\", f\"player_single_contacts_Endzone_pid{pid}\",\n",
    "                       f\"pred_coords_i_Sideline_pid{pid}\", f\"pred_coords_j_Sideline_pid{pid}\", f\"player_single_contacts_Sideline_pid{pid}\"]\n",
    "        gc.collect()\n",
    "    \n",
    "    del df_side_map, df_end_map\n",
    "    gc.collect()\n",
    " \n",
    "    # add some features\n",
    "    for view in [\"Endzone\", \"Sideline\"]:\n",
    "        train_df[f\"pred_coords_delta_i_{view}\"] = np.abs(train_df[f\"pred_coords_i_{view}_pid1\"] - train_df[f\"pred_coords_i_{view}_pid2\"])\n",
    "        train_df[f\"pred_coords_delta_j_{view}\"] = np.abs(train_df[f\"pred_coords_j_{view}_pid1\"] - train_df[f\"pred_coords_j_{view}_pid2\"])\n",
    "        train_df[f\"pred_coords_dist_{view}\"] = np.sqrt(train_df[f\"pred_coords_delta_i_{view}\"]**2 + train_df[f\"pred_coords_delta_j_{view}\"]**2)\n",
    "        train_df[f\"player_single_contacts_multiply_{view}\"] = train_df[f\"player_single_contacts_{view}_pid1\"] * train_df[f\"player_single_contacts_{view}_pid2\"]\n",
    "        train_df[f\"pred_coords_distratio_{view}\"] = train_df[f\"pred_coords_dist_{view}\"] / train_df[\"distance\"]\n",
    "        train_df[f\"player_single_pair_relative_contacts_{view}_pid1\"] = train_df[f\"cnn_pred_{view}\"] / train_df[f\"player_single_contacts_{view}_pid1\"]\n",
    "        train_df[f\"player_single_pair_relative_contacts_{view}_pid2\"] = train_df[f\"cnn_pred_{view}\"] / train_df[f\"player_single_contacts_{view}_pid2\"]\n",
    "        new_columns += [f\"pred_coords_delta_i_{view}\", f\"pred_coords_delta_j_{view}\", \n",
    "                       f\"pred_coords_dist_{view}\", f\"player_single_contacts_multiply_{view}\",\n",
    "                       f\"pred_coords_distratio_{view}\", \n",
    "                       f\"player_single_pair_relative_contacts_{view}_pid1\", f\"player_single_pair_relative_contacts_{view}_pid2\"]\n",
    "        \n",
    "    # pair contact (different view\n",
    "    train_df[\"player_single_contacts_multiply_EndSide_12\"] = train_df[\"player_single_contacts_Endzone_pid1\"] * train_df[\"player_single_contacts_Sideline_pid2\"]\n",
    "    train_df[\"player_single_contacts_multiply_EndSide_21\"] = train_df[\"player_single_contacts_Endzone_pid2\"] * train_df[\"player_single_contacts_Sideline_pid1\"]\n",
    "    \n",
    "    # train_df[new_columns].to_csv(\"not_pair_predictions.csv\", index=False)\n",
    "    \n",
    "    \n",
    "    del train_df[\"datetime_ngs\"]\n",
    "    return train_df\n",
    "\n",
    "def merege_2nd_candidate(df_inp, col_score, col_p1, col_p2, col_p1_err, col_p2_err, col_p1_err_base, col_p2_err_base, thresh_rate=1.0, suffix=\"A\"):\n",
    "    #\"col_score\": \"cnn_pred_Sideline\"\n",
    "    #\"col_p1\": \"nfl_player_id_1_Side_A\"\n",
    "    #\"col_p2\": \"nfl_player_id_2\"\n",
    "    #\"col_p1_err\": \"nfl_player_id_1_Side_A_residual\"\n",
    "    #\"col_p2_err\": \"nfl_player_id_2_Side_residual\"\n",
    "    #\"col_p1_err_base\": \"nfl_player_id_1_Side_residual\"\n",
    "    #\"col_p2_err_base\": \"nfl_player_id_2_Side_residual\"\n",
    "     \n",
    "    valid_pair = np.logical_and(df_inp[col_p1_err]<=(df_inp[col_p1_err_base]*thresh_rate), df_inp[col_p2_err]<=(df_inp[col_p2_err_base]*thresh_rate))\n",
    "\n",
    "    df = df_inp.loc[valid_pair, [\"game_play\",\"step\", col_p1, col_p2, col_score]].copy()\n",
    "    #mask = np.logical_and()\n",
    "    df = df.rename(columns={col_p1:\"nfl_player_id_1\", col_p2:\"nfl_player_id_2\",\n",
    "                            col_score: f\"{col_score}_{suffix}\"\n",
    "                           })\n",
    "    df_inp = pd.merge(df_inp, df, on=[\"game_play\", \"step\", \"nfl_player_id_1\", \"nfl_player_id_2\"], how=\"left\")\n",
    "    return df_inp\n",
    "\n",
    "def num_player_contact_with_other_player(df_inp):\n",
    "    df = df_inp[[\"game_play\", \"step\", \"nfl_player_id_1\", \"nfl_player_id_2\", \"cnn_pred_Sideline\", \"cnn_pred_Endzone\"]].copy()\n",
    "    df = pd.concat([df, df.rename(columns={\"nfl_player_id_1\":\"nfl_player_id_2\", \"nfl_player_id_2\":\"nfl_player_id_1\"})], axis=0)\n",
    "    df = df.groupby([\"game_play\", \"step\", \"nfl_player_id_1\"])[[\"cnn_pred_Sideline\", \"cnn_pred_Endzone\"]].max().reset_index()\n",
    "    df = df.rename(columns={\"cnn_pred_Sideline\":\"cnn_pred_Sideline_player_average\", \"cnn_pred_Endzone\":\"cnn_pred_Endzone_player_average\", \"nfl_player_id_1\":\"nfl_player_id\"})\n",
    "\n",
    "    df_p = df[df[\"nfl_player_id\"]!=-1] # not ground\n",
    "    df_g = df[df[\"nfl_player_id\"]==-1] # ground\n",
    "    \n",
    "    df_inp = pd.merge(df_inp, df_p, left_on=[\"game_play\", \"step\", \"nfl_player_id_1\"], right_on=[\"game_play\", \"step\", \"nfl_player_id\"], how=\"left\").drop(columns=[\"nfl_player_id\"])\n",
    "    df_inp = df_inp.rename(columns={\"cnn_pred_Sideline_player_average\":\"cnn_pred_Sideline_player_average_1\", \"cnn_pred_Endzone_player_average\":\"cnn_pred_Endzone_player_average_1\"})\n",
    "    df_inp = pd.merge(df_inp, df_p, left_on=[\"game_play\", \"step\", \"nfl_player_id_2\"], right_on=[\"game_play\", \"step\", \"nfl_player_id\"], how=\"left\").drop(columns=[\"nfl_player_id\"])\n",
    "    df_inp = df_inp.rename(columns={\"cnn_pred_Sideline_player_average\":\"cnn_pred_Sideline_player_average_2\", \"cnn_pred_Endzone_player_average\":\"cnn_pred_Endzone_player_average_2\"})\n",
    "        \n",
    "    df_inp[\"cnn_pred_Sideline_player_average_1\"] = df_inp[\"cnn_pred_Sideline_player_average_1\"] * (df_inp[\"cnn_pred_Sideline\"]!=df_inp[\"cnn_pred_Sideline_player_average_1\"])\n",
    "    df_inp[\"cnn_pred_Endzone_player_average_1\"] = df_inp[\"cnn_pred_Endzone_player_average_1\"] * (df_inp[\"cnn_pred_Endzone\"]!=df_inp[\"cnn_pred_Endzone_player_average_1\"])\n",
    "    df_inp[\"cnn_pred_Sideline_player_average_2\"] = df_inp[\"cnn_pred_Sideline_player_average_2\"] * (df_inp[\"cnn_pred_Sideline\"]!=df_inp[\"cnn_pred_Sideline_player_average_2\"])\n",
    "    df_inp[\"cnn_pred_Endzone_player_average_2\"] = df_inp[\"cnn_pred_Endzone_player_average_2\"] * (df_inp[\"cnn_pred_Endzone\"]!=df_inp[\"cnn_pred_Endzone_player_average_2\"])\n",
    "\n",
    "    df_inp = pd.merge(df_inp, df_g, left_on=[\"game_play\", \"step\", \"nfl_player_id_1\"], right_on=[\"game_play\", \"step\", \"nfl_player_id\"], how=\"left\").drop(columns=[\"nfl_player_id\"])\n",
    "    df_inp = df_inp.rename(columns={\"cnn_pred_Sideline_player_average\":\"cnn_pred_Sideline_ground_1\", \"cnn_pred_Endzone_player_average\":\"cnn_pred_Endzone_ground_1\"})\n",
    "    df_inp = pd.merge(df_inp, df_g, left_on=[\"game_play\", \"step\", \"nfl_player_id_2\"], right_on=[\"game_play\", \"step\", \"nfl_player_id\"], how=\"left\").drop(columns=[\"nfl_player_id\"])\n",
    "    df_inp = df_inp.rename(columns={\"cnn_pred_Sideline_player_average\":\"cnn_pred_Sideline_ground_2\", \"cnn_pred_Endzone_player_average\":\"cnn_pred_Endzone_ground_2\"})\n",
    "    return df_inp\n",
    "\n",
    "\n",
    "class PairRollHolder:\n",
    "    def __init__(self, window_size, all_players):\n",
    "        self.window_size = window_size\n",
    "        self.window_dev = int(window_size//2)\n",
    "        self.pair_vals = {str(p1)+str(p2):[0]*window_size for p1 in all_players for p2 in all_players}\n",
    "        self.pair_counts = {str(p1)+str(p2):[0]*window_size for p1 in all_players for p2 in all_players}\n",
    "        self.roll_map = {}\n",
    "        # self.roll_map_half = {}\n",
    "        \n",
    "    def ready_next(self):\n",
    "        self.pair_vals = {key:val[1:]+[0] for key,val in self.pair_vals.items()}\n",
    "        self.pair_counts = {key:val[1:]+[0] for key,val in self.pair_counts.items()}\n",
    "        \n",
    "    def add_if_not_nan(self, p1, p2, val):\n",
    "        if not np.isnan(val):\n",
    "            self.pair_vals[str(p1)+str(p2)][-1] = val\n",
    "            self.pair_counts[str(p1)+str(p2)][-1] = 1\n",
    "            self.pair_vals[str(p2)+str(p1)][-1] = val\n",
    "            self.pair_counts[str(p2)+str(p1)][-1] = 1\n",
    "            \n",
    "    def end_of_step(self, step):\n",
    "        self.roll_map.update({f\"{step-self.window_dev}_\" + key : sum(val)/(sum(self.pair_counts[key])+1e-7) for key,val in self.pair_vals.items()})\n",
    "        # self.roll_map_half.update({f\"{step-self.window_dev}_half_\" + key : sum(val[:self.window_dev])/(sum(self.pair_counts[key][:self.window_dev])+1e-7) for key,val in self.pair_vals.items()})\n",
    "        \n",
    "        \n",
    "def pair_step_roll_features_old(train_df, window_sizes=[5,11,21], columns_to_roll=['cnn_pred_Sideline', 'cnn_pred_Endzone', 'camaro_pred', 'camaro_pred2']):\n",
    "    \"\"\"\n",
    "    game_play, pair(player1, player2), でstep方向にroll (現状average。ガウス重みがベター？)とる。\n",
    "    画像系特徴の 予測欠損補間が主目的。特にGround接触時はヘルメットが隠れやすく、画像からの予測が存在しないところがあるはず。\n",
    "    \"\"\"\n",
    "    df_roll_features = []\n",
    "    c=0\n",
    "    num_gp = len(train_df[\"game_play\"].unique())\n",
    "    feature_names = []\n",
    "    for window_size in window_sizes:\n",
    "        feature_names += [f\"{column}_roll{window_size}\" for column in columns_to_roll]\n",
    "    for gp, df_gp in train_df.groupby(\"game_play\"):\n",
    "        print(f\"\\r{c} / {num_gp} game_play\", end=\"\")\n",
    "        c+=1\n",
    "        p1_uniq = list(df_gp[\"nfl_player_id_1\"].unique())\n",
    "        p2_uniq = list(df_gp[\"nfl_player_id_2\"].unique())\n",
    "        step_uniq = list(df_gp[\"step\"].unique())\n",
    "        df_gp[\"key\"] = df_gp[\"step\"].astype(str) + \"_\" + df_gp[\"nfl_player_id_1\"].astype(str) + df_gp[\"nfl_player_id_2\"].astype(str)\n",
    "        min_step = np.min(step_uniq)\n",
    "        max_step = np.max(step_uniq)\n",
    "        all_players = list(set(p1_uniq + p2_uniq))\n",
    "        num_player = len(all_players)\n",
    "        num_step = int(1 + (max_step - min_step))\n",
    "        dev = max(window_sizes)//2\n",
    "        roll_feature_holders = {w: [PairRollHolder(w, all_players) for _ in columns_to_roll] for w in window_sizes}\n",
    "\n",
    "        for step in range(min_step, max_step+1+dev):\n",
    "            df_gp_step = df_gp[df_gp[\"step\"]==step]\n",
    "            p1s = df_gp_step[\"nfl_player_id_1\"].values\n",
    "            p2s = df_gp_step[\"nfl_player_id_2\"].values\n",
    "            values = [df_gp_step[column].values for column in columns_to_roll]\n",
    "            for w, holders in roll_feature_holders.items():\n",
    "                for holder in holders:\n",
    "                    holder.ready_next() \n",
    "            for p1p2values in zip(p1s, p2s, *values):\n",
    "                p1 = p1p2values[0]\n",
    "                p2 = p1p2values[1]\n",
    "                each_val = p1p2values[2:]\n",
    "                for w, holders in roll_feature_holders.items():\n",
    "                    for holder, v in zip(holders, each_val):\n",
    "                        holder.add_if_not_nan(p1,p2,v)\n",
    "            for w, holders in roll_feature_holders.items():\n",
    "                for holder in holders:\n",
    "                    holder.end_of_step(step) \n",
    "        for window, holders in roll_feature_holders.items():\n",
    "            for column, holder in zip(columns_to_roll, holders):\n",
    "                df_gp[f\"{column}_roll{window}\"] = df_gp[\"key\"].map(holder.roll_map)\n",
    "        df_roll_features.append(df_gp[[\"game_play\", \"step\", \"nfl_player_id_1\", \"nfl_player_id_2\"]+feature_names])\n",
    "        \n",
    "    df_roll_features = pd.concat(df_roll_features, axis=0)\n",
    "    # merge遅いし重いし良くないかも…\n",
    "    train_df = pd.merge(train_df, df_roll_features, how=\"left\", on=[\"game_play\", \"step\", \"nfl_player_id_1\", \"nfl_player_id_2\"])\n",
    "    return train_df\n",
    "\n",
    "\n",
    "# hata's version\n",
    "def pair_step_roll_features(train_df, \n",
    "                            window_sizes=[5,11,21],\n",
    "                            columns_to_roll= ['cnn_pred_Sideline', 'cnn_pred_Endzone', 'camaro_pred', 'camaro_pred2'],\n",
    "                            full_sample_df=None):\n",
    "    if full_sample_df is None:\n",
    "        full_sample_df = train_df\n",
    "        # raise NotImplementedError('未実装.元のデータを呼び出しても良い')\n",
    "\n",
    "    merged_columns = ['game_play', 'step', 'nfl_player_id_1', 'nfl_player_id_2', 'distance']\n",
    "    _tmp = pd.merge(full_sample_df[merged_columns], train_df[merged_columns+columns_to_roll],\n",
    "                    on=merged_columns,\n",
    "                   how='left'\n",
    "                   )\n",
    "    _tmp[\"distance\"] < 3\n",
    "    \n",
    "    '''\n",
    "    #nfl_player_idでスワップしたデータがテストデータに含まれていたら嫌なのでスワップしておく\n",
    "    _tmp2 = _tmp.copy()\n",
    "    _tmp2['nfl_player_id_1_2'] = _tmp2['nfl_player_id_1']\n",
    "    _tmp2['nfl_player_id_1'] =_tmp2['nfl_player_id_2']\n",
    "    _tmp2['nfl_player_id_2'] =_tmp2['nfl_player_id_1_2']\n",
    "    del _tmp2['nfl_player_id_1_2']\n",
    "    _tmp = pd.concat([_tmp, _tmp2], axis=0)\n",
    "    '''\n",
    "    # rolling featureを計算する。\n",
    "    for window in window_sizes:\n",
    "        renamed_columns = [f'{c}_roll{window}' for c in columns_to_roll]\n",
    "        data = (_tmp\n",
    "                .groupby(['game_play', 'nfl_player_id_1', 'nfl_player_id_2'])[columns_to_roll]\n",
    "                .rolling(window, center=True, min_periods=1)\n",
    "                .mean()\n",
    "                .reset_index()\n",
    "                .rename(columns={c:f'{c}_roll{window}' for c in columns_to_roll})\n",
    "                .sort_values('level_3')\n",
    "                .reset_index(drop=True))\n",
    "        \n",
    "        for column in renamed_columns:\n",
    "            _tmp[column] = data[column].fillna(0.0)\n",
    "        merged_df = _tmp[merged_columns+[f'{c}_roll{window}' for c in columns_to_roll]]\n",
    "\n",
    "        train_df = pd.merge(train_df, merged_df, on=merged_columns, how='left')\n",
    "\n",
    "    return train_df\n",
    "\n",
    "\n",
    "def pair_step_roll_features_for_nn_pred(train_df, \n",
    "                            window_sizes=[5,11,21],\n",
    "                            columns_to_roll= ['cnn_pred_Sideline', 'cnn_pred_Endzone', 'camaro_pred', 'camaro_pred2'],\n",
    "                            full_sample_df=None):\n",
    "    \"\"\"\n",
    "    fill dist > threshold(3?) -> zero\n",
    "    dist < 3 but no helmet detected -> nan\n",
    "    \"\"\"\n",
    "    if full_sample_df is None:\n",
    "        full_sample_df = train_df\n",
    "        # raise NotImplementedError('未実装.元のデータを呼び出しても良い')\n",
    "\n",
    "    merged_columns = ['game_play', 'step', 'nfl_player_id_1', 'nfl_player_id_2']\n",
    "    isna_columns = [c+\"_isna\" for c in columns_to_roll]\n",
    "    train_df[isna_columns] = train_df[columns_to_roll].isna()\n",
    "    _tmp = pd.merge(full_sample_df[merged_columns], \n",
    "                    train_df[merged_columns+columns_to_roll+isna_columns],\n",
    "                    on=merged_columns,\n",
    "                   how='left'\n",
    "                   )\n",
    "    _tmp[columns_to_roll] = _tmp[columns_to_roll].fillna(0.0)\n",
    "    for c_f, c_nan in zip(columns_to_roll, isna_columns):\n",
    "        _tmp.loc[c_nan, c_f] = np.nan\n",
    "    _tmp  = _tmp.drop(columns=isna_columns)\n",
    "    '''\n",
    "    #nfl_player_idでスワップしたデータがテストデータに含まれていたら嫌なのでスワップしておく\n",
    "    _tmp2 = _tmp.copy()\n",
    "    _tmp2['nfl_player_id_1_2'] = _tmp2['nfl_player_id_1']\n",
    "    _tmp2['nfl_player_id_1'] =_tmp2['nfl_player_id_2']\n",
    "    _tmp2['nfl_player_id_2'] =_tmp2['nfl_player_id_1_2']\n",
    "    del _tmp2['nfl_player_id_1_2']\n",
    "    _tmp = pd.concat([_tmp, _tmp2], axis=0)\n",
    "    '''\n",
    "    # rolling featureを計算する。\n",
    "    for window in window_sizes:\n",
    "        renamed_columns = [f'{c}_roll{window}' for c in columns_to_roll]\n",
    "        data = (_tmp\n",
    "                .groupby(['game_play', 'nfl_player_id_1', 'nfl_player_id_2'])[columns_to_roll]\n",
    "                .rolling(window, center=True, min_periods=1)\n",
    "                .mean()\n",
    "                .reset_index()\n",
    "                .rename(columns={c:f'{c}_roll{window}' for c in columns_to_roll})\n",
    "                .sort_values('level_3')\n",
    "                .reset_index(drop=True))\n",
    "        \n",
    "        for column in renamed_columns:\n",
    "            _tmp[column] = data[column].fillna(0.0)\n",
    "        merged_df = _tmp[merged_columns+[f'{c}_roll{window}' for c in columns_to_roll]]\n",
    "        train_df = pd.merge(train_df, merged_df, on=merged_columns, how='left')\n",
    "    return train_df\n",
    "\n",
    "\n",
    "def bbox_std_overlap_feature(df):\n",
    "    for view in [\"Sideline\", \"Endzone\"]:\n",
    "        xc1 = df[f\"bbox_center_x_{view}_1\"]\n",
    "        yc1 = df[f\"bbox_center_y_{view}_1\"]\n",
    "        xc2 = df[f\"bbox_center_x_{view}_2\"]\n",
    "        yc2 = df[f\"bbox_center_y_{view}_2\"]\n",
    "        w1 = df[f\"width_{view}_1\"]\n",
    "        h1 = df[f\"height_{view}_1\"]\n",
    "        w2 = df[f\"width_{view}_2\"]\n",
    "        h2 = df[f\"height_{view}_2\"]\n",
    "\n",
    "        df[f\"bbox_x_std_overlap_{view}\"] = (np.minimum(xc1+w1/2, xc2+w2/2) - np.maximum(xc1-w1/2, xc2-w2/2)) / (w1 + w2)\n",
    "        df[f\"bbox_y_std_overlap_{view}\"] = (np.minimum(yc1+h1/2, yc2+h2/2) - np.maximum(yc1-h1/2, yc2-h2/2)) / (h1 + h2)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def interceptor_feature(df):\n",
    "    # 1-2の間に別のプレイヤー(3)がいるかどうかを計算する。以下のどちらかに該当するケースを抽出。\n",
    "    # - 角3-1-2が60度以下で、距離1-2より距離1-3のほうが短い場合\n",
    "    # - 角3-2-1が60度以下で、距離2-1より距離2-3のほうが短い場合\n",
    "    # 複数が該当する場合は角度が小さいものを優先する\n",
    "    dy = df[\"y_position_2\"] - df[\"y_position_1\"]\n",
    "    dx = df[\"x_position_2\"] - df[\"x_position_1\"]\n",
    "\n",
    "    angle_th = 60\n",
    "\n",
    "    df[\"angle_dxdy\"] = np.rad2deg(np.arctan2(dy, dx))\n",
    "\n",
    "    angles = df[[\"game_play\", \"step\", \"nfl_player_id_1\", \"nfl_player_id_2\", \"angle_dxdy\", \"distance\"]].copy()\n",
    "    angles = angles[angles[\"nfl_player_id_2\"] != -1]\n",
    "\n",
    "    def negate_angle(s):\n",
    "        s = s + 180\n",
    "        s.loc[s > 180] = s.loc[s > 180] - 360\n",
    "        return s\n",
    "\n",
    "    angles_ = angles.copy()\n",
    "    angles_.columns = [\"game_play\", \"step\", \"nfl_player_id_2\", \"nfl_player_id_1\", \"angle_dxdy\", \"distance\"]\n",
    "    angles_[\"angle_dxdy\"] = negate_angle(angles_[\"angle_dxdy\"])\n",
    "\n",
    "    angles = pd.concat([angles, angles_[angles.columns]]).reset_index(drop=True)\n",
    "\n",
    "    angles_triplet = pd.merge(angles, angles, left_on=[\"game_play\", \"step\", \"nfl_player_id_2\"], right_on=[\"game_play\", \"step\", \"nfl_player_id_1\"], how=\"left\")\n",
    "\n",
    "    del angles_triplet[\"nfl_player_id_1_y\"]\n",
    "    angles_triplet.columns = [\"game_play\", \"step\", \"nfl_player_id_1\", \"nfl_player_id_2\", \"angle_2to1\", \"distance_2to1\", \"nfl_player_id_3\", \"angle_2to3\", \"distance_2to3\"]\n",
    "    angles_triplet[\"angle_2to1\"] = negate_angle(angles_triplet[\"angle_2to1\"])\n",
    "    angles_triplet = angles_triplet[angles_triplet[\"nfl_player_id_1\"] != angles_triplet[\"nfl_player_id_3\"]]\n",
    "\n",
    "    angles_triplet[\"angle_123\"] = angle_diff(angles_triplet[\"angle_2to1\"], angles_triplet[\"angle_2to3\"])\n",
    "\n",
    "    interceptors = angles_triplet[(angles_triplet[\"distance_2to3\"] <= angles_triplet[\"distance_2to1\"]) & (angles_triplet[\"angle_123\"] <= angle_th)].sort_values(by=\"angle_123\").drop_duplicates(subset=[\"game_play\", \"step\", \"nfl_player_id_1\", \"nfl_player_id_2\"])\n",
    "\n",
    "    interceptor_player2 = interceptors[[\"game_play\", \"step\", \"nfl_player_id_1\", \"nfl_player_id_2\", \"distance_2to3\", \"angle_123\", \"nfl_player_id_3\"]].copy()\n",
    "    interceptor_player2.columns = [\"game_play\", \"step\", \"nfl_player_id_1\", \"nfl_player_id_2\", \"distance_of_interceptor_2\", \"angle_interceptor_2\", \"nfl_player_id_interceptor_2\"]\n",
    "\n",
    "    interceptor_player1 = interceptors[[\"game_play\", \"step\", \"nfl_player_id_1\", \"nfl_player_id_2\", \"distance_2to3\", \"angle_123\", \"nfl_player_id_3\"]].copy()\n",
    "    interceptor_player1.columns = [\"game_play\", \"step\", \"nfl_player_id_2\", \"nfl_player_id_1\", \"distance_of_interceptor_1\", \"angle_interceptor_1\", \"nfl_player_id_interceptor_1\"]\n",
    "\n",
    "    df = pd.merge(df, interceptor_player1, on=[\"game_play\", \"step\", \"nfl_player_id_1\", \"nfl_player_id_2\"], how=\"left\")\n",
    "    df = pd.merge(df, interceptor_player2, on=[\"game_play\", \"step\", \"nfl_player_id_1\", \"nfl_player_id_2\"], how=\"left\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6e6226",
   "metadata": {
    "papermill": {
     "duration": 0.052925,
     "end_time": "2023-02-24T23:50:09.934520",
     "exception": false,
     "start_time": "2023-02-24T23:50:09.881595",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "make features にphase追加。for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab05e36",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.088434,
     "end_time": "2023-02-24T23:50:10.076721",
     "exception": false,
     "start_time": "2023-02-24T23:50:09.988287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_cnn_shift_feature(df:pd.DataFrame, shift_steps:List[int]=[-5, -3, -1, 1, 3, 5]) -> pd.DataFrame:\n",
    "    for shift_step in shift_steps:\n",
    "        df[f'cnn_pred_Sideline_shift_{shift_step}'] = (df\n",
    "                                                       .sort_values('step')\n",
    "                                                       .groupby(['game_play', 'nfl_player_id_1', 'nfl_player_id_2'])['cnn_pred_Sideline']\n",
    "                                                       .shift(shift_step).reset_index().sort_values('index').set_index('index'))\n",
    "        df[f'cnn_pred_Sideline_diff_{shift_step}'] = df['cnn_pred_Sideline'] - df[f'cnn_pred_Sideline_shift_{shift_step}']\n",
    "        df[f'cnn_pred_Endzone_shift_{shift_step}'] = (df\n",
    "                                                       .sort_values('step')\n",
    "                                                       .groupby(['game_play', 'nfl_player_id_1', 'nfl_player_id_2'])['cnn_pred_Endzone']\n",
    "                                                       .shift(shift_step).reset_index().sort_values('index').set_index('index'))\n",
    "        df[f'cnn_pred_Endzone_diff_{shift_step}'] = df['cnn_pred_Endzone'] - df[f'cnn_pred_Endzone_shift_{shift_step}']\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_cnn_agg_features(train_df):\n",
    "    def g_con_around_feature(df_train, dist_thresh = 1.5, columns = ['cnn_pred_Sideline_roll11', 'cnn_pred_Endzone_roll11']):\n",
    "        \"\"\"\n",
    "        周辺が倒れている場合、その人も倒れている。\n",
    "        画像中の隠れているプレイヤに対するグラウンドコンタクト紐づけ　兼　アサインメントミスの補助\n",
    "        \"\"\"\n",
    "        print(df_train.shape)    \n",
    "        add_columns = [n+\"_g_contact_around\" for n in columns]\n",
    "        df_train_g = df_train.loc[df_train[\"nfl_player_id_2\"]==-1, [\"game_play\", \"step\", \"nfl_player_id_1\"]+columns].rename(columns={c:a for c,a in zip(columns, add_columns)})\n",
    "        p_pairs = df_train.loc[df_train[\"nfl_player_id_2\"]!=-1, [\"game_play\", \"step\", \"nfl_player_id_1\", \"nfl_player_id_2\", \"distance\"]]\n",
    "        p_pairs = p_pairs[p_pairs[\"distance\"] < dist_thresh]\n",
    "        p_pairs = pd.concat([p_pairs, p_pairs.rename(columns={\"nfl_player_id_1\": \"nfl_player_id_2\", \"nfl_player_id_2\":\"nfl_player_id_1\"})], axis=0)\n",
    "        p_pairs = pd.merge(p_pairs, \n",
    "                          df_train_g, \n",
    "                          on=[\"game_play\", \"step\", \"nfl_player_id_1\"], how=\"left\")\n",
    "        # 周辺プレイヤの地面コンタクト(自分以外)をとりあえずsumる。\n",
    "        p_pairs = p_pairs.groupby([\"game_play\", \"step\", \"nfl_player_id_2\"])[add_columns].sum().reset_index()#(\"sum\", \"mean\")\n",
    "        p_pairs = p_pairs.rename(columns={\"nfl_player_id_2\":\"nfl_player_id_1\"})\n",
    "\n",
    "        df_train = pd.merge(df_train, \n",
    "                          p_pairs, \n",
    "                          on=[\"game_play\", \"step\", \"nfl_player_id_1\"], how=\"left\")\n",
    "\n",
    "        # df_train.loc[df_train[\"nfl_player_id_2\"]!=-1, add_columns] = np.nan\n",
    "\n",
    "        return df_train\n",
    "\n",
    "    def g_conact_as_condition(df_train, score_columns=['cnn_pred_Sideline_roll5', 'cnn_pred_Endzone_roll5'], dist_ratio=1.):\n",
    "        \"\"\"\n",
    "        同じ状態(高さ姿勢)のものはコンタクトしやすい\n",
    "        グラウンドコンタクトの発生状態を高さ姿勢と考えてペア間の高さ姿勢を比較する\n",
    "        あわせて、疑似的な距離を算出する。\n",
    "        \"\"\"\n",
    "        print(df_train.shape)    \n",
    "        temp_columns = [n+\"_as_g_contact_cond\" for n in score_columns]\n",
    "        dev_columns = [n+\"_dev_as_g_contact_cond\" for n in score_columns]\n",
    "        dist_columns = [n+\"_dist_as_g_contact_cond\" for n in score_columns]\n",
    "\n",
    "        df_train_g = df_train.loc[df_train[\"nfl_player_id_2\"]==-1, [\"game_play\", \"step\", \"nfl_player_id_1\"]+score_columns].rename(columns={c:a for c,a in zip(score_columns, temp_columns)})\n",
    "        \n",
    "        df_train = pd.merge(df_train, \n",
    "                          df_train_g, \n",
    "                          on=[\"game_play\", \"step\", \"nfl_player_id_1\"], how=\"left\").rename(columns={c: c+\"_1\" for c in temp_columns})\n",
    "        df_train = pd.merge(df_train, \n",
    "                          df_train_g.rename(columns={\"nfl_player_id_1\": \"nfl_player_id_2\"}), \n",
    "                          on=[\"game_play\", \"step\", \"nfl_player_id_2\"], how=\"left\").rename(columns={c: c+\"_2\" for c in temp_columns})\n",
    "\n",
    "\n",
    "        for dist_c, dev_c, temp_c in zip(dist_columns, dev_columns, temp_columns):\n",
    "            df_train[dev_c] = np.abs(df_train[temp_c+\"_2\"] - df_train[temp_c+\"_1\"])\n",
    "            df_train[dist_c] = np.sqrt((df_train[dev_c]*dist_ratio)**2 + df_train[\"distance\"]**2)\n",
    "            \n",
    "        df_train = df_train.drop(columns=[c+\"_1\" for c in temp_columns] + [c+\"_2\" for c in temp_columns])\n",
    "\n",
    "        return df_train\n",
    "    \n",
    "    def p_con_shift_feature(df_train, step_offset = 5, score_columns = ['cnn_pred_Sideline_roll11', 'cnn_pred_Endzone_roll11']):\n",
    "        \"\"\"\n",
    "        過去にコンタクトがあった人は再度何かしらのコンタクト(特に地面と?)することが多い。\n",
    "        (逆に地面コンタクトしている人はさかのぼると誰かにコンタクトしていると思う。TODO?)\n",
    "        \"\"\"\n",
    "        print(df_train.shape)    \n",
    "        add_columns = [n+f\"_p_contact_past{step_offset}\" for n in score_columns]\n",
    "        df_train_p = df_train.loc[df_train[\"nfl_player_id_2\"]!=-1, [\"game_play\", \"step\", \"nfl_player_id_1\", \"nfl_player_id_2\"]+score_columns].rename(columns={c:a for c,a in zip(score_columns, add_columns)})\n",
    "        df_train_p = pd.concat([df_train_p, df_train_p.rename(columns={\"nfl_player_id_1\": \"nfl_player_id_2\", \"nfl_player_id_2\":\"nfl_player_id_1\"})], axis=0)\n",
    "        df_train_p = df_train_p.groupby([\"game_play\", \"step\", \"nfl_player_id_1\"])[add_columns].sum().reset_index()\n",
    "        df_train_p[\"step\"] = df_train_p[\"step\"] + step_offset\n",
    "        \n",
    "        df_train = pd.merge(df_train, \n",
    "                          df_train_p, \n",
    "                          on=[\"game_play\", \"step\", \"nfl_player_id_1\"], how=\"left\").rename(columns={c: c+\"_1\" for c in add_columns})\n",
    "        df_train = pd.merge(df_train, \n",
    "                          df_train_p.rename(columns={\"nfl_player_id_1\": \"nfl_player_id_2\"}), \n",
    "                          on=[\"game_play\", \"step\", \"nfl_player_id_2\"], how=\"left\").rename(columns={c: c+\"_2\" for c in add_columns})\n",
    "\n",
    "        return df_train\n",
    "    \n",
    "    train_df = g_con_around_feature(train_df, dist_thresh = 1.5, columns = ['cnn_pred_Sideline_roll11', 'cnn_pred_Endzone_roll11', 'camaro_pred_roll11', 'camaro_pred2_roll11'])\n",
    "    train_df = g_con_around_feature(train_df, dist_thresh = 0.75, columns = ['cnn_pred_Sideline_roll5', 'cnn_pred_Endzone_roll5', 'camaro_pred_roll5', 'camaro_pred2_roll5'])\n",
    "    train_df = p_con_shift_feature(train_df, step_offset = 5, score_columns = ['cnn_pred_Sideline_roll5', 'cnn_pred_Endzone_roll5', 'camaro_pred_roll5', 'camaro_pred2_roll5'])\n",
    "    train_df = p_con_shift_feature(train_df, step_offset = -5, score_columns = ['cnn_pred_Sideline_roll5', 'cnn_pred_Endzone_roll5', 'camaro_pred_roll5', 'camaro_pred2_roll5'])\n",
    "    #train_df = g_conact_as_condition(train_df, score_columns = ['cnn_pred_Sideline_roll11', 'cnn_pred_Endzone_roll11'])\n",
    "    return train_df\n",
    "\n",
    "\n",
    "def agg_cnn_feature(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    for column in ['cnn_pred_Sideline', 'cnn_pred_Endzone', 'camaro_pred', 'camaro_pred2']:\n",
    "        for agg in ['max', 'min', 'std']:\n",
    "            df[f\"{column}_{agg}_pair\"] = df.groupby(['game_play', 'nfl_player_id_1', 'nfl_player_id_2'])[column].transform(agg)\n",
    "            df[f\"{column}_{agg}_step\"] = df.groupby(['game_play', 'step'])[column].transform(agg)\n",
    "    return df\n",
    "\n",
    "def add_distance_agg_feature(df):\n",
    "    for shift in [1, -1]:\n",
    "        df[f'distance_shift{shift}']=df.groupby(['game_play', 'nfl_player_id_1', 'nfl_player_id_2'])['distance'].shift(shift)\n",
    "    \n",
    "    for roll in [5, 11, 21]:\n",
    "        df[f'distance_window{roll}'] = (df.groupby(['game_play', 'nfl_player_id_1', 'nfl_player_id_2'])['distance']\n",
    "                                        .rolling(roll)\n",
    "                                        .mean()\n",
    "                                        .reset_index()\n",
    "                                        .sort_values('level_3')\n",
    "                                        .set_index('level_3')\n",
    "                                        .rename(columns={'distance':f'distance_window{roll}'})[f'distance_window{roll}'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd766fe",
   "metadata": {
    "papermill": {
     "duration": 0.073284,
     "end_time": "2023-02-24T23:50:10.544529",
     "exception": false,
     "start_time": "2023-02-24T23:50:10.471245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# phase \"train\" or \"test\"を追加させてください。\n",
    "def make_features(df, tracking, phase, cnn_features_model_type=\"B\"):\n",
    "    with timer(\"merge\"):\n",
    "        tracking = tracking_prep(tracking)\n",
    "        train_df = merge_cols(\n",
    "            df,\n",
    "            tracking,\n",
    "            [\n",
    "                \"team\", \"position\", \"x_position\", \"y_position\", \n",
    "                \"speed\", \"distance\", \"direction\", \"orientation\", \"acceleration\", \n",
    "                \"sa\",\n",
    "                #\"direction_p1_diff\", \"direction_m1_diff\",\n",
    "                #\"orientation_p1_diff\", \"orientation_m1_diff\",\n",
    "                #\"distance_p1\", \"distance_m1\"\n",
    "            ]\n",
    "        )\n",
    "        # Subsequent serialization in feather format replaces np.nan with None. \n",
    "        # For consistency between going through serialization and not, nan is converted here in advance.\n",
    "        train_df[\"position_2\"] = train_df[\"position_2\"].replace({np.nan: None})\n",
    "\n",
    "    print(train_df.shape)\n",
    "    \n",
    "    with timer(\"tracking_agg_features\"):\n",
    "        train_df = make_basic_features(train_df) \n",
    "        \n",
    "        # この辺は全サンプルで計算\n",
    "        train_df = create_bbox_features(train_df)\n",
    "        # train_df = bbox_distance_around_player(train_df)\n",
    "        train_df = add_distance_agg_feature(train_df)\n",
    "        train_df = distance_around_player(train_df, True)\n",
    "        train_df = second_nearest_distance(train_df, tracking, \"1\")\n",
    "        train_df = second_nearest_distance(train_df, tracking, \"2\")\n",
    "        train_df = reduce_dtype(train_df)\n",
    "        gc.collect()\n",
    "\n",
    "    with timer(\"cnn_features\"):\n",
    "        train_df = cnn_features(train_df, tracking, phase=phase, model_type=cnn_features_model_type)\n",
    "        train_df = reduce_dtype(train_df)\n",
    "        gc.collect()  \n",
    "\n",
    "    with timer(\"pair_step_roll_features_for_nn_pred\"):\n",
    "        train_df = pair_step_roll_features_for_nn_pred(train_df, window_sizes = [5,11,21], \n",
    "                                           columns_to_roll = ['cnn_pred_Sideline', 'cnn_pred_Endzone', 'camaro_pred', 'camaro_pred2'],\n",
    "                                           full_sample_df=train_df)\n",
    "        train_df = reduce_dtype(train_df)\n",
    "        gc.collect()\n",
    "\n",
    "    with timer(\"sampling\"):\n",
    "        # メモリ対策。分けてみる\n",
    "        is_hard_sample = np.logical_or(train_df[\"distance\"]<=3, train_df[\"nfl_player_id_2\"]==-1)\n",
    "        # full_sample_df = train_df[['game_play', 'step', 'nfl_player_id_1', 'nfl_player_id_2']].copy()\n",
    "        train_df = train_df[is_hard_sample]\n",
    "\n",
    "    with timer(\"p2p_matching_features\"):\n",
    "        # distance 使用したいのでbasic直後に画像系特徴。\n",
    "        train_df = p2p_matching_features(train_df, tracking, phase=phase)\n",
    "        train_df = reduce_dtype(train_df)\n",
    "        \n",
    "        # p2pでとってきた特徴量のrollなので、p2pの後にやる\n",
    "        train_df = pair_step_roll_features(train_df, window_sizes = [5,11,21], \n",
    "                                           columns_to_roll = ['x_rel_position_offset_on_img_End_1', 'y_rel_position_offset_on_img_Side_1'],\n",
    "                                           full_sample_df=train_df)\n",
    "        gc.collect()\n",
    "\n",
    "    with timer(\"cnn-p2p\"):\n",
    "        # 追加\n",
    "        train_df = reduce_dtype(train_df)\n",
    "        train_df = add_cnn_agg_features(train_df)\n",
    "        train_df = add_cnn_shift_feature(train_df)\n",
    "        train_df = agg_cnn_feature(train_df)\n",
    "\n",
    "        train_df = step_feature(train_df, tracking)\n",
    "        train_df = tracking_agg_features(train_df, tracking)\n",
    "        train_df = t0_feature(train_df, tracking)\n",
    "        train_df = reduce_dtype(train_df)\n",
    "        train_df = distance_around_player(train_df)\n",
    "        train_df = aspect_ratio_feature(train_df, False)\n",
    "        train_df = misc_features_after_agg(train_df)\n",
    "        train_df = shift_of_player(train_df, tracking, [-5, 5, 10], add_diff=True, player_id=\"1\")\n",
    "        train_df = shift_of_player(train_df, tracking, [-5, 5], add_diff=True, player_id=\"2\")\n",
    "        train_df = bbox_std_overlap_feature(train_df)\n",
    "\n",
    "        train_df = interceptor_feature(train_df)\n",
    "        train_df = reduce_dtype(train_df)\n",
    "        \n",
    "        # TODO temporary!\n",
    "        for view in [\"Sideline\", \"Endzone\"]:\n",
    "            train_df[f\"std_bbox_size_{view}\"] = np.sqrt(train_df[f\"width_{view}_mean\"] * train_df[f\"height_{view}_mean\"])\n",
    "            del train_df[f\"width_{view}_mean\"]\n",
    "            del train_df[f\"height_{view}_mean\"]\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "    print(train_df.shape)\n",
    "    print(train_df.columns.tolist())\n",
    "    return train_df, is_hard_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847abf0",
   "metadata": {
    "papermill": {
     "duration": 0.052043,
     "end_time": "2023-02-24T23:50:10.648478",
     "exception": false,
     "start_time": "2023-02-24T23:50:10.596435",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# モデリング\n",
    "\n",
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199bda03",
   "metadata": {
    "papermill": {
     "duration": 0.090345,
     "end_time": "2023-02-24T23:50:10.791161",
     "exception": false,
     "start_time": "2023-02-24T23:50:10.700816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict_lgbm(booster: Union[lgb.Booster, lgb.CVBooster, xgb.Booster], X: np.ndarray, init_score=None):\n",
    "    try:\n",
    "        y_pred = booster.predict(X, raw_score=init_score is not None)\n",
    "        if init_score is not None:\n",
    "            y_pred = special.expit(init_score + y_pred)\n",
    "\n",
    "        return y_pred\n",
    "    except TypeError:\n",
    "        # 例外をキャッチすることで強引にxgboostを分岐する\n",
    "        if isinstance(booster, xgb.Booster):\n",
    "            feature_names = booster.feature_names\n",
    "        else:\n",
    "            feature_names = booster.boosters[0].feature_names\n",
    "        \n",
    "        return booster.predict(xgb.DMatrix(X, feature_names=feature_names))\n",
    "\n",
    "\n",
    "\n",
    "def make_oof(cvboosters, X_train: np.ndarray, y_train: pd.Series, split, init_score=None):\n",
    "    oof = np.zeros(len(X_train))\n",
    "\n",
    "    for booster, (idx_train, idx_valid) in zip(cvboosters, split):\n",
    "        init_score_fold = init_score[idx_valid] if init_score is not None and isinstance(init_score, np.ndarray) else init_score\n",
    "        y_pred = predict_lgbm(booster, X_train[idx_valid], init_score_fold)\n",
    "        oof[idx_valid] = y_pred\n",
    "        print(f\"{roc_auc_score(y_train.iloc[idx_valid].values, y_pred)}\")\n",
    "        \n",
    "    return oof\n",
    "\n",
    "\n",
    "def plot_importance(cvbooster, figsize=(12, 20)):\n",
    "    raw_importances = cvbooster.feature_importance(importance_type='gain')\n",
    "    feature_name = cvbooster.boosters[0].feature_name()\n",
    "    importance_df = pd.DataFrame(data=raw_importances,\n",
    "                                 columns=feature_name)\n",
    "    # order by average importance across folds\n",
    "    sorted_indices = importance_df.mean(axis=0).sort_values(ascending=False).index\n",
    "    sorted_importance_df = importance_df.loc[:, sorted_indices]\n",
    "    # plot top-n\n",
    "    PLOT_TOP_N = 100\n",
    "    plot_cols = sorted_importance_df.columns[:PLOT_TOP_N]\n",
    "    _, ax = plt.subplots(figsize=figsize)\n",
    "    ax.grid()\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylabel('Feature')\n",
    "    ax.set_xlabel('Importance')\n",
    "    sns.boxplot(data=sorted_importance_df[plot_cols],\n",
    "                orient='h',\n",
    "                ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def search_best_threshold(y_true, y_pred):\n",
    "    def func(x_list):\n",
    "        score = matthews_corrcoef(y_true, y_pred>x_list[0])\n",
    "        return -score\n",
    "\n",
    "    x0 = [0.4]\n",
    "    result = minimize(func, x0,  method=\"nelder-mead\")\n",
    "    \n",
    "    return result.x[0]\n",
    "\n",
    "\n",
    "def binarize_pred(y_pred, threshold, threshold2, threshold2_mask):\n",
    "    return ~threshold2_mask*(y_pred>threshold)+threshold2_mask*(y_pred>threshold2)\n",
    "\n",
    "\n",
    "def search_best_threshold_pair(y_true, y_pred, is_ground):\n",
    "    def func(x_list):\n",
    "        score = matthews_corrcoef(y_true, binarize_pred(y_pred, x_list[0], x_list[1], is_ground))\n",
    "        return -score\n",
    "\n",
    "    x0 = [0.5, 0.5]\n",
    "    result = minimize(func, x0,  method=\"nelder-mead\")\n",
    "\n",
    "    return result.x[0], result.x[1]\n",
    "\n",
    "\n",
    "def search_best_threshold_pair_optuna(y_true, y_pred, is_ground, n_trials=100):\n",
    "\n",
    "    def objective(trial):\n",
    "        th1 = trial.suggest_float('th1', 0.1, 0.6)\n",
    "        th2 = trial.suggest_float('th2', 0.1, 0.6)\n",
    "        score = matthews_corrcoef(y_true, binarize_pred(y_pred, th1, th2, is_ground))\n",
    "        return -score\n",
    "\n",
    "    study = optuna.create_study()  # Create a new study.\n",
    "    study.optimize(objective, n_trials=n_trials)  # Invoke optimization of the objective function.\n",
    "    return study.best_params[\"th1\"], study.best_params[\"th2\"]\n",
    "\n",
    "\n",
    "def metrics(y_true, y_pred, threshold=None, threshold2=None, threshold2_mask=None):\n",
    "    if threshold is None:\n",
    "        threshold = search_best_threshold(y_true, y_pred)\n",
    "        \n",
    "    if threshold2 is not None and threshold2_mask is not None:\n",
    "        return matthews_corrcoef(y_true, binarize_pred(y_pred, threshold, threshold2, threshold2_mask))\n",
    "    else:\n",
    "        return matthews_corrcoef(y_true, y_pred>threshold)\n",
    "\n",
    "    \n",
    "def make_x(train_df: pd.DataFrame, encoder: LabelEncoders, feature_names, use_existing_encoders=True):\n",
    "    X_train = np.empty((len(train_df), len(feature_names)), dtype=np.float32)\n",
    "    print(X_train.shape)\n",
    "\n",
    "    for i, c in enumerate(feature_names):\n",
    "        if train_df[c].dtype.name == \"object\":\n",
    "            if use_existing_encoders:\n",
    "                X_train[:, i] = encoder.transform_one(train_df[c])\n",
    "            else:\n",
    "                X_train[:, i] = encoder.fit_transform_one(train_df[c])\n",
    "        else:\n",
    "            X_train[:, i] = train_df[c]\n",
    "            \n",
    "    return X_train\n",
    "\n",
    "\n",
    "def train_cv(train_df: pd.DataFrame, \n",
    "             non_feature_cols: List[str], \n",
    "             encoder: Optional[LabelEncoders] = None,\n",
    "             lgb_params = None,\n",
    "             seed = None,\n",
    "             add_full_fold = True,\n",
    "             debug = False):\n",
    "    lgb_params = lgb_params or {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"max_depth\": -1,\n",
    "        \"num_leaves\": 128,\n",
    "        \"feature_fraction\": 0.3,\n",
    "        \"verbose\": -1,\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"min_child_samples\": 10,\n",
    "        \"min_child_weight\": 5,\n",
    "        \"subsample_for_bin\": 50000,\n",
    "        #\"reg_lambda\": 1\n",
    "    }\n",
    "    if seed is not None:\n",
    "        lgb_params[\"seed\"] = seed\n",
    "\n",
    "    is_ground = train_df[\"nfl_player_id_2\"] == -1\n",
    "    y_train = train_df[\"contact\"]\n",
    "\n",
    "    split_df = train_df[[\"game_play\"]].copy()\n",
    "    split_df[\"game\"] = split_df[\"game_play\"].str[:5].astype(int)\n",
    "    split_df = pd.merge(split_df, split_defs, how=\"left\")\n",
    "    split = list(PredefinedSplit(split_df[\"fold\"]).split())\n",
    "    \n",
    "    use_existing_encoders = encoder is not None\n",
    "    encoder = encoder or LabelEncoders()\n",
    "\n",
    "    with timer(\"make dataset\"):\n",
    "        # train_df.values.astype(np.float32)とかやるとOOMで死ぬので、箱を先に用意して値を入れていく\n",
    "        feature_names = [c for c in train_df.columns if c not in non_feature_cols]\n",
    "\n",
    "        X_train = make_x(train_df, encoder, feature_names, use_existing_encoders)\n",
    "\n",
    "        gc.collect()\n",
    "        print(f\"features: {feature_names}\")\n",
    "        print(f\"category: {list(encoder.encoders.keys())}\")\n",
    "\n",
    "        weight = None\n",
    "\n",
    "        ds_train = lgb.Dataset(X_train, y_train,\n",
    "                               feature_name=feature_names, \n",
    "                               weight=weight)\n",
    "        gc.collect()\n",
    "\n",
    "    with timer(\"lgb.cv\"):\n",
    "        if add_full_fold:\n",
    "            split.append((np.arange(len(train_df)), np.arange(10)))\n",
    "\n",
    "        ret = lgb.cv(lgb_params, ds_train, \n",
    "                     num_boost_round=50 if debug else 4000, \n",
    "                     folds=split,\n",
    "                     return_cvbooster=True,\n",
    "                     callbacks=[\n",
    "                         #lgb.early_stopping(stopping_rounds=100, verbose=True),\n",
    "                         lgb.log_evaluation(25)\n",
    "                     ]) \n",
    "\n",
    "        for booster in ret[\"cvbooster\"].boosters:\n",
    "            booster.best_iteration = ret[\"cvbooster\"].best_iteration\n",
    "\n",
    "        del ds_train\n",
    "        gc.collect()\n",
    "\n",
    "    plot_importance(ret[\"cvbooster\"])\n",
    "\n",
    "    oof = make_oof(ret[\"cvbooster\"].boosters[:4], X_train, y_train, split)\n",
    "\n",
    "    # np.save(\"oof.npy\", oof)\n",
    "\n",
    "    print(is_ground.mean())\n",
    "    \n",
    "    if is_ground.mean() in [0.0, 1.0]:\n",
    "        # g, non-gどちらかしかない。\n",
    "        with timer(\"find best threshold\"):\n",
    "            threshold = search_best_threshold(y_train, oof)\n",
    "\n",
    "        mcc = metrics(y_train, oof, threshold)\n",
    "        auc = roc_auc_score(y_train, oof)\n",
    "        print(f\"threshold: {threshold:.5f}, mcc: {mcc:.5f}, auc: {auc:.5f}\")\n",
    "\n",
    "        return ret[\"cvbooster\"], oof, encoder, threshold\n",
    "    else:\n",
    "        with timer(\"find best threshold\"):\n",
    "            #threshold = search_best_threshold(y_train, oof)\n",
    "            threshold_p, threshold_g = search_best_threshold_pair_optuna(y_train, oof, is_ground)\n",
    "\n",
    "        mcc = metrics(y_train, oof, threshold_p, threshold_g, is_ground)\n",
    "        auc = roc_auc_score(y_train, oof)\n",
    "        print(f\"threshold: {threshold_p:.5f}, {threshold_g:.5f}, mcc: {mcc:.5f}, auc: {auc:.5f}\")\n",
    "\n",
    "        mcc_ground = metrics(y_train[is_ground], oof[is_ground], threshold_g)\n",
    "        mcc_non_ground = metrics(y_train[~is_ground], oof[~is_ground], threshold_p)\n",
    "\n",
    "        print(f\"mcc(ground): {mcc_ground:.5f}, mcc(non-ground): {mcc_non_ground:.5f}\")\n",
    "\n",
    "        return ret[\"cvbooster\"], oof, encoder, threshold_p, threshold_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c8656a",
   "metadata": {
    "papermill": {
     "duration": 0.068161,
     "end_time": "2023-02-24T23:50:10.912748",
     "exception": false,
     "start_time": "2023-02-24T23:50:10.844587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LGBMSerializer:\n",
    "    def __init__(self,\n",
    "                 booster: lgb.CVBooster,\n",
    "                 encoders: LabelEncoders,\n",
    "                 threshold_p: float,\n",
    "                 threshold_g: float,\n",
    "                 init_score: float = None,\n",
    "                 treelite: bool = False,\n",
    "                 feature_names = None):\n",
    "        self.booster = booster\n",
    "        self.encoders = encoders\n",
    "        self.threshold_p = threshold_p\n",
    "        self.threshold_g = threshold_g\n",
    "        self.init_score = init_score\n",
    "        self.is_xgb = isinstance(self.booster.boosters[0], xgb.Booster)\n",
    "        self.treelite = treelite\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "        if feature_names is None and not treelite:\n",
    "            if self.is_xgb:\n",
    "                self.feature_names = list(self.booster.boosters[0].feature_names)\n",
    "            else:\n",
    "                self.feature_names = self.booster.boosters[0].feature_name()\n",
    "\n",
    "        self.encoders.patch()\n",
    "\n",
    "    def to_file(self, filename: str):\n",
    "        is_xgb = isinstance(self.booster.boosters[0], xgb.Booster)\n",
    "        model = {\n",
    "            \"best_iteration\": self.booster.best_iteration,\n",
    "            \"threshold_p\": self.threshold_p,\n",
    "            \"threshold_g\": self.threshold_g,\n",
    "            \"init_score\": self.init_score,\n",
    "            \"is_xgb\": is_xgb,\n",
    "            \"n_models\": len(self.booster.boosters),\n",
    "            \"feature_names\": self.feature_names\n",
    "        }\n",
    "        if is_xgb:\n",
    "            for i, b in enumerate(self.booster.boosters):\n",
    "                b.save_model(f'{filename}_xgb_{i}.json')\n",
    "        else:\n",
    "            model[\"boosters\"] = [b.model_to_string() for b in self.booster.boosters]\n",
    "\n",
    "        with open(f\"{filename}_model.json\", \"w\") as f:\n",
    "            json.dump(model, f)\n",
    "\n",
    "        with open(f\"{filename}_encoder.bin\", \"wb\") as f:\n",
    "            pickle.dump(self.encoders, f)\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, filename: str, treelite: bool = False) -> \"TrainedModel\":\n",
    "\n",
    "        with open(f\"{filename}_model.json\", \"r\") as f:\n",
    "            model = json.load(f)\n",
    "\n",
    "        cvbooster = lgb.CVBooster()\n",
    "\n",
    "        feature_names = model.get(\"feature_names\", [])\n",
    "\n",
    "        if treelite:\n",
    "            print(\"loading treelite...\")\n",
    "            lib_files = glob.glob(os.path.join(os.path.dirname(filename), \"*.so\"))\n",
    "            assert len(lib_files) > 0, \"treelite file not found!\"\n",
    "            for lib_file in lib_files:\n",
    "                b = treelite_runtime.Predictor(lib_file, verbose=False)\n",
    "                cvbooster.boosters.append(b)\n",
    "        elif model.get(\"is_xgb\"):\n",
    "            print(\"loading native-xgb...\")\n",
    "            for i in range(model[\"n_models\"]):\n",
    "                b = xgb.Booster()\n",
    "                b.load_model(f'{filename}_xgb_{i}.json')\n",
    "                cvbooster.boosters.append(b)\n",
    "            feature_names = list(b.feature_names)\n",
    "        else:\n",
    "            print(\"loading native-lgb...\")\n",
    "            cvbooster.boosters = [lgb.Booster(model_str=b) for b in model[\"boosters\"]]\n",
    "            cvbooster.best_iteration = model[\"best_iteration\"]\n",
    "            for b in cvbooster.boosters:\n",
    "                b.best_iteration = cvbooster.best_iteration\n",
    "            feature_names = b.feature_name()\n",
    "\n",
    "        with open(f\"{filename}_encoder.bin\", \"rb\") as f:\n",
    "            encoders = pickle.load(f)\n",
    "\n",
    "        return cls(\n",
    "            cvbooster,\n",
    "            encoders,\n",
    "            model[\"threshold_p\"],\n",
    "            model[\"threshold_g\"],\n",
    "            model.get(\"init_score\", None),\n",
    "            treelite=treelite,\n",
    "            feature_names=feature_names)\n",
    "\n",
    "    def predict(self, X: np.ndarray):\n",
    "        if self.treelite:\n",
    "            X_mat = treelite_runtime.DMatrix(X)\n",
    "            predicted = []\n",
    "            for b in self.booster.boosters:\n",
    "                predicted.append(b.predict(X_mat))\n",
    "\n",
    "            avg_pred = np.array(predicted).mean(axis=0)\n",
    "        elif self.is_xgb:\n",
    "            feature_names = self.booster.boosters[0].feature_names\n",
    "            avg_pred = np.array(self.booster.predict(xgb.DMatrix(X, feature_names=feature_names))).mean(axis=0)\n",
    "        else:\n",
    "            avg_pred = np.array(predict_lgbm(self.booster, X, self.init_score)).mean(axis=0)\n",
    "\n",
    "        return avg_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da28e244",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/66681443/how-can-i-get-the-trained-model-from-xgboost-cv\n",
    "class SaveBestModel(xgb.callback.TrainingCallback):\n",
    "    def __init__(self, cvboosters):\n",
    "        self._cvboosters = cvboosters\n",
    "\n",
    "    def after_training(self, model):\n",
    "        self._cvboosters[:] = [cvpack.bst for cvpack in model.cvfolds]\n",
    "        return model\n",
    "\n",
    "\n",
    "def train_cv_xgb(train_df: pd.DataFrame,\n",
    "                 non_feature_cols: List[str],\n",
    "                 encoder: Optional[LabelEncoders] = None,\n",
    "                 xgb_params = None,\n",
    "                 seed = None,\n",
    "                 add_full_fold = True,\n",
    "                 debug = False,\n",
    "                 gpu = True):\n",
    "    xgb_params = xgb_params or {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"max_depth\": 9,\n",
    "        \"colsample_bytree\": 0.3,\n",
    "        \"eta\": 0.02,\n",
    "        \"min_child_weight\": 5,\n",
    "    }\n",
    "    if seed is not None:\n",
    "        xgb_params[\"seed\"] = seed\n",
    "\n",
    "    if gpu:\n",
    "        xgb_params[\"gpu_id\"] = 0\n",
    "        xgb_params['tree_method'] = 'gpu_hist'\n",
    "\n",
    "    is_ground = train_df[\"nfl_player_id_2\"] == -1\n",
    "    y_train = train_df[\"contact\"]\n",
    "\n",
    "    split_df = train_df[[\"game_play\"]].copy()\n",
    "    split_df[\"game\"] = split_df[\"game_play\"].str[:5].astype(int)\n",
    "    split_df = pd.merge(split_df, split_defs, how=\"left\")\n",
    "    split = list(PredefinedSplit(split_df[\"fold\"]).split())\n",
    "\n",
    "    use_existing_encoders = encoder is not None\n",
    "    encoder = encoder or LabelEncoders()\n",
    "\n",
    "    with timer(\"make dataset\"):\n",
    "        feature_names = [c for c in train_df.columns if c not in non_feature_cols]\n",
    "\n",
    "        X_train = make_x(train_df, encoder, feature_names, use_existing_encoders)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        weight = None\n",
    "\n",
    "        ds_train = xgb.DMatrix(X_train, y_train, feature_names=feature_names, weight = weight)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    with timer(\"lgb.cv\"):\n",
    "        if add_full_fold:\n",
    "            split.append((np.arange(len(train_df)), np.arange(200)))\n",
    "\n",
    "        boosters = []\n",
    "\n",
    "        xgb.cv(xgb_params, ds_train,\n",
    "               num_boost_round=50 if debug else 4000,\n",
    "               folds=split,\n",
    "               verbose_eval=25,\n",
    "               callbacks=[SaveBestModel(boosters)])\n",
    "\n",
    "        cvboosters = lgb.CVBooster()\n",
    "        cvboosters.boosters = boosters  # ただの箱として使うならxgb入れてもＯＫ\n",
    "\n",
    "        #for booster in ret[\"cvbooster\"].boosters:\n",
    "        #    booster.best_iteration = ret[\"cvbooster\"].best_iteration\n",
    "\n",
    "        del ds_train\n",
    "        gc.collect()\n",
    "\n",
    "    # plot_importance_xgb(cvboosters)\n",
    "\n",
    "    oof = make_oof(cvboosters.boosters[:4], X_train, y_train, split, init_score=None)\n",
    "\n",
    "    # np.save(\"oof.npy\", oof)\n",
    "\n",
    "    print(is_ground.mean())\n",
    "\n",
    "    if is_ground.mean() in [0.0, 1.0]:\n",
    "        # g, non-gどちらかしかない。\n",
    "        with timer(\"find best threshold\"):\n",
    "            threshold = search_best_threshold(y_train, oof)\n",
    "\n",
    "        mcc = metrics(y_train, oof, threshold)\n",
    "        auc = roc_auc_score(y_train, oof)\n",
    "        print(f\"threshold: {threshold:.5f}, mcc: {mcc:.5f}, auc: {auc:.5f}\")\n",
    "\n",
    "        return cvboosters, oof, encoder, threshold\n",
    "    else:\n",
    "        with timer(\"find best threshold\"):\n",
    "            threshold_p, threshold_g = search_best_threshold_pair_optuna(y_train, oof, is_ground)\n",
    "\n",
    "        mcc = metrics(y_train, oof, threshold_p, threshold_g, is_ground)\n",
    "        auc = roc_auc_score(y_train, oof)\n",
    "        print(f\"threshold: {threshold_p:.5f}, {threshold_g:.5f}, mcc: {mcc:.5f}, auc: {auc:.5f}\")\n",
    "\n",
    "        mcc_ground = metrics(y_train[is_ground], oof[is_ground], threshold_g)\n",
    "        mcc_non_ground = metrics(y_train[~is_ground], oof[~is_ground], threshold_p)\n",
    "\n",
    "        print(f\"mcc(ground): {mcc_ground:.5f}, mcc(non-ground): {mcc_non_ground:.5f}\")\n",
    "\n",
    "        return cvboosters, oof, encoder, threshold_p, threshold_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ad81a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "non_feature_cols = [\n",
    "    \"contacgt_id\",\n",
    "    \"game_play\",\n",
    "    \"datetime\",\n",
    "    \"step\",\n",
    "    \"nfl_player_id_1\",\n",
    "    \"nfl_player_id_2\",\n",
    "    \"contact\",\n",
    "    \"team_1\",\n",
    "    \"team_2\",\n",
    "    \"contact_id\",\n",
    "    #\"position_1\",\n",
    "    #\"position_2\"\n",
    "    #\"direction_1\",\n",
    "    #\"direction_2\",\n",
    "    \"x_position_1\",\n",
    "    \"x_position_2\",\n",
    "    \"y_position_1\",\n",
    "    \"y_position_2\",\n",
    "    \"x_position_start_1\",\n",
    "    \"x_position_start_2\",\n",
    "    \"y_position_start_1\",\n",
    "    \"y_position_start_2\",\n",
    "\n",
    "    \"x_position_future5_1\",\n",
    "    \"x_position_future5_2\",\n",
    "    \"y_position_future5_1\",\n",
    "    \"y_position_future5_2\",\n",
    "    \"x_position_past5_1\",\n",
    "    \"x_position_past5_2\",\n",
    "    \"y_position_past5_1\",\n",
    "    \"y_position_past5_2\",\n",
    "    \"nfl_player_id_interceptor_1\",\n",
    "    \"nfl_player_id_interceptor_2\",\n",
    "\n",
    "    #\"orientation_past5_1\",\n",
    "    #\"direction_past5_1\",\n",
    "    #\"orientation_past5_2\",\n",
    "    #\"direction_past5_2\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0c8808",
   "metadata": {
    "papermill": {
     "duration": 4.735514,
     "end_time": "2023-02-24T23:50:16.531577",
     "exception": false,
     "start_time": "2023-02-24T23:50:11.796063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_treelite(original_path: str, test_X: np.ndarray):\n",
    "    dirname = os.path.dirname(original_path)\n",
    "    original_model = LGBMSerializer.from_file(original_path)\n",
    "    print(f\"dirname: {dirname}\")\n",
    "\n",
    "    for i, b in enumerate(original_model.booster.boosters):\n",
    "        print(f\"compile model {i}\")\n",
    "        with timer(\"annotate\"):\n",
    "            if original_model.is_xgb:\n",
    "                model = treelite.Model.from_xgboost(b)\n",
    "            else:\n",
    "                model = treelite.Model.from_lightgbm(b)\n",
    "\n",
    "            dmat = treelite_runtime.DMatrix(data=test_X)\n",
    "            annotator = treelite.Annotator()\n",
    "            annotator.annotate_branch(model=model, dmat=dmat, verbose=True)\n",
    "            annotator.save(path='annot.json')\n",
    "\n",
    "        with timer(\"compile\"):\n",
    "            path = os.path.join(dirname, f'treelite_{i}.so')\n",
    "            model.export_lib(toolchain='gcc',\n",
    "                             libpath=path,\n",
    "                             verbose=False,\n",
    "                             params={'parallel_comp': 32, 'annotate_in': 'annot.json'},\n",
    "                             nthread=32)\n",
    "            print(f\"model has been saved as {path}.\")\n",
    "\n",
    "\n",
    "def train_model(dst_directory: str, model_name: str, train_df: pd.DataFrame, non_feature_cols: List[str], is_xgb: bool = False, compile_treelite: bool = False, debug: bool = False):\n",
    "    os.makedirs(f\"{dst_directory}/{model_name}\", exist_ok=True)\n",
    "    if is_xgb:\n",
    "        model_basename = f\"{dst_directory}/{model_name}/model\"\n",
    "        cvbooster, oof, encoder, threshold_p, threshold_g = train_cv_xgb(train_df, non_feature_cols, debug=debug, gpu=cfg.USE_GPU, add_full_fold=False) # just forgot to add full-fold\n",
    "        serializer = LGBMSerializer(cvbooster, encoder, threshold_p, threshold_g)\n",
    "        serializer.to_file(f\"{model_name}/model\")\n",
    "    else:\n",
    "        model_basename = f\"{dst_directory}/{model_name}/lgb\"\n",
    "        cvbooster, oof, encoder, threshold_p, threshold_g = train_cv(train_df, non_feature_cols, debug=debug, add_full_fold=True)\n",
    "    serializer = LGBMSerializer(cvbooster, encoder, threshold_p, threshold_g)\n",
    "\n",
    "    serializer.to_file(model_basename)\n",
    "    np.save(f\"{dst_directory}/{model_name}/oof.npy\", oof)\n",
    "\n",
    "    if compile_treelite:\n",
    "        try:\n",
    "            encoder.patch()\n",
    "            test_X = make_x(train_df, encoder, serializer.feature_names)\n",
    "            build_treelite(model_basename, test_X)\n",
    "        except Exception:\n",
    "            print(\"Failed to build treelite\")\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "with timer(\"make_features(K_mat-B)\"):\n",
    "    df_path = os.path.join(cfg.CACHE_DIR, \"train_df_B.f\")\n",
    "    hard_sample_path = os.path.join(cfg.CACHE_DIR, \"tr_is_hard_sample.f\")\n",
    "\n",
    "    if os.path.exists(df_path):\n",
    "        train_df = pd.read_feather(df_path)\n",
    "        tr_is_hard_sample = pd.read_feather(hard_sample_path)[\"is_hard_sample\"]\n",
    "    else:\n",
    "        train_df, tr_is_hard_sample = make_features(train, tr_tracking, phase=\"train\", cnn_features_model_type=\"B\")\n",
    "        gc.collect()\n",
    "\n",
    "        train_df.to_feather(df_path)\n",
    "        pd.DataFrame(tr_is_hard_sample, columns=[\"is_hard_sample\"]).to_feather(hard_sample_path)\n",
    "\n",
    "\n",
    "debug = False\n",
    "\n",
    "# equivalent to dataset: nyanpn/nyanp-model-b-0227\n",
    "with timer(\"train model(nyanp-b)\"):\n",
    "    train_model(cfg.INPUT_DIR, \"nyanp-model-b-0227\", train_df, non_feature_cols, is_xgb=False, debug=debug, compile_treelite=cfg.TREELITE_COMPILE)\n",
    "\n",
    "# equivalent to dataset: nyanpn/nfl-kmat-only-2\n",
    "with timer(\"train model(kmat-only)\"):\n",
    "    train_model(cfg.INPUT_DIR, \"nfl-kmat-only-2\", train_df, non_feature_cols + [c for c in train_df.columns if \"camaro\" in c], is_xgb=False, debug=debug, compile_treelite=cfg.TREELITE_COMPILE)\n",
    "\n",
    "# equivalent to dataset: nyanpn/nfl-xgb-8030\n",
    "with timer(\"train model(xgb-8030)\"):\n",
    "    train_model(cfg.INPUT_DIR, \"nfl-xgb-8030\", train_df, non_feature_cols, is_xgb=True, debug=debug, compile_treelite=cfg.TREELITE_COMPILE)\n",
    "\n",
    "del train_df\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a248c2",
   "metadata": {
    "papermill": {
     "duration": 0.073831,
     "end_time": "2023-02-24T23:50:16.659241",
     "exception": false,
     "start_time": "2023-02-24T23:50:16.585410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with timer(\"make_features(K_mat-A)\"):\n",
    "    df_path = os.path.join(cfg.CACHE_DIR, \"train_df_A.f\")\n",
    "\n",
    "    if os.path.exists(df_path):\n",
    "        train_df = pd.read_feather(df_path)\n",
    "    else:\n",
    "        train_df, tr_is_hard_sample = make_features(train, tr_tracking, phase=\"train\", cnn_features_model_type=\"A\")\n",
    "        gc.collect()\n",
    "\n",
    "        train_df.to_feather(df_path)\n",
    "\n",
    "\n",
    "# equivalent to dataset: nyanpn/nyanp-model-a-0227\n",
    "with timer(\"train model(nyanp-a)\"):\n",
    "    train_model(cfg.INPUT_DIR, \"nyanp-model-a-0227\", train_df, non_feature_cols, is_xgb=False, debug=debug, compile_treelite=cfg.TREELITE_COMPILE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 675.111825,
   "end_time": "2023-02-24T23:56:03.993132",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-02-24T23:44:48.881307",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
